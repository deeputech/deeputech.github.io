var documents=[{id:0,url:"https://deepu.tech/404.html",title:"404",body:"404 Page does not exist!Please use the search bar at the top or visit our homepage! "},{id:1,url:"https://deepu.tech/appearances",title:"Talks & Appearances",body:"2022: All Day DevOpsNovember 2022 Building a Kubernetes monitoring dashboard in Rust. Why not? slides JFallNovember 2022 Build and deploy cloud native Java microservices on Kubernetes with Istio service mesh (workshop) slides Cloud Native/Kubernetes meetupOctober 2022 Is containerless the future of Kubernetes? slides | video DevoxxOctober 2022 What the heck is Project Loom and what can a normal Java developer expect from it? slides | video Build and deploy cloud native Java microservices on Kubernetes with Istio service mesh (workshop) slides | video DevOps Global Summit’22October 2022 Is containerless the future of Kubernetes? slides RustLabOctober 2022 Building a Kubernetes monitoring dashboard in Rust. Why not? slides JavaZoneSeptember 2022 Let’s talk about Foreign functions in Java slides DreamixSeptember 2022 Polyglot mindset: Beyond JHipster, Java, Rust and Open Source interview Foojay PodcastSeptember 2022 OpenJDK Discussion Panel podcast Devopsdays Amsterdam 2022June 2022 Is containerless the future of Kubernetes? Let’s see how WebAssembly can make containers obsolete slides | video OpenValue Utrecht MeetupJune 2022 Reactive Java Microservices on Kubernetes with Spring WebFlux, Spring Cloud and JHipster slides | video J-SpringJune 2022 Reactive Java Microservices on Kubernetes with Spring WebFlux, Spring Cloud and JHipster slides | video Java. ILJune 2022 Let’s talk about Foreign functions in Java slides | video Geekle Online Summit for Java devs’22May 2022 Let’s talk about Foreign functions in Java slides Rust Meetup LinzApril 2022 Is Rust a great language for building Kubernetes ecosystem? slides | video Devops. jsMarch 2022 How to Secure Your Node. js Containers on Kubernetes With Best Practices slides | video FOSDEM’22February 2022 Why safe programming matters and why Rust slides | video Let’s Talk About Foreign Functions In Java slides | video DevOpStars’22Building a Kubernetes monitoring dashboard in Rust. Why not? January 2022 slides | video 2021: iDev Enterprise Integration SummitLow code microservice on the cloud with JHipster, Istio, and Kubernetes December 2021 video Tech Lead JournalBeing an Effective Generalist &amp; Building Good Developer Experience November 2021 podcast Amsterdam Java User GroupState of pattern matching in Java October 2021 slides JavaBin OsloPattern Matching &amp; Sealed Classes: Best Features of OpenJDK 17? October 2021 video | slides Japan Java User GroupPattern Matching &amp; Sealed Classes: Best Features of OpenJDK 17? September 2021 video | slides Silesia Java User GroupPattern Matching &amp; Sealed Classes: Best Features of OpenJDK 17? September 2021 video | slides ING Tech meetupCan Rust be the language of the future? September 2021 slides Global Adyen Developer Events and MeetupsCasual chat about Developer Experience and Developer Relations September 2021 meetup The Relicans - Polyglot podcastWhat Is Developer Experience and Why Should We Care? August 2021 podcast | transcript Global Adyen Developer Events and MeetupsFunctional programming in Java June 2021 slides Func Prog SwedenFunctional programming primer for Java April 2021 video | slides Global Summit for Java DevsMicroservices with Istio, JHipster and Kubernetes March 2021 video | slides ING Tech Meetup - Amsterdam, NetherlandsUsing JHipster for boosting development speed and learning March 2021 video | slides 2020: JHipster Conf - Bordeaux, FranceKeynote September 2020 video JHipster Conf - Bordeaux, FranceInterview of Deepu K Sasidharan September 2020 video 2019: Codemotion - Berlin, GermanyMicroservices with Istio, JHipster and Kubernetes November 2019 slides Devoxx Ukraine - Kyiv, UkraineBuilding Native TypeScript applications using Deno runtime November 2019 video | slides Oracle Code One - San Francisco, USAJava Microservices Made Easy with JHipster, Istio, and Kubernetes Sept 2019 video | slides Oracle Code One - San Francisco, USAContributing to Open Source: Practical Steps and Advice Sept 2019 JHipster Conf - Paris, FranceKeynote June 2019 video | slides ItKonekt - Zagreb, CroatiaMicroservices with Istio, JHipster and Kubernetes April 2019 slides Codemotion - Rome, ItalyMicroservices with Istio, JHipster and Kubernetes March 2019 slides 2018: Devoxx Morocco- Marrakech, MoroccoJHipster Microservices with Spring boot 2 and Kubernetes November 2018 slides Devoxx - Antwerp, BelgiumJHipster 5 What’s new and noteworthy November 2018 video | slides Devoxx - Antwerp, BelgiumWhy you should love TypeScript A practical guide November 2018 video | slides Devoxx - Antwerp, BelgiumTypeScript: Complete November 2018 video | slides ChangeCon - Zagreb, CroatiaHow we built our Analytics platform with JHipster and OSS technologies October 2018 video Heapcon - Belgrade, SerbiaHow we built our Analytics platform with JHipster and OSS technologies October 2018 video | slides JHipster Conf - Paris, FranceKeynote June 2018 slides Devoxx FR - Paris, FranceJHipster What’s new in our Spring:Summer 2018 collection April 2018 video | slides Devoxx FR - Paris, FranceFrontend stack for Java developers April 2018 video | slides Facebook Developer Circles - MorrocoHow to be hip with JHipster &amp; React February 2018 slides 2017: Devoxx - Antwerp, BelgiumInterview with Deepu K Sasidharan &amp; Julien Dubois November 2017 video Devoxx - Antwerp, BelgiumAngular vs React Smackdown November 2017 video | slides Devoxx - Antwerp, BelgiumBuilding React applications with JHipster November 2017 video Devoxx - Antwerp, BelgiumEasy microservices with JHipster November 2017 video | slides Spring I/O - Barcelona, SpainBuilding Spring boot + Angular4 apps in minutes with JHipster July 2017 video 2016: Devoxx - Antwerp, BelgiumBe Productive with JHipster November 2016 video | slides Spring User Group - SingaporeJHipster 3. 0 night! March 2016 video "},{id:2,url:"https://deepu.tech/books",title:"Books",body:"Full Stack Development with JHipster - second edition      Get it from:                                                  Follow on Twitter @FullstackBook JHipster is an open source development platform that allows you to easily create web applications and microservices from scratch without spending time on wiring and integrating different technologies together. Updated to include JHipster 6, Java 11, Spring Boot 2. 1, Vue. js, and Istio, this second edition of Full Stack Development with JHipster will help you build full stack applications and microservices seamlessly.   You’ll start by understanding JHipster and its associated tools, along with the essentials of full stack development, before building a monolithic web application. You’ll then learn the JHipster Domain Language (JDL) with entity modeling using JDL Studio. With this book, you’ll create production-ready web applications using Spring Boot, Spring Framework, Angular, and Bootstrap, and run tests and set up Continuous Integration pipelines with Jenkins. As you advance, you’ll learn how to convert your monoliths to microservices and how to package your application for production with various deployment options, including Heroku and Google Cloud. You’ll also learn about Docker and Kubernetes, along with an introduction to the Istio service mesh. Finally, you’ll build your client-side with React and Vue. js and discover JHipster’s best practices. By the end of the book, you’ll be able to leverage the best tools available to build modern web applications. Full Stack Development with JHipster  Get it from:                        JHipster is a development platform to generate, develop, and deploy Spring Boot and Angular/React applications and Spring microservices. It provides you with a variety of tools that will help you quickly build modern web applications. This book will be your guide to building full stack applications with Spring and Angular using the JHipster tool set.   You will begin by understanding what JHipster is and the various tools and technologies associated with it. You will learn the essentials of a full stack developer before getting hands-on and building a monolithic web application with JHipster. From here you will learn the JHipster Domain Language with entity modeling and entity creation using JDL and JDL studio. Moving on, you will be introduced to client side technologies such as Angular and Bootstrap and will delve into technologies such as Spring Security, Spring MVC, and Spring Data. You will learn to build and package applications for production with various deployment options such as Heroku and more. During the course of the book, you will be introduced to microservice server-side technologies and how to break your monolithic application into microservices. Next, the book takes you through cloud deployment of microservices on Docker and Kubernetes. Going forward, you will learn to build your client side with React and master JHipster best practices. By the end of the book, you will be able to leverage the power of the best tools available to build modern web applications. "},{id:3,url:"https://deepu.tech/categories",title:"Categories",body:""},{id:4,url:"https://deepu.tech/comment-policy",title:"Comment policy",body:"Keep it civil aka don’t be a jerk: We’re going to get into the thick of a lot of heated discussions and that’s okay. These discussions often entail topics that we all personally care a lot about and will passionately defend. But in order for discussions to thrive here, we need to remember to criticize ideas, not people.   So, remember to avoid:  name-calling ad hominem attacks Responding to a post’s tone instead of its actual content.  knee-jerk contradictionComments that we find to be hateful, inflammatory, or harassing may be removed. If you don’t have something nice to say about another user, don’t say it. Treat others the way you’d like to be treated. Always strive to add value to every interaction and discussion you participate in: There are a lot of discussions that happen every day. Before joining in a discussion, browse through some of the most recent and active discussions happening in the community, especially if you’re new there.  If you are not sure your post adds to the conversation, think over what you want to say and try again later. Keep it tidy: Help make moderators’ lives easier by taking a moment to ensure that what you’re about to post is in the right place. That means:  don’t post off-topic comments or discussions don’t cross-post the same thing multiple times review any specific posting guidelines for the community check if another active discussion on your topic has already been postedIf you see something, say something: Moderators are at the forefront of combatting spam, mediating disputes and enforcing community guidelines and, so are you.   If you see an issue, contact the site owner if possible or flag any comments for review. If you believe someone has violated the Basic Rules, report it to us by flagging the user’s profile. No Self-promotion A discussion or comment that contains only a link to your blog, a product, or your article on another site will almost always be removed. Choose Your (Curse) Words Wisely Comments that contain profanity are automatically held for moderator review before being posted. Depending on the context of the comment, it may be removed. Profanity used to insult, antagonize, or inflame will always be removed. Don’t Be a Jerk Personal attacks and harassment will not be tolerated. Sexist, racist, misogynist, homophobic, and broad, offensive generalizations about groups of people are simply not allowed. Comments or discussions written intentionally to provoke will also be removed. Don’t Copy and Paste If you didn’t write it, or haven’t properly cited the article you’re quoting, don’t post it. English Only We currently only support English-only discussions. "},{id:5,url:"https://deepu.tech/",title:"Deepu K Sasidharan",body:"                                                                                                                                                                                 Deepu K Sasidharan:                                     Open-source software aficionado and a technology advocate by passion &amp; profession              Co-lead of JHipster, the best development platform in the world 😸              Creator of KDash, a terminal dashboard for Kubernetes written in Rust 🚀              Creator of JDL Studio, JHipster React, JHipster Blueprints, and JHipster Entity Audit              Staff Developer Advocate at Auth0 by Okta. Alumni of Adyen, TCS and XebiaLabs 💵              Java Champion ☕              Polyglot developer mainly working with Java, Rust, JavaScript, TypeScript, Go, and Kotlin. Dabbling with few other languages occasionally.               Cloud &amp; Container technology enthusiast and advocate              Published author, international conference speaker, and an avid blogger              Passionate about developer experience and user experience              Love Astronomy, Motorbikes, Coding and Robotics                                                               Latest Book:                               Full Stack Development with JHipster - second edition. Get it on          Packt,          Amazon and          Safari.                                                                    Latest Blog Post:                                                                                                                                                               Rust Easy! Modern Cross-platform Command Line Tools to Supercharge Your Terminal      :     Rust tooling is taking over the terminal. Use these awesome tools to supercharge your Linux/macOS/Windows terminal. :                                               Deepu K Sasidharan        07 Nov 2022 | 9 mins read                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Some OSS projects I work on:                                                                         JHipster Generator:                                             A cool generator for Angular/React/VueJS + Spring stack                                                                                         KDash:                             A Kubernetes dashboard written in Rust                                                                           JDL Studio:                             An awesome online JDL editor and visualizer                                                                                     JHipster Registry:                                             Service Registry, based on Spring Cloud Netflix Eureka and Spring Cloud Config                                                                                                   JHipster Entity Audit Generator:                                             A yeoman generator to enable entity audit in JHipster generated apps                                                                                         JHipster Bootswatch Theme Generator:                                             A yeoman generator to enable Bootswatch themes in JHipster generated apps                                                                                                   Angular Object Diff:                                             An AngularJS plugin to generate and view object difference                                                                                         UML and Sequence Diagram Generator:                                                           A sequence diagram generator using angularJS and an UML Diagram Generator based on PlantUML. Experimental.                                                                                                 Follow me on social media:                                                                                                                                                                                                                             Upcoming talks:                            Utrecht JUG (Utrecht, Netherlands)- November 30, 2022     What the heck is Project Loom?    Developer Day Amsterdam (Amsterdam, Netherlands)- December 1, 2022     Lock your containers down!                                               Featured posts:                                                                                                                                                My sleek and modern Linux development machine in 2021      :     My 2021 Linux development machine explained:                                               Deepu K Sasidharan        01 Nov 2021 | 6 mins read                                                                                                                                                                         My second impression of Rust and why I think it's a great general-purpose language!      :     My second impression of Rust, after building a real-life use case with it and my thoughts on why it is a great general-purpose language for. . . :                                               Deepu K Sasidharan        07 May 2021 | 14 mins read                                                                                                                                                                        The state of Linux as a daily use OS in 2021      :     Let's see what is the state of Linux, for daily use, in 2021 is and if its a viable alternative to Windows and macOS:                                               Deepu K Sasidharan        24 Mar 2021 | 16 mins read                                                                                                                                                                            Concurrency in modern programming languages: Introduction      :     Building a concurrent web server in Rust, Go, JS, TS, Kotlin, and Java to compare concurrency performance:                                               Deepu K Sasidharan        27 Nov 2020 | 5 mins read                                                                                                                                                                                   🚀 Demystifying memory management in modern programming languages      :     Let us take a look at how modern programming languages manage memory. :                                               Deepu K Sasidharan        08 Jan 2020 | 6 mins read                                                                                                                                                                                      My reflections on Golang      :     After using Go for more than 9 months, here is what I think of it:                                               Deepu K Sasidharan        12 Jul 2019 | 12 mins read                                                                                                                      Blogs on Dev. to:                           "},{id:6,url:"https://deepu.tech/series",title:"Blog Series",body:""},{id:7,url:"https://deepu.tech/robots.txt",title:"",body:"      Sitemap: {{ “sitemap. xml”   absolute_url }}   "},{id:8,url:"https://deepu.tech/blogs/index.html",title:"Blogs",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:9,url:"https://deepu.tech/blogs/page/2/index.html",title:" - page 2",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:10,url:"https://deepu.tech/blogs/page/3/index.html",title:" - page 3",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:11,url:"https://deepu.tech/blogs/page/4/index.html",title:" - page 4",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:12,url:"https://deepu.tech/blogs/page/5/index.html",title:" - page 5",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:13,url:"https://deepu.tech/blogs/page/6/index.html",title:" - page 6",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:14,url:"https://deepu.tech/blogs/page/7/index.html",title:" - page 7",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:15,url:"https://deepu.tech/blogs/page/8/index.html",title:" - page 8",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:16,url:"https://deepu.tech/blogs/page/9/index.html",title:" - page 9",body:"       All Blog series   |      All Tags         Latest:         {% for post in site. posts %}      {% if forloop. index &lt; 3 %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        Featured:         {% for post in site. posts %}      {% if post. featured == true %}          {% include featuredbox. html %}          {% endif %}    {% endfor %}        {% include carbon-ad. html %}         All Stories:               {% for post in paginator. posts %}              {% include postbox. html %}            {% endfor %}                          {% include pagination. html %}            "},{id:17,url:"https://deepu.tech/rust-terminal-tools-linux-mac-windows-fish-zsh/",title:"Rust Easy! Modern Cross-platform Command Line Tools to Supercharge Your Terminal",body:"2022/11/07 - Rust is taking over the terminal. Rust is a general-purpose programming language that is blazing fast and memory safe. It is the fastest-growing and most loved programming language in the world. It is used to build everything from operating systems to web servers to command-line tools. Recently there has been a surge of command line tools and utilities written in Rust, and many of them are intended to replace standard Unix commands. They are faster, more user-friendly, and have more features than their standard Unix counterparts. In this post, I will cover some of the best Rust command line tools I have used for a while. You can also use these to supercharge your terminal. These tools are available for both Linux and macOS. I have not tested them on Windows, but most should also work on Windows. I recommend aliasing the commands to replace the standard commands based on your preferences. If you have Cargo, the rust package manager, you can install all these using Cargo. Alacritty: Let us start with the terminal itself. Alacritty is a cross-platform modern terminal emulator with sensible defaults. It is GPU accelerated, super fast, and highly configurable. You can use it on Linux, macOS, and Windows. It doesn’t have much in terms of a UI, and hence all configurations are done through YAML files. I don’t use it as my primary terminal as I love Yakuake too much for all its cool features. We can get most of those features (tabs, split panes, dropdown mode) using tmux and tdrop if really needed. I use Alacrity when I need speed and GPU acceleration. There is an excellent tutorial on using Alacritty with tmux. You could also use Zellij, a modern terminal multiplexer written in Rust, with Alacritty. There is also the Warp terminal, but it is not open source. It is a great terminal, but I prefer open source software. Thanks to Fran Sancisco for the suggestion.  Installation: 123456789101112131415# Arch Linuxyay -S alacritty# Fedora/CentOSdnf copr enable atim/alacrittydnf install alacritty# Debian/Ubuntuadd-apt-repository ppa:aslatter/ppaapt install alacritty# macOS Homebrewbrew install --cask alacritty# Windows Scoopscoop bucket add extrasscoop install alacritty# Cargo on anycargo install alacrittyStarship: Starship is the best terminal prompt I have ever used. Forget Oh My Zsh and stuff like that. Starship is fast, highly customizable, and has a great default theme and settings. I didn’t even change most of the default settings, as things were perfect as it is. Starship works on shells like zsh, fish, and bash and can also work alongside other prompts like Oh My Zsh, in case you still want to use Oh My Zsh for other plugins like autosuggestions and so on. Starship works best with a Nerd Font as it can show icons and ligatures based on context. I used Oh My Zsh for many years with the powerlevel10k theme, but the prompt was a bit slow. Starship is blazing fast with more features and an excellent UX.  Installation: 1234567891011121314# Arch Linuxyay -S starship# Fedora/CentOSdnf install starship# Debian/Ubuntucurl -sS https://starship. rs/install. sh | sh# macOS/Linux Homebrewbrew install starship# macOS MacPortsport install starship# Windows Scoopscoop install starship# Cargocargo install starship --lockedbat: bat is one of my favorite tools from this list. It’s a replacement for cat, and once you have used bat, you will never go back. It provides features like syntax highlight, line numbers, Git change highlight, shows special chars, paging, and so on. It is super fast and looks beautiful. I have aliased cat to bat immediately after trying it for the first time. By default, bat behaves similarly to less by paging large output, but that can be disabled to make it work precisely like cat. It can be used as a drop-in replacement for cat even in scripts. bat can also be used as a previewer for fzf. It can also be combined with many other commands and tools like tail, man, and git, among others, to add syntax highlighting to outputs. Syntax highlighting themes are configurable.  Installation: 1234567891011121314# Arch Linuxyay -S bat# Fedora/CentOSdnf install bat# Debian/Ubuntuapt install bat# macOS/Linux Homebrewbrew install bat# macOS MacPortsport install bat# Windows Scoopscoop install bat# Cargocargo install bat --lockedLSD and exa: Both LSD and exa are replacements for the ls command. They both look gorgeous with nice colors and icons and have features like headers, sorting, tree views, and so on. Exa is a bit faster than LSD for tree views and can show the Git status of files and folders. I prefer exa due to the Git support and faster tree views. I have set up my ls alias to use exa by default. Both can be configured to show custom columns and sorting behaviors.  exa Installation: 12345678910111213# Arch Linuxyay -S exa# Fedora/CentOSdnf install exa# Debian/Ubuntuapt install exa# macOS Homebrewbrew install exa# Cargocargo install exa# Alias ls to exaalias ls='exa --git --icons --color=always --group-directories-first'LSD Installation: 1234567891011121314151617# Arch Linuxyay -S lsd# Fedora/CentOSdnf install lsd# Debian/Ubuntudpkg -i lsd_0. 23. 1_amd64. deb # get . deb file from https://github. com/Peltoche/lsd/releases# macOS Homebrewbrew install lsd# macOS MacPortsport install lsd# Windows Scoopscop install lsd# Cargocargo install lsd# Alias ls to lsdalias ls='lsd --header --color=always --group-directories-first'rip: rip is an improved version of the rm command. It is faster, safer, and user-friendly. rip sends deleted files to a temp location so they can be recovered using rip -u. I really like the simplicity and the revert feature, as I don’t have to worry about accidentally deleting something using rm. While rip can be aliased to replace rm, the creators advise not doing that as you might get used to it and do rm on other systems where you cannot revert the delete. Installation: 12345678# Arch Linuxyay -S rm-improved# Fedora/CentOS/Debian/Ubuntu# Install from binary or build locally using Cargo# macOS Homebrewbrew install rm-improved# Cargocargo install rm-improvedxcp: xcp is a partial clone of the cp command. It is faster and more user-friendly with progress bars, parallel copying, . gitignore support, and so on. I like its simplicity and developer experience, especially the progress bars. I have aliased cp to xcp so I can use it everywhere.  Installation: 123456789# Arch Linuxyay -S xcp# Fedora/CentOS/Debian/Ubuntu/macOS# Install from binary or build locally using Cargo# Cargocargo install xcp# Alias cp to xcpalias cp='xcp'zoxide: zoxide is a smarter cd replacement. It remembers the directories you visit, and you can jump to them without providing a full path. You can provide partial paths or even a word from the path. When there are similar paths, zoxide offers an interactive selection using fzf. It is super fast and works with all major shells. I like how it works, and I have aliased cd to z so I can use it everywhere.  Installation: 1234567891011121314# Arch Linuxyay -S zoxide# Fedora/CentOSdnf install zoxide# Debian/Ubuntuapt install zoxide# macOS/Linux Homebrewbrew install zoxide# macOS MacPortsport install zoxide# Windows Scoopscoop install zoxide# Cargocargo install zoxide --lockedOnce installed, you must add the following to your shell config file. For other shells, refer the docs 123456789# bash (~/. bashrc)eval  $(zoxide init bash) # zsh (~/. zshrc)eval  $(zoxide init zsh) # fish (~/. config/fish/config. fish)zoxide init fish | source# Alias cd to zalias cd='z'dust: Dust is an alternative for the du command. It is fast and has a better UX with nice visualization for disk usage.  Installation: 1234567891011121314# Arch Linuxyay -S dust# Fedora/CentOS# Install binary from https://github. com/bootandy/dust/releases# Debian/Ubuntudeb-get install du-dust# macOS Homebrewbrew install dust# macOS MacPortsport install dust# Windows Scoopscoop install dust# Cargocargo install du-dustripgrep: ripgrep (rg) is a line-oriented search tool that recursively searches your current directory for a regex pattern. It is faster than grep and has many features like compressed files search, colorized output, smart case, file type filtering, multi-threading, and so on. It understands . gitignore files and skips hidden and ignored files. Here is a feature comparison with other similar tools, and yes, it is faster than all the other tools in the list.  Installation: 1234567891011121314# Arch Linuxyay -S ripgrep# Fedora/CentOSdnf install ripgrep# Debian/Ubuntuapt-get install ripgrep# macOS/Linux Homebrewbrew install ripgrep# macOS MacPortsport install ripgrep# Windows Scoopscoop install ripgrep# Cargocargo install ripgrepfd: fd is a simpler alternative to find. It is more intuitive to use and comes with sensible defaults. It is extremely fast due to parallel traversing and shows a modern colorized output and supports patterns and regex, parallel commands, smart case, understands . gitignore files, and so on. I have aliased find to fd as I could never remember what options to pass to get a basic find command working.  Installation: 1234567891011121314# Arch Linuxyay -S fd# Fedora/CentOSdnf install fd-find# Debian/Ubuntuapt install fd-find# macOS Homebrewbrew install fd# macOS MacPortsport install fd# Windows Scoopscoop install fd# Cargocargo install fd-findsd: sd is a find-and-replace CLI, and you can use it as a replacement for sed and awk. It is way more user-friendly and modern. It is also magnitudes faster than sed. Installation: 123456789101112# Arch Linuxyay -S sd# Fedora/CentOSdnf install sd# Debian/Ubuntu# Install binary from the release page# macOS Homebrewbrew install sd# Windows Scoopchoco install sd-cli# Cargocargo install sdprocs: procs is a ps replacement. It provides colorized human-readable output, multi-column search, more information than ps, docker support, paging, watch mode, and tree view. It is a much more user-friendly and modern alternative to ps. You can filter by name and PID and use logical and/or operators to combine multiple filters. It also has a tree view which is very useful for seeing the process hierarchy. It can also show docker container names for the process running docker containers.  Installation: 1234567891011121314# Arch Linuxyay -S procs# Fedora/CentOSdnf install procs# Debian/Ubuntu# Install binary from the release page# macOS Homebrewbrew install procs# macOS MacPortsport install procs# Windows Scoopscoop install procs# Cargocargo install procsbottom: bottom is a top replacement with a nice terminal UI. It’s quite feature-rich and customizable.  Installation: 123456789101112131415# Arch Linuxyay -S bottom# Fedora/CentOSdnf copr enable atim/bottom -ydnf install bottom# Debian/Ubuntudpkg -i bottom_0. 6. 8_amd64. deb# macOS Homebrewbrew install bottom# macOS MacPortsport install bottom# Windows Scoopscoop install bottom# Cargocargo install bottom --lockedTopgrade: Topgrade is a fantastic utility if you prefer to keep your system up-to-date, like me. It detects most of the package managers on your system and triggers updates. It is configurable, so you can configure it to ignore certain package managers. On my system, it detected pacman, SDKMAN, Flatpak, snap, Homebrew, rustup, Linux firmware, Pip, and so on. Topgrade is cross-platform; you can use it on Windows, macOS, and Linux.  Installation: 12345678910# Arch Linuxyay -S topgrade# Fedora/CentOS/Debian/Ubuntu/Windows# Install binary from the release page# macOS Homebrewbrew install topgrade# macOS MacPortsport install topgrade# Cargocargo install topgrade --lockedBroot: Broot is a tree alternative with a better user experience, and you can use it to navigate a file structure. It’s fast and respects . gitignore. You can cd into a directory from the tree view, open sub-directories in a panel, and even preview files. It has excellent keyboard navigation as well. It has many more features.  Installation: 12345678910# Arch Linuxyay -S broot# Fedora/CentOS/Debian/Ubuntu/Windows# Install binary from release page https://dystroy. org/broot/install/# macOS Homebrewbrew install broot# macOS MacPortsport install broot# Cargocargo install broot --lockedTokei: Tokei is a nice utility to count lines and stats of code. It is very fast, accurate, and has a nice output. It supports over 150 languages and can output in JSON, YAML, CBOR, and human-readable tables.  Installation: 1234567891011121314# Arch Linuxyay -S tokei# Fedora/CentOSdnf install tokei# Debian/Ubuntu# Install binary from the release page# macOS Homebrewbrew install tokei# macOS MacPortsport install tokei# Windows Scoopscoop install tokei# Cargocargo install tokeiOther notable tools:  kdash: A fast and simple dashboard for Kubernetes. Its created by me :) Zellij: A feature rich modern terminal multiplexer with batteries included.  Nushell: A modern shell written in Rust. Looks quite promising.  xh: A HTTPie alternative with better performance.  monolith: Convert any webpage into a single HTML file with all assets inlined.  delta: A syntax-highlighting pager for git, diff, and grep output.  ripsecrets: Find secret keys in your code before committing them to git.  eva: A CLI REPL calculator.  You can find a list of other Rust CLI tools hereIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:18,url:"https://deepu.tech/state-of-java-project-loom/",title:"What the Heck Is Project Loom for Java?",body:"2022/08/27 - Java has had good multi-threading and concurrency capabilities from early on in its evolution and can effectively utilize multi-threaded and multi-core CPUs. Java Development Kit (JDK) 1. 1 had basic support for platform threads (or Operating System (OS) threads), and JDK 1. 5 had more utilities and updates to improve concurrency and multi-threading. JDK 8 brought asynchronous programming support and more concurrency improvements. While things have continued to improve over multiple versions, there has been nothing groundbreaking in Java for the last three decades, apart from support for concurrency and multi-threading using OS threads. Though the concurrency model in Java is powerful and flexible as a feature, it was not the easiest to use, and the developer experience hasn’t been great. This is primarily due to the shared state concurrency model used by default. One has to resort to synchronizing threads to avoid issues like data races and thread blocking. I wrote more about Java concurrency in my Concurrency in modern programming languages: Java post. What is Project Loom?:  Project Loom aims to drastically reduce the effort of writing, maintaining, and observing high-throughput concurrent applications that make the best use of available hardware.  — Ron Pressler (Tech lead, Project Loom) OS threads are at the core of Java’s concurrency model and have a very mature ecosystem around them, but they also come with some drawbacks and are expensive computationally. Let’s look at the two most common use cases for concurrency and the drawbacks of the current Java concurrency model in these cases. One of the most common concurrency use cases is serving requests over the wire using a server. For this, the preferred approach is the thread-per-request model, where a separate thread handles each request. Throughput of such systems can be explained using Little’s law, which states that in a stable system, the average concurrency (number of requests concurrently processed by the server), L, is equal to the throughput (average rate of requests), λ, times the latency (average duration of processing each request), W. With this, you can derive that throughput equals average concurrency divided by latency (λ = L/W). So in a thread-per-request model, the throughput will be limited by the number of OS threads available, which depends on the number of physical cores/threads available on the hardware. To work around this, you have to use shared thread pools or asynchronous concurrency, both of which have their drawbacks. Thread pools have many limitations, like thread leaking, deadlocks, resource thrashing, etc. Asynchronous concurrency means you must adapt to a more complex programming style and handle data races carefully. There are also chances for memory leaks, thread locking, etc. Another common use case is parallel processing or multi-threading, where you might split a task into subtasks across multiple threads. Here you have to write solutions to avoid data corruption and data races. In some cases, you must also ensure thread synchronization when executing a parallel task distributed over multiple threads. The implementation becomes even more fragile and puts a lot more responsibility on the developer to ensure there are no issues like thread leaks and cancellation delays. Project Loom aims to fix these issues in the current concurrency model by introducing two new features: virtual threads and structured concurrency. Virtual threads:  Java 19 is scheduled to be released in September 2022, and Virtual threads will be a preview feature. Yayyy! Virtual threads are lightweight threads that are not tied to OS threads but are managed by the JVM. They are suitable for thread-per-request programming styles without having the limitations of OS threads. You can create millions of virtual threads without affecting throughput. This is quite similar to coroutines, like goroutines, made famous by the Go programming language (Golang). The new virtual threads in Java 19 will be pretty easy to use. Compare the below with Golang’s goroutines or Kotlin’s coroutines. Virtual thread 123Thread. startVirtualThread(() -&gt; {  System. out. println( Hello, Project Loom! );});Goroutine 123go func() {  println( Hello, Goroutines! )}()Kotlin coroutine 12345runBlocking {  launch {    println( Hello, Kotlin coroutines! )  }}Fun fact: before JDK 1. 1, Java had support for green threads (aka virtual threads), but the feature was removed in JDK 1. 1 as that implementation was not any better than platform threads. The new implementation of virtual threads is done in the JVM, where it maps multiple virtual threads to one or more OS threads, and the developer can use virtual threads or platform threads as per their needs. A few other important aspects of this implementation of virtual threads:  It is a Thread in code, runtime, debugger, and profiler It’s a Java entity and not a wrapper around a native thread Creating and blocking them are cheap operations They should not be pooled Virtual threads use a work-stealing ForkJoinPool scheduler Pluggable schedulers can be used for asynchronous programming A virtual thread will have its own stack memory The virtual threads API is very similar to platform threads and hence easier to adopt/migrateLet’s look at some examples that show the power of virtual threads. Total number of threads: First, let’s see how many platform threads vs. virtual threads we can create on a machine. My machine is Intel Core i9-11900H with 8 cores, 16 threads, and 64GB RAM running Fedora 36. Platform threads 12345678var counter = new AtomicInteger();while (true) {  new Thread(() -&gt; {    int count = counter. incrementAndGet();    System. out. println( Thread count =   + count);    LockSupport. park();  }). start();}On my machine, the code crashed after 32_539 platform threads. Virtual threads 12345678var counter = new AtomicInteger();while (true) {  Thread. startVirtualThread(() -&gt; {    int count = counter. incrementAndGet();    System. out. println( Thread count =   + count);    LockSupport. park();  });}On my machine, the process hung after 14_625_956 virtual threads but didn’t crash, and as memory became available, it kept going slowly. You may be wondering why this behavior! It’s due to the parked virtual threads being garbage collected, and the JVM is able to create more virtual threads and assign them to the underlying platform thread. Task throughput: Let’s try to run 100,000 tasks using platform threads. 1234567try (var executor = Executors. newThreadPerTaskExecutor(Executors. defaultThreadFactory())) {  IntStream. range(0, 100_000). forEach(i -&gt; executor. submit(() -&gt; {    Thread. sleep(Duration. ofSeconds(1));    System. out. println(i);    return i;  }));}This uses the newThreadPerTaskExecutor with the default thread factory and thus uses a thread group. When I ran this code and timed it, I got the numbers shown here. I get better performance when I use a thread pool with Executors. newCachedThreadPool(). 1234# 'newThreadPerTaskExecutor' with 'defaultThreadFactory'0:18. 77 real,  18. 15 s user,  7. 19 s sys,   135% 3891pu,  0 amem,     743584 mmem# 'newCachedThreadPool' with 'defaultThreadFactory'0:11. 52 real,  13. 21 s user,  4. 91 s sys,   157% 6019pu,  0 amem,     2215972 mmemNot so bad. Now, let’s do the same using virtual threads. 1234567try (var executor = Executors. newVirtualThreadPerTaskExecutor()) {  IntStream. range(0, 100_000). forEach(i -&gt; executor. submit(() -&gt; {    Thread. sleep(Duration. ofSeconds(1));    System. out. println(i);    return i;  }));}If I run and time it, I get the following numbers. 10:02. 62 real,  6. 83 s user,  1. 46 s sys,   316% 14840pu,  0 amem,     350268 mmemThis is far more performant than using platform threads with thread pools. Of course, these are simple use cases; both thread pools and virtual thread implementations can be further optimized for better performance, but that’s not the point of this post. Running Java Microbenchmark Harness (JMH) with the same code gives the following results, and you can see that virtual threads outperform platform threads by a huge margin. 1234567891011# ThroughputBenchmark               Mode Cnt Score  Error UnitsLoomBenchmark. platformThreadPerTask thrpt  5 0. 362 ± 0. 079 ops/sLoomBenchmark. platformThreadPool   thrpt  5 0. 528 ± 0. 067 ops/sLoomBenchmark. virtualThreadPerTask  thrpt  5 1. 843 ± 0. 093 ops/s# Average timeBenchmark               Mode Cnt Score  Error UnitsLoomBenchmark. platformThreadPerTask  avgt  5 5. 600 ± 0. 768  s/opLoomBenchmark. platformThreadPool   avgt  5 3. 887 ± 0. 717  s/opLoomBenchmark. virtualThreadPerTask  avgt  5 1. 098 ± 0. 020  s/opYou can find the benchmark source code on GitHub. Here are some other meaningful benchmarks for virtual threads:  An interesting benchmark using ApacheBench on GitHub by Elliot Barlas A benchmark using Akka actors on Medium by Alexander Zakusylo JMH benchmarks for I/O and non-I/O tasks on GitHub by Colin CachiaStructured concurrency:  Structured concurrency will be an incubator feature in Java 19. Structured concurrency aims to simplify multi-threaded and parallel programming. It treats multiple tasks running in different threads as a single unit of work, streamlining error handling and cancellation while improving reliability and observability. This helps to avoid issues like thread leaking and cancellation delays. Being an incubator feature, this might go through further changes during stabilization. Consider the following example using java. util. concurrent. ExecutorService. 1234567891011void handleOrder() throws ExecutionException, InterruptedException {  try (var esvc = new ScheduledThreadPoolExecutor(8)) {    Future&lt;Integer&gt; inventory = esvc. submit(() -&gt; updateInventory());    Future&lt;Integer&gt; order = esvc. submit(() -&gt; updateOrder());    int theInventory = inventory. get();  // Join updateInventory    int theOrder = order. get();      // Join updateOrder    System. out. println( Inventory   + theInventory +   updated for order   + theOrder);  }}We want updateInventory() and updateOrder() subtasks to be executed concurrently. Each of those can succeed or fail independently. Ideally, the handleOrder() method should fail if any subtask fails. However, if a failure occurs in one subtask, things get messy.  Imagine that updateInventory() fails and throws an exception. Then, the handleOrder() method throws an exception when calling inventory. get(). So far this is fine, but what about updateOrder()? Since it runs on its own thread, it can complete successfully. But now we have an issue with a mismatch in inventory and order. Suppose the updateOrder() is an expensive operation. In that case, we are just wasting the resources for nothing, and we will have to write some sort of guard logic to revert the updates done to order as our overall operation has failed.  Imagine that updateInventory() is an expensive long-running operation and updateOrder() throws an error. The handleOrder() task will be blocked on inventory. get() even though updateOrder() threw an error. Ideally, we would like the handleOrder() task to cancel updateInventory() when a failure occurs in updateOrder() so that we are not wasting time.  If the thread executing handleOrder() is interrupted, the interruption is not propagated to the subtasks. In this case updateInventory() and updateOrder() will leak and continue to run in the background. For these situations, we would have to carefully write workarounds and failsafe, putting all the burden on the developer. We can achieve the same functionality with structured concurrency using the code below. 123456789101112void handleOrder() throws ExecutionException, InterruptedException {  try (var scope = new StructuredTaskScope. ShutdownOnFailure()) {    Future&lt;Integer&gt; inventory = scope. fork(() -&gt; updateInventory());    Future&lt;Integer&gt; order = scope. fork(() -&gt; updateOrder());    scope. join();      // Join both forks    scope. throwIfFailed(); // . . . and propagate errors    // Here, both forks have succeeded, so compose their results    System. out. println( Inventory   + inventory. resultNow() +   updated for order   + order. resultNow());  }}Unlike the previous sample using ExecutorService, we can now use StructuredTaskScope to achieve the same result while confining the lifetimes of the subtasks to the lexical scope, in this case, the body of the try-with-resources statement. The code is much more readable, and the intent is also clear. StructuredTaskScope also ensures the following behavior automatically.    Error handling with short-circuiting — If either the updateInventory() or updateOrder() fails, the other is canceled unless its already completed. This is managed by the cancellation policy implemented by ShutdownOnFailure(); other policies are possible.     Cancellation propagation — If the thread running handleOrder() is interrupted before or during the call to join(), both forks are canceled automatically when the thread exits the scope.     Observability — A thread dump would clearly display the task hierarchy, with the threads running updateInventory() and updateOrder() shown as children of the scope.  State of Project Loom: The Loom project started in 2017 and has undergone many changes and proposals. Virtual threads were initially called fibers, but later on they were renamed to avoid confusion. Today with Java 19 getting closer to release, the project has delivered the two features discussed above. One as a preview and another as an incubator. Hence the path to stabilization of the features should be more precise. What does this mean to regular Java developers?: When these features are production ready, it should not affect regular Java developers much, as these developers may be using libraries for concurrency use cases. But it can be a big deal in those rare scenarios where you are doing a lot of multi-threading without using libraries. Virtual threads could be a no-brainer replacement for all use cases where you use thread pools today. This will increase performance and scalability in most cases based on the benchmarks out there. Structured concurrency can help simplify the multi-threading or parallel processing use cases and make them less fragile and more maintainable. What does this mean to Java library developers?: When these features are production ready, it will be a big deal for libraries and frameworks that use threads or parallelism. Library authors will see huge performance and scalability improvements while simplifying the codebase and making it more maintainable. Most Java projects using thread pools and platform threads will benefit from switching to virtual threads. Candidates include Java server software like Tomcat, Undertow, and Netty; and web frameworks like Spring and Micronaut. I expect most Java web technologies to migrate to virtual threads from thread pools. Java web technologies and trendy reactive programming libraries like RxJava and Akka could also use structured concurrency effectively. This doesn’t mean that virtual threads will be the one solution for all; there will still be use cases and benefits for asynchronous and reactive programming. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. The cover image was created using a photo by Peter Herrmann on Unsplash This post was originally published on the Okta Developer Blog. "},{id:19,url:"https://deepu.tech/go-impressions-part-2/",title:"Golang Finally Gets Generics! Does It Make Go a Better Language?",body:"2022/04/04 - So Golang finally made the sane decision to add support for generics, in version 1. 18, after dragging it out for years and after multiple proposals. Well, simplicity is nice when you are a newbie in the language but becomes annoying really fast, in my personal experience, especially in large codebases. So #golang finally makes the sane decision to introduce Generics in version 1. 18 Finally people can get rid of like atleast 1/3 of their codebase which is basically just finding an element from an array or map 😂😂😂They could have kept the syntax as &lt;&gt; though 🤷‍♂️ &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) March 21, 2022As a polyglot developer who has worked extensively in Golang for a few years, I have been quite critical of it in a previous blog post titled “My reflections on Golang”, written in 2019. If you are ready to be triggered, go ahead and read that post and come back here. One of my biggest gripes was that Go didn’t have generics. So now that it has generics, I decided to re-evaluate my opinions about Go. Generics in Go: First, let’s see how generics work in Go. Fortunately, generics in Go work pretty much the same way as in other languages like C++, Java, C#, and Rust. Of course, there will be some minor differences and some missing features since the Go version is, as you guessed, simpler. The design document for generics has some comparisons with Java, C++, and Rust. Though I would have preferred to see the &lt;&gt; syntax to keep in line with most other popular languages, there seem to be pretty good reasons to use [] instead, like avoiding ambiguity when mixed with channels and multiple variables assignments. Let’s look at some examples. One common use case where Go developers usually write duplicated code is array and map operations like find, filter, map, and reduce. I can remember being annoyed about this and for the lack of a library like Lodash for the same reason. With generics, you can finally write reusable utility code once and use it for all data types. Let’s write a findIndex method for arrays with and without generics. Without generics: Without generics, you would write a function for each type of data you want to use, and as you start using structs, you would need a function for every unique struct. In large codebases, it’s common to find thousands and thousands of lines of code just covering utilities where the only difference would be the type of argument in the method signature. This is the complete opposite of the DRY (Don’t repeat yourself) principle. Every time you have to use a new data type, you must write a new function or a new block of code. Yikes! 1234567891011121314151617181920212223242526272829303132333435func main() {\ta := FindIndexFromStringArray([]string{ a ,  b ,  c },  a )\tb := FindIndexFromIntArray([]int{1, 2, 3}, 2)\tc := FindIndexFromFloat64Array([]float64{1. 1, 2. 2, 3. 3}, 3. 3)\tprintln(a, b, c)}func FindIndexFromStringArray(arr []string, target string) int {\tfor i, v := range arr {\t\tif v == target {\t\t\treturn i\t\t}\t}\treturn -1}func FindIndexFromIntArray(arr []int, target int) int {\tfor i, v := range arr {\t\tif v == target {\t\t\treturn i\t\t}\t}\treturn -1}func FindIndexFromFloat64Array(arr []float64, target float64) int {\tfor i, v := range arr {\t\tif v == target {\t\t\treturn i\t\t}\t}\treturn -1}// and moreWith generics: You can simplify the above code to one function with generics, and it will work for all possible data types you pass. 1234567891011121314151617181920212223func main() {\ttype Foo struct {\t\ta string\t\tb bool\t}\ta := FindIndexFromArray([]string{ a ,  b ,  c },  a )\tb := FindIndexFromArray([]int{1, 2, 3}, 2)\tc := FindIndexFromArray([]float64{1. 1, 2. 2, 3. 3}, 3. 3)\td := FindIndexFromArray([]Foo{  { a , true}, { b , false}, { c , true}, }, Foo{ a , true})\tprintln(a, b, c, d)}func FindIndexFromArray[T comparable](arr []T, target T) int {\tfor i, v := range arr {\t\tif v == target {\t\t\treturn i\t\t}\t}\treturn -1}Generics would significantly reduce duplicated code in your codebase. You can also write other useful generic functions like map, reduce, filter, and so on for arrays and maps. Here are some examples from the official design document. 123456789101112131415161718192021222324252627282930313233343536373839404142434445// Map over any slice using the given mapping function. func Map[T1, T2 any](s []T1, f func(T1) T2) []T2 { r := make([]T2, len(s)) for i, v := range s {  r[i] = f(v) } return r}floats := Map([]int{1, 2, 3}, func(i int) float64 { return float64(i) })// Reduce any slice using the given reduction function. func Reduce[T1, T2 any](s []T1, initializer T2, f func(T2, T1) T2) T2 { r := initializer for _, v := range s {  r = f(r, v) } return r}sum := Reduce([]int{1, 2, 3}, 0, func(i, j int) int { return i + j })// Filter any slice using the given predicate function. func Filter[T any](s []T, f func(T) bool) []T { var r []T for _, v := range s {  if f(v) {   r = append(r, v)  } } return r}evens := Filter([]int{1, 2, 3}, func(i int) bool { return i%2 == 0 })// Keys returns a slice of keys from a map. func Keys[K comparable, V any](m map[K]V) []K { r := make([]K, 0, len(m)) for k := range m {  r = append(r, k) } return r}keys := Keys(map[string]int{ a :2,  b :4})When declaring generic types, the type can be specific (T comparable), any (T any), approximate (T ~string), or a union (T int64 | float64 | int). Type constraints can be defined as type aliases as well. Like in Java or Rust, you can use generics in Go for functions, struct containers, interface implementations, etc. This can help to reduce boilerplate code and make writing Go code much more enjoyable. Does this make Go better?: Yes! Without a doubt, generics make Go much more enjoyable to write. While generics adds a bit of complexity, IMO the amount of boilerplate you can get rid of is worth that added complexity. For polyglot developers, who are used to languages like Java, TypeScript, Rust, C#, or C++, this is an excellent feature that might sway them to try Go. I talked about the things I liked in Go in my previous post, so I’m not going to reiterate that here. Let’s see if what I didn’t like remains the same and if generics help alleviate those pain points. Please note that this is very opinionated base on my tastes and experiences, especially as a polyglot developer.  Generics: ✅ Finally here and works great Error handling: ❌ It is still tedious and needs boilerplate Default values: ❌ There is still no default values for methods Too much boilerplate: ✅ Introduction of generics will remove a lot of it Dependency management: ❌ Still not a fan of Go’s dependency management and especially the way breaking versions are handled Source code in GOPATH: ✅ Not a problem anymore with Go modules Confusing pointer behaviors: ❌ Pointers are still confusing and need to be used with care Struct hell: ✅ Generics should help to make this pain point much less painful Weird interface construct: ❌ I’m still not a fan of this, and IMO Rust has a much better design here Single GC algorithm: ❌ Maybe I’m just spoiled by Java. Many testimonials detail how the Go GC algorithm doesn’t work in some use cases.  Developer experience: ✅ I would say this has improved over the years. Still not as good as Rust, but better than many other languages. Where does that leave Go?: Recently I was pretty interested in systems programming and CLIs. I would have used Go for those if I hadn’t known Rust. Once I started using Rust, I gave up on Go, as I didn’t see much reason to use it over Rust for the use cases that I was interested in. Honestly, the annoyance with Go was one reason that prompted me to learn Rust. I did use Go for some simple stuff like building a CLI for the Elgato Keylights, and probably if I had a use case of building microservices, then I might have given Go a thought along with Java. With generics making Go much better, IMO, I might give Go more weightage for microservices and simple CLIs. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Image derived from work in egonelbre/gophers created by @egonelbre. "},{id:20,url:"https://deepu.tech/concurrency-in-modern-languages-final/",title:"Concurrency in modern programming languages: Rust vs Go vs Java vs Node.js vs Deno vs .NET 6",body:"2022/02/04 - This is a multi-part series where I’ll discuss concurrency in modern programming languages. I will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin, and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksWhat is concurrency:  Concurrency is one of the most complex aspects of programming, and depending on your language of choice, the complexity can be anywhere from “that looks confusing” to “what black magic is this”. Concurrency is the ability where multiple tasks can be executed in overlapping time periods, in no specific order without affecting the final outcome. Concurrency is a very broad term and can be achieved by multi-threading, parallelism, and/or asynchronous processing.  First, I suggest you read the introduction post to understand this post better. Benchmarking &amp; comparison: In the previous posts, I built a simple web server in Rust, Go, Node. js, Deno, and Java. I kept it as simple as possible without using external dependencies as much as possible. I also kept the code similar across languages. In this final post, we will compare the performance of all these implementations to see which language offers the best performance for a concurrent web server. If the language supports both asynchronous and multi-threaded concurrency, we will try both and a combination of both and pick the best performer for the comparison. The complexity of the application will hence depend on language features and language complexity. We will use whatever the language provides to make concurrency performance as good as possible without over-complicating stuff. The web server will just serve one endpoint, and it will add a sleep of two seconds on every tenth request. This will simulate a more realistic load, IMO. We will use promises, thread pools, and workers if required and if the language supports it. We won’t use any unnecessary I/O in the application. The code implementations are probably not the best possible; if you have a suggestion for improvement, please open and issue or PR on this repository. Further improvements possible are:  Use a thread pool for Java multi-threaded version Use a Java webserver library Use createReadStream for Node. js Use Warp, Rocket or actix-web for Rust Added a Rust actix-web sample to comparisonDisclaimer: I’m not claiming this to be an accurate scientific method or the best benchmark for concurrency. I’m pretty sure different use cases will have different results, and real-world web servers will have more complexity that requires communication between concurrent processes affecting performance. I’m just trying to provide some simple base comparisons for a simple use case. Also, my knowledge of some languages is better than others; hence I might miss some optimizations here and there. So please don’t shout at me. If you think the code for a particular language can be improved out of the box to enhance concurrency performance, let me know. If you think this benchmark is useless, well, please suggest a better one :) Update: Despite the above disclaimer, people were still mad at me for using thread. sleep to simulate blocking and for using ApacheBench for this benchmark. I have since updated the post with more benchmarks using different tools. It’s still not scientific or the best way to benchmark concurrency. This is just me, doing experiments. If you have better ideas, please feel free to use the code and publish a follow-up or comment with your results, and I’ll update the post with it and attribute you. All the implementations used in this comparison can be found in the nosleep branch of this GitHub repository. Benchmarking conditions: These will be some of the conditions I’ll use for the benchmark.  The latest stable release versions of language/runtimes available are used, and as of writing, those are:     Rust: 1. 58. 1-Stable   Go: 1. 17. 6   Java: OpenJDK 17. 0. 2   Node. js: 17. 4. 0   Deno: 1. 18. 1   . NET: 6. 0. 100    Update: Thread. sleep has been removed from all implementations.  We will be using external dependencies only if that is the standard recommended way in the language.      latest versions of such dependencies as of writing will be used    We are not going to look at improving concurrency performance using any configuration tweaks Update: Many people pointed out that ApacheBench is not the best tool for this benchmark. I have hence also included results from wrk and drill We will use ApacheBench for the benchmarks with the below settings:     Concurrency factor of 100 requests   10000 total requests   The benchmark will be done ten times for each language with a warmup round, and the mean values will be used.    ApacheBench version on Fedora: httpd-tools-2. 4. 52-1. fc35. x86_64   Command used: ab -c 100 -n 10000 http://localhost:8080/    All the benchmarks are run on the same machine running Fedora 35 on an Intel i9-11900H (8 core/16 thread) processor with 64GB memory.      The wrk and drill clients were run from another similar machine on the same network and also from the same computer; the results were more or less the same; I used the results from the client computer for comparisons.    Comparison parameters: I’ll be comparing the below aspects related to concurrency as well.  Performance, based on benchmark results Community consensus Ease of use and simplicity, especially for complex use cases External libraries and ecosystem for concurrencyBenchmark results: Updated: I have updated the benchmark results with the results from wrk, drill and also updated previous results from ApacheBench after tweaks suggested by various folks. Update 2: There is a . NET 6 version in the repo now, thanks to srollinet for the PR. Benchmarks updated with the . NET results. Update 3: Rust using actix-web and Java undertow is now included in the wrk and drill benchmarks. The implementations were simplified to return just a string instead of doing a file I/O for these, and hence they are shown as a separate set. I started this series as a concurrency in languages experiment. Now, this feels like a benchmark of web server frameworks; while concurrency is an important aspect of these, I’m not sure if the results mean anything from a concurrency of the language aspect. Results from wrk: Benchmark using wrk with the below command (Threads 8, Connections 500, duration 30 seconds): 1wrk -t8 -c500 -d30s http://127. 0. 0. 1:8080 Update comparison of Go HTTP, Rust actix-web, Java Undertow, and . NET 6 The Go, Rust, and Java web server versions blow everything out of the water when it comes to req/second performance. If we remove it, we get a better picture as below.  Results from drill: Benchmark using drill with concurrency 1000 and 1 million requests Update comparison of Go HTTP, Rust actix-web, Java Undertow, and . NET 6 Benchmark using drill with concurrency 2000 and 1 million requests Update comparison of Go HTTP, Rust actix-web, Java Undertow, and . NET 6 Previous ApacheBench results with thread blocking: The average values for different metrics with a thread. sleep every ten requests across ten benchmark runs are as below: You can find all the results used in the GitHub repo Conclusion: Based on the benchmark results, these are my observations. Benchmark observations: Since recommendations based on benchmarks are hot topics, I’ll just share my observations, and you can make decisions yourself.  For the HTTP server benchmark using wrk, Go HTTP wins in request/sec, latency, and throughput, but it uses more memory and CPU than Rust. This might be because Go has one of the best built-in HTTP libraries, and it’s extremely tuned for the best possible performance; hence it’s not fair to compare that with the simple TCP implementations I did for Java and Rust. But you can compare it to Node. js and Deno as they also have standard HTTP libs that are used here for benchmarks. Update: I have now compared Go HTTP to Rust actix-web and Java Undertow, and surprisingly Undertow performs better, and actix-web comes second. Probably a Go web framework, like Gin, will come closer to Undertow and actix-web.  The Go TCP version is a fair comparison to the Rust and Java implementations, and in this case, Both Java and Rust outperforms Go and hence would be logical to expect third party HTTP libraries in Rust and Java that can compete with Go and if I’m a betting person I would bet that there is a Rust library that can outperform Go.  Resource usage is a whole different story, Rust seems to use the least memory and CPU consistently in all the benchmarks, while Java uses the most memory, and Node. js multi-threaded version uses the most CPU.  Asynchronous Rust seems to perform worst than multi-threaded Rust implementations.  In the benchmarks using drill, the Asynchronous Java version outperformed Rust and was a surprise to me.  Java and Deno have more failed requests than others.  When concurrent requests are increased from 1000 to 2000, most implementations have a very high failure rate. The Go HTTP and Rust Tokio versions have nearly 100% failure rates, while multi-threaded Node. js have the least failure and have good performance at that concurrency level but with high CPU usage. It runs multiple versions of V8 for multi-threading, which explains the high CPU use.  Overall, Node. js still seems to perform better than Deno.  Another important takeaway is that benchmarking tools like ApacheBench, wrk, or drill seem to offer very different results, and hence micro-benchmarks are not as reliable as ultimate performance benchmarks. Based on the actual use case and implementation-specific details, there could be a lot of differences. Thanks to Eamon Nerbonne for pointing it out.  Apache Benchmarks run on versions with and without thread. sleep doesn’t say much as the results are similar for all implementations, and it might be due to limitations of the ApacheBench tool. Hence as many people pointed out, I’m disregarding them. For more comprehensive benchmarks for web frameworks, I recommend checking out TechEmpower’s Web framework benchmarks With ApacheBench, as you can see, there isn’t any significant difference between the languages when it comes to total time taken for 10k requests for a system with considerable thread blocking, which means for a real-world use case, the language choice isn’t going to be a huge factor for concurrency performance. But of course, if you want the best possible performance, then Rust clearly seems faster than other languages as it gives you the highest throughput, followed by Java and Golang. JavaScript and TypeScript are behind them, but not by a considerable margin. The Go version using the built-in HTTP server is the slowest of the bunch due to inconsistent performance across runs, probably due to garbage collection (GC) kicking in, causing spikes. Also interesting is to see the difference between the multi-threaded and asynchronous approaches. While for Rust, multi-threaded implementation performs the best by a slight margin, the asynchronous version performs slightly better for Java and JavaScript. But none of the differences is significant enough to justify suggesting one approach over another for this particular case. But in general, I would recommend using the asynchronous approach if available as it’s more flexible without some of the limitations you might encounter with threads. Community consensus: The community consensus when it comes to concurrency performance is quite split. For example, both Rust and Go communities claim to be the best in concurrency performance. From personal experience, I find them relatively close in performance, with Rust having a slight lead over Go. The Node. js ecosystem was built over the promise of asynchronous concurrency performance, and there are testimonials of huge performance improvements when switching to Node. js. Java also boasts of real-world projects serving millions of concurrent requests without any issues; hence it’s hard to take a side here. Another general observation is that Rust was quite consistent in terms of performance across runs while all other languages had some variance, especially when GC kicks in. Simplicity: While performance is an important aspect, ease of use and simplicity is also very important. I think it’s also important to differentiate between asynchronous and multi-threaded approaches. Asynchronous: I personally find Node. js and Deno the simplest and easy-to-use platforms for async concurrency. Golang would be my second choice as it’s also easy to use and simple without compromising on features or performance. Rust follows it as it is a bit more complex as it has more features and needs getting used to. I would rate Java last as it requires much more boilerplate, and doing asynchronous programming is more complex than in others. I hope project Loom fixes that for Java. Multi-threaded: For multi-threaded concurrency, I will put Rust first as it’s packed with features, and doing multi-threading is easy and worry-free in Rust due to memory and thread-safety. You don’t have to worry about race conditions and such. I’ll put Java and Go second here. Java has a mature ecosystem for multi-threading and is not too difficult to use. Go is very easy to use, but you don’t have a lot of control over OS threads else I would rate Go higher than Java. Finally, there are multi-threading capabilities in Node. js and Deno, but they are not as flexible as other languages; hence I’ll put them last. Ecosystem: Rust has the best ecosystem for concurrency, in my opinion, followed by Java and Golang, which have matured options. Node. js and Deno, while not as good as others, offer a descent ecosystem as well. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:21,url:"https://deepu.tech/state-of-pattern-matching-java/",title:"The state of pattern matching in Java 17",body:"2021/12/23 - So what exactly is pattern matching?  The act of checking a given sequence of tokens for the presence of the constituents of some pattern    Wikipedia Or simply put, it’s a language feature where you can test for a specific pattern on a character sequence or a data structure. Pattern matching can be classified into two types.  Sequence patterns: pattern matching on character sequence or strings. Also known as our beloved Regular Expressions 😉 — I still wish I could write RegEx without cursing and looking up the syntax.  Tree patterns: testing for patterns on a data structure. This is what we are going to talk about today. Why Pattern matching?: Why do we need pattern matching? We don’t, to be honest! Pattern matching is not a requirement for a good programming language. Many of the most popular languages like JavaScript, Java, and Go get by just fine without it. But still, it’s a great feature to have due to these advantages it offers  Reduced cognitive complexity     Much more concise code and better readability.    More complex logic can be expressed with fewer lines of code. While not very important, it’s still a good thing.    Simpler to write and maintain.     Reduced reliance on reflection and casting, especially in Java.  Avoid bugs caused by pattern dominance and pattern non-exhaustiveness.      Pattern dominance is when a previous pattern supersedes another making it unreachable. A modern compiler should be able to catch it.    Pattern exhaustiveness is when the compiler warns you when you have not checked for all possible variants of a type, like having a case for all enum values or all subclasses of a type and so on.    Of course, these advantages depend on how a language implements pattern matching. So later, we will look at these from the perspective of Java. Pattern matching features: These are many languages that have great support for pattern matching. Rust and OCaml lead the pack here. On the JVM world, Scala also offers many of these pattern matching features. So when a language claims to have support for pattern matching, these are the features we expect:  Enum matching in switch statements — Most languages, including Java, already does this Match the value in switch/if statements — Common in any Turing complete language Match type in switch/if statements — Required for pattern matching Pattern matched variable assignments Null checks — Required in a language like Java Type guards Refined patterns — If we can match for data types, then its logical to expect refining the pattern further using the matched type, which acts as a type guard Pattern dominance and type exhaustion Partial/Nested/Compound type and/or value checks Shallow/Deep Position-based Destructured matchingIt’s not an exhaustive list but more of a general expectation to fulfill the previous advantages we saw. Pattern matching in Java: Unfortunately, Java is still a bit behind the curve when it comes to pattern matching. But fortunately, we already have most of the building blocks required to achieve most of the features we saw earlier.  Switch statements Switch expressions (Java 14) — As compared to the switch statements, switch expressions can return a value, have multiple case labels on the same line, and need to be exhaustive. And hence can be used for variable assignments and statements without fallthrough Pattern matching for instanceof (Java 16) — It can now pattern match data types, which means casting after a check is no longer required, and this can be used in if statements, assignments, and returns.  Sealed classes (Java 17) Pattern matching for switch (Java 17 preview - JEP 406)Now let us look at some examples of what is already possible to do as of Java 17 Pattern matching for instanceof: From Java 16 onwards, we can do pattern matching using the instanceof operator. It can be used on if statements, and we can use it as type guards on variable assignments and returns. This lets us use a variable without further casting after a type check. This is a new syntax added to the language. Here we can see pattern matching using the instanceof operator in action. A type and a variable name follow the operator, and we can access the variable without further casting. 12345678910111213// Beforeif (obj instanceof String) {  String s = (String) obj;  System. out. println(s. length());}// Afterif (obj instanceof String s) {  // Let pattern matching do the work!  System. out. println(s. length());}We can also use pattern matching as a type guard in returns and variable assignments. See how concise the code is when we do the return using a type guard instead of casting the type. 123456789101112131415// Beforepublic boolean equals(Object o) {  if (!(o instanceof Point))    return false;  Point other = (Point) o;  return x == other. x    &amp;&amp; y == other. y;}// Afterpublic boolean equals(Object o) {  return (o instanceof Point other)    &amp;&amp; x == other. x    &amp;&amp; y == other. y;}Here is a variable assignment using a type guard. Again the code is much nicer with pattern matching. 123456789// Beforevar x = o instanceof Point ? ((Point)o). x : 0;System. out. println(x);// Aftervar x = o instanceof Point p ? p. x : 0;System. out. println(x);For a realistic use case, we could do something like below, when we want to do different logic based on the type, using the instanceof operator. But that’s a lot of if-else and cognitive load. 12345678910111213static String formatter(Object o) {  String formatted =  unknown ;  if (o instanceof Integer i) {    formatted = String. format( int %d , i);  } else if (o instanceof Long l) {    formatted = String. format( long %d , l);  } else if (o instanceof Double d) {    formatted = String. format( double %f , d);  } else if (o instanceof String s) {    formatted = String. format( String %s , s);  }  return formatted;}Pattern matching for switch: But with the new preview feature in Java 17, we can do pattern matching for data types in switch cases as well. For Both, switch statements and switch expressions. This will let us rewrite the previous code using a switch expression like below. As you can see, this is also a new syntax. And it’s similar to the one added to the instanceof operator. 123456789static String formatter(Object o) {  return switch (o) {    case Integer i -&gt; String. format( int %d , i);    case Long l  -&gt; String. format( long %d , l);    case Double d -&gt; String. format( double %f , d);    case String s -&gt; String. format( String %s , s);    default    -&gt; o. toString();  };}This has Reduced cognitive complexity, and the syntax is closer to most other languages with pattern matching. With this, the compiler can warn us when pattern dominance occurs, as generic types should always come after specific types. Since this is a preview feature, the syntax might change in future versions. I hope not, as I find this syntax nice in Java. We can also do null checks in these switch cases, which makes pattern matching more useful in the case of Java. 123456789101112131415161718static String formatter(Object o) {  return switch (o) {    case null   -&gt;  Oops ;    . . .    case String s -&gt; String. format( String %s , s);    default    -&gt; o. toString();  };}// orstatic String formatter(Object o) {  return switch (o) {    . . .    case String s    -&gt; String. format( String %s , s);    case null, default -&gt;  Oops ;  };}Type Guards &amp; pattern refinement: As we saw earlier, Type guards are already supported for instanceof operator from Java 16 onwards, and Java 17 preview adds that for switch cases as well. This means we can rely on the type guards to refine the patterns further to have conditions, relations, and value checks. 1234567891011121314151617static void test(Object o) {  if ((o instanceof String s) &amp;&amp; s. length() &gt; 3) {    System. out. println(s);  } else {    System. out. println( Not a string );  }}// Orstatic void test(Object o) {  switch (o) {    case String s &amp;&amp; (s. length() &gt; 3) -&gt; System. out. println(s);    case String s           -&gt; System. out. println( Invalid string );    default              -&gt; System. out. println( Not a string );  }}While it’s not as flexible as in Rust or OCaml, it’s a good start, in my opinion. Pattern exhaustion with Sealed classes: We need to understand another new feature in Java 17, Sealed classes, to understand pattern exhaustion. Sealed classes: A sealed class lets you control which class can extend it. The same applies to sealed interfaces as well. Permitted classes can be defined either as simple inner classes or in separate class files using the new permits keyword. 1234567891011public abstract sealed class Shape permits Circle, Rectangle, Square { . . . }// Orpublic abstract sealed class Shape { final class Circle extends Shape { . . . } final class Square extends Shape { . . . } final class Rectangle extends Shape { . . . }}A sealed class imposes three constraints on its permitted sub-classes.  A Sealed class and its permitted subclasses must be in the same module or the same package in case of an unnamed module.  Every permitted subclass must directly extend the sealed class.  Every permitted subclass must use a modifier to describe the propagation of sealing. Allowed modifiers are final, sealed, and non-sealed.      Final subclasses cannot further be extended. Hence the class hierarchy ends with them. Records are implicitly final and hence do not need the keyword.    A Sealed subclass can further restrict class hierarchy with its own permitted subclasses, and the same three rules apply all over again for them   A Non-sealed subclass is a bit different. As the keyword suggests, it reverts to being open for extension by any class and breaks the class hierarchy rules set by the parent sealed class. Non-sealed sub-classes are still valid children and work well with other concepts. We will see that later.     A fun fact, non-sealed is the first hyphenated keyword in Java. Here is how it would look in practice. Sealed classes can also be used with Records. Here the Shape interface is sealed, and it permits Circle, Rectangle, Square, and WeirdShape as subclasses. Circle is implicitly final as it’s a record. Square is declared as final. And hence both cannot be extended further, and their hierarchy ends here. Rectangle is declared as sealed and permits only TransparentRectangle and FilledRectangle as subclasses. Hence Rectangle’s class hierarchy can extend further, and TransparentRectangle and FilledRectangle can even define their own hierarchy. WeirdShape is declared as non-sealed and hence can be extended by any other class or record. Here the class hierarchy is wide open, and subclasses of a WeirdClass do not have to follow the rules of a sealed class. All instances of subclasses of WeirdShape will also be an instance of WeirdShape, and hence any code that checks for exhaustiveness of subtypes of Shape will still be valid. 12345678910public sealed interface Shape  permits Circle, Rectangle, Square, WeirdShape { . . . }public record Circle(int r) implements Shape { . . . }public final class Square implements Shape { . . . }public sealed class Rectangle implements Shape  permits TransparentRectangle, FilledRectangle { . . . }public non-sealed class WeirdShape implements Shape { . . . }A sealed class can be abstract and can have abstract members, and similarly, sealed interfaces can have default implementations and so on. But abstract subclasses should be sealed or non-sealed and not final. Extending a non-permitted class with a sealed class will be a compile-time error.  Another fun fact: A combination of sealed classes and records is nothing but algebraic data types. (type formed by combining others, ex, Tuples, Records, unions) Pattern exhaustion: Remember pattern exhaustion we talked about? The java compiler cannot check for all possible variations for normal classes and hence would require us to add a default case to avoid missing paths in a switch expression, as switch expressions do not fall through and must be exhaustive in Java. That does not apply to switch statements as we can choose not to have a default case for them as they fall through. But with sealed classes, the compiler knows exactly what the expected variants are. We can use this for exhaustive pattern matching. In that case, the compiler will warn when we miss a variant in the check without adding a default case. This is a much less error-prone and clearer approach. Even if the subclasses are non-sealed, the compiler will still do the exhaustiveness check. See this example; if we use the same sealed classes we saw earlier in the first sample, we will get a compile-time error as we are not checking all variants of Shape, and there is no default case. In the second example, however, there is no error even without a default case as the compiler knows all possible variants of Shape have been accounted for. If we remove the sealed keyword for Shape, then the second sample will have the same error as there is no default case, and the compiler doesn’t know about all variants of Shape. 12345678910111213141516Shape rotate(Shape shape, double angle) {  return switch (shape) {  // this will be 'switch' expression does not cover all possible input values error    case Circle c  -&gt; c;    case Square s  -&gt; shape. rotate(angle);  };}Shape rotate(Shape shape, double angle) {  return switch (shape) {    case Circle c   -&gt; c;    case Rectangle r -&gt; shape. rotate(angle);    case Square s   -&gt; shape. rotate(angle);    case WeirdShape w -&gt; shape. rotate(angle); // still exhaustive    // no default needed!  };}Partial patterns and destructing: There is a candidate feature to add preview for destructing/deconstruction syntax for the instanceof operator, so logically at some point, that should extend to the switch syntax as well. There are also talks about improving the feature further to add support for primitives in switch case patterns and to declare how it should be deconstructed at the class level so that normal classes can also be deconstructed. Record Patterns &amp; Array Patterns (Preview - JEP 405): So far, the only solid thing is the candidate for deconstruction in instanceof operator. With this proposal, we should be able to deconstruct Records and Arrays. This is closer to how it works in Rust, for example. Look at how Point is deconstructed to its members, and we can directly use the members with type guard 1234567record Point(int x, int y) {}void printSum(Object o) {  if (o instanceof Point(int x, int y)) {    System. out. println(x+y);  }}Deconstruction can be nested as well but doesn’t have any facility for ignoring members like in other languages yet. 123456789record Point(int x, int y) {}enum Color { RED, GREEN, BLUE }record ColoredPoint(Point p, Color c) {}void printSum(Object o) {  if (o instanceof ColoredPoint(Point(int x, int y), Color c) {    System. out. println(x+y);  }}Array deconstruction is also proposed, and syntax is a bit similar to array deconstruction in JavaScript. Thankfully at least elements at the end can be ignored in this case; it would be pretty useless otherwise 😉 See how the string array is deconstructed to the first two elements with the rest ignored. The syntax is closer to the new array syntax as well. 12345static void printFirstTwoStrings(Object o) {  if (o instanceof String[] { String s1, String s2, . . . }){    System. out. println(s1 + s2);  }}Nested deconstruction is also possible for a mix of arrays and records. See how an array of Points are deconstructed here. And yes, var should work instead of type as well. 12345static void printSumOfFirstTwoXCoords(Object o) {  if (o instanceof Point[] { Point(var x1, var y1), Point(var x2, var y2), . . . }) {    System. out. println(x1 + x2);  }}Of course, it’s a baby step and not as powerful as deconstruction in Rust or JS, but I hope we get there, and even this baby step would make pattern matching much more powerful. Hopefully, this would be extended to switch as well. That would make pattern matching in Java closer to complete, with only normal classes remaining to be accounted for. If the normal classes can specify how they should be deconstructed, that issue will be solved, and we will have powerful pattern matching in Java. ConclusionSo based on what we saw so far, the state of pattern matching in Java is as below.  Enum matching in switch statements ✅ Match type/value in switch statements ✅ Match type/value in if statements ✅ Pattern matched variable assignments ✅ Null checks ✅ Type guards ✅ Refined patterns ✅ Pattern dominance and type exhaustion 🆗 Partial/Nested/Compound type and/or value checks 🆗 Shallow/Deep Position-based Destructured matching 🆗Most of the basic requirements are met with the instanceof operator pattern matching, and the Java 17 preview makes many other features possible. Of course, type exhaustion only works for sealed classes, and refined patterns are still quite basic, but the majority of the features for proper pattern matching are already available with the preview. To summarize, these are the current and future limitations as far as we can see JDK 17 preview  No deconstruction No nested patterns Type exhaustion is only for sealed classes and enums Pattern refinement is limited Still a preview featureJDK 18+ (maybe)  No deconstruction for classes and in switch cases No nested patterns for classes and in switch cases Type exhaustion is only for sealed classes and enums No feature for ignoring don’t-care patterns during deconstruction No named patterns Still a preview featureIn a future version of Java, some of these limitations might be removed, at least for instanceof operator. We probably would still have to wait a few years before we can actually start using full-fledged pattern matching in production. I’ll update this post when more features are released. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Meme from makeameme. org "},{id:22,url:"https://deepu.tech/my-beautiful-linux-development-environment-2021/",title:"My sleek and modern Linux development machine in 2021",body:"2021/11/01 - One of my most popular posts of all time was when I wrote about my beautiful Linux development machine in 2019. Since I got a new machine and a new setup, it naturally calls for a follow-up. By now, I’m a die-hard Linux user. It’s been five years since I’m using Linux as my primary OS. I would take a Linux machine any day over macOS or Windows. I own a Windows gaming PC and a Macbook pro from work. Still, they are nowhere near the Linux experience, personally for me. My primary driver is my Linux laptop, and I can’t be any happier with it (well, I can, I’ll come to that at the end of the post). So a Dell Precision from 2016 was my primary device until last month, and it is still going good. Fedora runs smoothly on it for most parts. There is some minor glitch now and then but nothing that bothers me. What is a Linux experience without glitches, right? 😉 But that setup is almost five years old now, and sometimes it shows from the increased CPU load and fan noise. Applications like Chrome, Slack, and VSCode aren’t becoming any lighter in CPU and memory usage. So when I had a chance to get a Linux machine from my new job, thank you Okta, I went all in and got myself a Dell XPS 15 9510. So here I’m detailing essential aspects of my setup and hoping it will inspire more people to use Linux. I also wrote about the state of Linux on the desktop. Check it out if you are interested in Linux. You might be surprised. So when I was choosing a new laptop, I narrowed my options down to either Dell XPS 15 or Tuxedo Pulse 15. Form factor, CPU, and memory was the most important factors for me since my work involves working with Docker and Kubernetes, polyglot development and travel. In the end, I chose Dell since Tuxedo had some issues with multiple monitors via USB-C as it didn’t have thunderbolt support. This is not just my work laptop; it’s my primary machine for all of the below.  Java, Rust, JS, TS, Go and web development (and everything in between) Running multiple web applications locally Running Docker containers and local Kubernetes clusters Kubernetes, Terraform, CloudFormation development and deployments VirtualBox for Windows testing and other VM stuff Azure, AWS, and GCP deployments using required CLI tools Heavy browser usage E-mail, chat, and video conferencing Plex media server Writing, presentations, and illustrations YouTube and Social mediaMachine configuration: The configuration of the machine is also quite crucial for any development setup. I selected a custom configuration from Dell to get the best possible setup at that time. It’s not cheap, but my company, Okta, provided a handsome budget. But I do think it can be a lot more competitive in pricing like Tuxedo and the like. This, in my opinion, is one of the best Laptops for developers. So here is what I have.   Processor: Intel® Core™ i9-11900H CPU @ 4. 9GHz (8 cores, 16 threads) Memory: 64GB, DDR4-3200MHz SDRAM, 2 DIMMS, non-ECC HDD: 1TB M. 2 NVMe SSD Graphics: NVIDIA GeForce RTX 3050Ti with 4 GB GDDR6 memory &amp; Intel® UHD Graphics Wireless: Killer Wifi 6 (2x2) + BT 5. 1 Keyboard: English QWERTY US, backlit Display: 15. 6” FHD 1920x1080, matte, non-touch Battery: 6-cell (86Wh) Ports: 2 x Thunderbolt 4 USB-C, 1 x USB-C 3. 2 Gen2, I use the laptop mostly with my home office, which consists of a 34 inch and a 27 inch monitor. A Keychron K2 keyboard, Logitech MX Vertical mouse, Razer Kiyo webcam, Elgato Keylight, Fifine mic and a Dell Thunderbolt dock that drives all my peripherals and monitors while powering the laptop all using a single USB-C cable. I have an old Sony Bluetooth headset and a cheap custom amplifier running a pair of old bookshelves and a subwoofer for audio. Operating system and desktop environment: Switching to Fedora was a great decision I made sometime in 2017, and I’m so happy with it. Updates are butter smooth. I don’t have to worry about stuff breaking all the time from rolling releases. At the same time, new stuff lands every six months, and packages are updated frequently, keeping everything running smoothly. For a developer, that is an excellent combination, in my opinion. So naturally, I choose to stick to Fedora; why fix something that isn’t broken, right? So I’m running Fedora 34. I have been using Gnome for the past five years, but recently KDE caught my eyes, and I decided to give it a shot. So I went with the KDE spin of Fedora. KDE has been great so far. I love the amount of customization you can do on it, and top of all, it looks super sleek, and there is an excellent selection of themes and styles to choose from. I’m definitely not missing Gnome. What good is a desktop without a nice theme, right? KDE has a great collection of themes, and I went with Ant-Dark theme for now, and it looks gorgeous with transparency and blur effects. While KDE themes are self-sufficient with icons, cursors, and lock screens, I switched from default icons to icons from KDE-Story theme.  I use many plugins in Gnome, but in KDE, all those came out of the box, like clipboard manager, notepad, system monitor, etc. Another reason I’m enjoying KDE. With the new installation, I also went with the BTRFS filesystem, which is fantastic. Snapshots and backups are so fast. Though setting up snapshots and backup was a challenge initially. Development tools: These are mostly objective choices and really don’t matter as long as you are comfortable with your chosen tools. Below are my choices for some of the important categories for development. Shell: This is one of the most important for a developer. I use ZSH along with the awesome Starship prompt as my shell. I tried to switch to Fish, but it was quite annoying due to its difference with bash/ZSH, and I gave up on it. I went with the default theme and settings from Starship as it was very close to my earlier setup with Oh My ZSH. I also use zsh-autosuggestions and zsh-syntax-highlighting plugins for ZSH and fzf for search. Terminal: What good is a nice shell without a good terminal. Fortunately, the default KDE shell, Konsole, is great, and KDE also has Yakuake, which is one of the best terminal applications out there. It has workspaces, tabs, split windows, Quake mode, etc. I mostly use Yakuake, which is bound to Ctrl+` IDE: Visual Studio Code - My go-to editor. I love it. I use VSCode for web development, Rust, Go, JS development, DevOps, and everything other than JVM languages. A VSCode setup is never complete without some good plugins. Here are the plugins that I’m using. You can run the script to install those. For Java development, I use IntelliJ IDEA Ultimate. Other notable development tools I use are GitKraken for Git repo management, Beyond Compare for code comparisons, VirtualBox, NVM for Node. js version management and SDKMan for JDK version management. Toolchains: Node. js and NPM, Rust toolchain, Golang, JVM, Deno, Ruby toolchain, and Python toolchain. I also ran sudo dnf groupinstall  Development Tools   Development Libraries  to install the toolchains required for compiling Linux applications. DevOps: Docker, kubectl, Terraform, k3d, and so on Productivity tools: Productivity tools are also quite necessary, and below are my choices. Browser: Google Chrome is my primary browser. I also use Firefox sometimes. I use the Bitwarden plugin for password management. E-mail: I use Mailspring as my e-mail client. It’s a fairly decent mail client with nice themes and a simple UI. Office suite: I mostly use Google Docs, but when I have to work on something on my desktop I use LibreOffice, which is a good office suite and even handles Microsoft Office &amp; Keynote formats. Communication: Of course I use Slack and for video conference I use Zoom. I also have Telegram and Signal installed. Screen capture: I use this nifty tool called Peek for screen recording and Flameshot for screenshots. Streaming &amp; Media: Streaming: I think OBS Studio is the default choice here and is one of the best options for streaming and screen recording. Image editing and processing: Gimp for photo editing, Inkscape for vector editing, and Draw. io for illustrations. KDE also has some default apps for photo management, which is quite good. Video editing: Kdenlive is a great choice for video editing on Linux. Video player: SMPlayer is my go-to video player. Conclusion: There are many other small and nifty utilities that I use; most are command-line utilities. There are some notable mentions like Timeshift, which is nice for backing up your machine. Of course, not everything is perfect in the Linux world, but it is the same with every OS from my experience. Before switching to Linux, I was a long-time Windows user, and last year, I spent more than a year using macOS now and then, so issues are everywhere. So like every Linux user, I have from time to time messed things up(With great power comes great responsibility, Peter). There are some minor annoyances with this new setup, but there is nothing that bothers me much. Some of the most annoying issues I have currently are below, and I’ll probably fix them if it bothers me.  New Intel platforms do not support deep sleep, and hence laptop does lose power over time, even in Idle.  Hibernation doesn’t work out of the box. It’s easy to fix, but at the moment doesn’t bother me.  Some random crashes from KDE settings. Not a big deal as it’s a rare occurrence.  KDE remembers open apps on a restart, but when on multiple monitors, it doesn’t remember which screen it was in.  Biometrics is a bit tricky in Linux. The built-in fingerprint sensor doesn’t have a proper driver for Fedora yet, there is one for Ubuntu, and I probably can get it working. For face recognition, there is Howdy, but for some reason, I couldn’t get it working and hence probably need to do some digging. I hope you find this inspires you to give Linux a chance. If you have any questions, please ask in the comments. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:23,url:"https://deepu.tech/what-is-developer-experience-and-why-care/",title:"What is Developer Experience and why should we care?",body:"2021/07/16 - Information Technology (IT) is undoubtedly one of the most important industries today and one that is ever-growing. Every company is becoming an IT company these days. From Taxis to food delivery to banking, every industry is dominated by companies that are IT companies first and domain second. With that growth, the demand for software and tools used by other developers also grows. As an industry it took us some time to realize the importance of user experience (UX), you will understand what I’m talking about if you have tried using the internet or any software before the 2000s, but fortunately, we took notice, and today there are entire departments dedicated to user research and UX in software development. User experience matters for any software, but if your primary consumers are developers, then there is something else that matters more than UX. Its developer experience (DX) as developers make IT possible. What is Developer Experience?: It is the overall feeling that a developer gets when using a technical product in her/his development workflow. It is akin to UX but from a developer’s perspective. Let’s take an example for the sake of non-developer folks out there.  Let’s say you are building a cool and fancy product that lets developers add an image gallery to their applications, as part of the product you provide:  an API, to get random optimized images from your service; a JS SDK to add this easily on WordPress sites; a web application to manage the images. Now for a developer who would use this product the DX is going to be the sum of:  The experience using the API, like:     how easy it was to onboard   how easy it is to integrate into their app?   simplicity of the API and the resulting code   learning curve of the API   how informative are error messages?   does it follow known standards and structure?   how easy it is to debug.     Performance of the API.  Documentation of the API and product.  The ease of use of the JS SDK if they are using it.  The user experience of the web app when managing images. And this experience determines if the developer is going to consider using your product for the next project or not. We could also loosely measure DX as the inverse of the amount of frustration a developer has when using a product. Sometimes these frustrations could be outside of your control but regardless it is going to affect the DX as after all we are human and our emotions and feelings bound us.  Of course other factors like features, pricing, sales, marketing, and so on will get you through the door, but good DX is what will keep you in the room. Of course, there are more things that can be done in the above example to make DX even better, like providing example applications, video tutorials, blog posts showing various use cases, CLI tools for debugging, and so on. Why should you care about Developer Experience?: You should care about the DX of your product for the same reasons you care about the UX and some more. If you are a developer, just think of what kind of experience you would want when using a similar product. A good DX also shows empathy on your part for your primary users. Developers are a very opinionated bunch. We love our opinions and we love to defend our favorite language, technology, and tools. Heck, we are even ready to go to war over something as trivial as tabs vs spaces. So if your product has great DX the developers using it will love it and will evangelize and defend your product to the death. You might even gain a community of ardent supporters for your product that no amount of marketing can get you. But if the DX is bad, they are going to badmouth your product. If you are a developer, I think you know what I’m talking about and I’m pretty sure you have done this a lot. Another reason for focusing on DX is that it will make marketing and sales much easier as you have less friction with your end-users and fewer things that you need to convince them about. The recent surge in the interest for developer advocacy has also helped to bring to the limelight the importance of DX and without good DX there is not much you can do about developer advocacy and evangelism. How can we have a great Developer Experience?: A product with great DX helps a developer to get up and running quickly and reach her/his goal with minimal frustrations. So let’s see what are some of the common things that could help make great DX. Please note that this is not an exhaustive list and there are many more things that could help, depending on the specific product/use case. If you are building APIs:  Stick to known and highly adopted standards and conventions. Do not reinvent the wheel or try to come up with new fancy conventions as it will increase the learning curve and make it harder for developers to onboard.  Provide good error handling. Adopt something like RFC 7807 to provide a consistent and easy-to-use error handling for your API. Errors are unavoidable but making developers scratch their heads when it happens is avoidable to an extend.  Provide consistent and easy-to-use documentation. Provide an OpenAPI or RAML file that describes your API and the endpoints. Also, provide easy-to-use interactive online API documentation like Swagger if possible.  Provide SDKs and libraries for most popular languages and frameworks. If you are building development tools/products:  Good UX, which is tailored for developers. Like providing high contrast options. Good keyboard navigation, consistent with an industry standard.  Customizability, did I mention developers are an opinionated bunch? Easy to install on different platforms. Support different OSes. Provide multiple installation methods, especially, support the most popular installation method/package manager used by the ecosystem relevant to your product.  Easy-to-use and well documented. If you are building SDKs/Libraries/Frameworks:  Consistency and following industry standards and conventions. Avoid reinventing the wheel without a solid reason.  Play well with other SDKs/Frameworks in the ecosystem.  Provide inline code documentation wherever possible. Developers appreciate not requiring to leave their IDE.  Provide great documentation, examples, tutorials, interactive learning, and so on.  Easy to use error reporting system.  Open source when possible.  Avoid gatekeeping as much as possible. No registration-only webinars and no white papers that ask for my mother’s favorite color. General:  In general, always ask this question, how can my product make a developer’s day better?  Make sure the product does exactly what it claims to do.  Make sure the product is reliable and performant otherwise good DX will not cut it.  Focus on easy-to-use self-service rather than traditional support channels. Developers hate bureaucratic processes. Provide transparent support channels for those edge cases but most developers would prefer self-service if available rather than talking to a support person.  Simple onboarding. Avoid having to go through sales or other channels just to get started with development or for trying out your product.  Provide modern tooling or make your stuff compatible with modern tooling. Nobody wants to download and set up something from the 80s to run your software.  Make trying out your product as easy as possible.  Make documentation easy to find and navigate. Avoid having to jump through hoops to find documentation.  Make developer resources easy to find. Make a developer-focused section on your company website as a landing point for developer resources.  Avoid jargon-rich fancy marketing as most of us can see right through it. Keep it simple and to the point.  Build developer advocacy rather than technical evangelism.  Build a community around your product. Conclusion: We are slowly transitioning to an era where the importance of developers is being recognized and their influence on decision making is no longer something companies can take for granted. This is very clear from the fact that more and more companies are building developer relationship teams and hiring developer advocates rather than marketing evangelists. In this crowded space, being developer-focused used to be a differentiator but things are going towards the same situation that happened with UX where it became a must have rather than a good to have. The same will happen for DX as well and if you are building a product that is going to be used by developers, then you should start caring. Building developer experience and developer advocacy don’t happen overnight. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Nubelson Fernandes on Unsplash "},{id:24,url:"https://deepu.tech/languages-i-like-and-why/",title:"The programming languages I like and why I like them",body:"2021/07/09 - Being a polyglot developer is fun. You are not married to a single language/ecosystem and you have a diverse tool-belt to choose from based on the issue at hand. But still, you are going to have favorites based on your experience with different languages and their ecosystem. In my career, spanning 11+ years, I have worked with many programming languages. The first programming language I encountered was Basic when I was in school, but I won’t count that since I didn’t pay any attention and I didn’t understand it back then. Later when I was in college I had a mandatory computer science class, I was doing Electrical and Electronic Engineering, and there I first came across C, C++, and Java. But again It was just basics and I wasn’t very interested and I just studied enough to clear the paper. Later, I was running a blog about motorcycles and I started dabbling with WordPress, Flash, ActionScript, and JavaScript. The first programming language I actually learned with an interest was Scheme, It was a training language used in my first job as an IT Intern/Trainee and that got me hooked and in few months I started learning Java and JavaScript. I became quite good at Java and JS and really was enjoying working with those and If you have asked me back then, I would have said that Java and JS were the greatest languages out there. So for a pretty good part of my career, I was coding in Java and JavaScript and later TypeScript. I did work with a bit of Groovy, Python, and Scala but I didn’t like those languages much and I had no interest in learning any new languages at that point. A few years ago when Golang was all the rage, I moved to a project that was built in Go and hence started looking into Go. It was extremely easy to learn and I really liked working with it (for a while). Rust was quite new at that point and I wanted to try it out but got around to it only a year later and I was in love with it. In the past two years, I also worked with Kotlin, PHP, Ruby, and C# as well occasionally. But I wasn’t very impressed with Ruby and PHP. So without further adieu, these are my favorite programming languages in order. 1. RustRust is currently my most favorite language. It’s so much fun to work with and I love the challenge of rethinking how you write code. You know, as they say, it sparks joy when you work with it. Rust is truly a modern and powerful programming language. #java: I&#39;ll provide you memory safety but you must pay performance penalty for abstractions and the runtime#C++: I&#39;ll provide you zero cost abstractions, but you must pay with memory safety#Rust: hold my 🍺 &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) July 8, 2021Rust would be my go-to for systems programming, embedded, CLIs, OS tools, WebAssembly, and so on. I guess the only place I won’t use Rust would be serious monolithic web application development as the ecosystem is not mature for that yet. I wrote in detail about what I like and dislike about Rust in this post, but I’ll summarize it here for the casual skimmers. 😍 Likes:  Safe by default: Rust is memory safe, thread-safe, type-safe, and null safe Zero cost abstractions: Write in any programming style without worrying about the added performance penalty Awesome concurrency: Great support for multi-threading, parallelism, and asynchronous programming Great tooling out of the box: Cargo, Clippy, Rustfmt, and so on. It’s the best tooling ecosystem I have come across. Testing, building, bootstrapping, benchmarking, linting and more included out of the box.  Immutable by default: You need to declare mutable operations explicitly Built-in functional programming abstractions: Monad like iterators, optional, chaining, and so on Macros: Powerful metaprogramming that can be used to define custom language features, code reusability, and so on Excellent community: Fast-growing, active, and really supportive community Superfast, as fast as or sometimes even faster than C/C++ No heavy runtime: No garbage collection and so on. There is a very small runtime for panics and stuff, but IMO it’s negligible Excellent language features: Pattern matching, generics, iterators, traits, expressions, and so on Hands down the best compiler out there. You have to try it to appreciate it.  Fast growing and being widely adopted: Rust is going places. There are a lot of big names(Google, Microsoft, AWS, Apple, and so on) adopting it and hence cementing its place. It’s also finding footing outside of systems programming like embedded, web assembly, web development, game development, and so on. Once the ecosystems for different use cases mature, I see great potential for Rust to be a great general-purpose language without any major compromise.  Native images: It’s so easy to build native images with Rust, no special setup required. Cargo supports building for your favorite platform out of the box. 😩 Dislikes:  Complexity: Being an amalgamation of many languages, Rust does feel more complex than many other languages. I especially would have preferred to not have multiple ways to do the same thing.  Learning curve: Rust is not the easiest language to learn, especially if you are a beginner or if you are used to working only in one language like Java, C#, or Python. But if you are polyglot and already familiar with a few languages like C/C++, Java, TS, and Go, you will feel quite at home with Rust. There are some advanced concepts like borrowing and lifetimes that take some getting used to and practice to grasp.  Ecosystem is not mature enough for many use cases. I would love to use Rust for web application development but the ecosystem for that is still very young and you are gonna have a hard time compared to something like Java or C#2. JavaJava isn’t cool like Rust or feature-rich like JavaScript but it makes up for all that by having one of the most stable, battle-tested, and rich ecosystems. So if I would start my own company with a web application as the product, I would 100% go with JVM on the backend with either Java or Kotlin, and of course I will build it using JHipster. If you like Java, it’s hard not to like Kotlin. It feels like a modern version of Java and building Android apps using Kotlin was a nice experience. Unlike Scala, Kotlin doesn’t go overboard with all the complexity and implicit stuff. I might choose Kotlin over Java if building a new web app and if the team has experience with Kotlin. Also, Java is close to my heart as it’s the programming language that jump-started my career. Here is what I like and dislike about Java 😍 Likes:  Ecosystem: The biggest strength of Java is its ecosystem. There are great frameworks and libraries for everything you can think of and most of these solutions are extremely stable with a lot of community and support. In reality, the ecosystem is whats keeping Java popular and growing.  Experienced community: Java has a very mature and experienced community so even if you are a beginner there is a lot of resources to help you Great tooling: There are a lot of great tools out there for Java. IDEs like IntelliJ, Eclipse, NetBeans offer some of the best developer experiences, and build tools like Gradle and Maven are so powerful and feature-rich. There are also profilers, linters, and so on.  Built-in functional programming abstractions: Monad like iterators, streams, optional, functional interfaces, and so on Fairly easy to learn: Java is not very complex and hence is fairly easy to learn and get started even with its boilerplate. 😩 Dislikes:  Boilerplate: Java needs too much boilerplate. There is no nice way to say this. Compared to other languages in its league, Java feels too verbose. Lambdas have made it more digestible but there are other JVM languages like Kotlin or Scala that have got this part right. I really wish Java gets there one day where you don’t have to write 20 lines of code to read a file.  Not so modern: Language features in Java are, let’s just say it doesn’t spark joy, it’s improving but if you are used to few other languages, then there is a lot to wish for, and compared to its peers Java feels old even though its just as old as JS, Ruby or Python Baggage: Java has great backward compatibility but that also means baggage. there is a lot of such baggage in Java and it’s holding the language back in my humble opinion.  Virtual machine: JVM was a great idea when Java was introduced as portability was not easy back then but in today’s IT landscape containers and native images have made portability easy and that makes a Java Virtual Machine redundant. JVM still provides a lot of valuable features but it also takes up resources and space whereas languages like Go or Rust can just build tiny native binaries that can be deployed using docker to get better performance with less resource usage than a Java app running on JVM. 3. TypeScript/JavaScriptMost Java developers hate JavaScript but I have a love-hate relationship with it. JavaScript is one of the quirkiest languages around and like it or not it’s the de-facto for the web and it gets the job done. I like TypeScript a lot more than JavaScript and find it way nicer to work with than JS. So TS and by extension JS would tie with Java for the second spot for me. TS/JS is still my go-to for quick scripting and building client-side apps and mobile apps (ReactNative/PWA) I wrote in detail about what I like and dislike about JS in this post, here is the summary 😍 Likes:  Flexible: The dynamic nature of JS/TS makes it extremely flexible and powerful. It’s also very forgiving language. If you know what you are doing, there is literally nothing you cant get done. Of course, the flexibility comes with its own price.  Community: JavaScript has the biggest community among programming languages. It’s active, mature, and thriving. It’s a huge strength as it helps beginners and experts alike in solving problems and improving the ecosystem on the whole.  Ecosystem: JS has a huge ecosystem with a lot of libraries and frameworks. You will find anything you could imagine as a library. The ecosystem is so big that it has started to become an issue in terms of “yet another framework syndrome”.  Asynchronous programming: JS has one of the best ecosystems for asynchronous programming. It’s so easy to write async code with Promises and async/await and it has become the de-facto way of programming in JS these days.  Great tooling: JS has great tooling, thanks to NodeJS and its ecosystem. IDEs like VS Code, Atom, Webstorm, and so on provide great JS support. There are also so many wonderful tools like Webpack, Rollup, Jest, and so on.  Functional programming: JS is multi-paradigm and has a lot of support for functional programming. It’s a great fit for functional programming.  Easy to learn: JS is a simple language to learn and get started. It’s one of the most beginner-friendly languages and communities out there. Of course, there is complex stuff in JS but you don’t need to know that to get started.  NodeJS: NodeJS was the best thing to happen for software development. It not just revolutionized the JS world but also inspired other languages to take note and be more developer-friendly and build easy-to-use tooling and ecosystem. You can see this inspiration in Rust, Golang, and so on.  TypeScript: TS is another best thing that happened to JS. It provides an answer to a lot of JS issues that bother people and makes it possible to use JS in a more developer-friendly way on a huge codebase. 😩 Dislikes:  Fragmentation: This is the biggest issue with JS. There are standards and there is vendors and that’s the perfect recipe for disaster. Most of the time a JS developer spends used to be to make it work across different browsers and platforms. The demise of IE has made this better but hey I heard Safari is the next IE so there is that.  Error-prone: This is the side effect of being too flexible. JS code is extremely error-prone and unmanageable in huge code bases. TypeScript makes this much much better but still, it’s so easy to shoot in your own foot with JS.  Framework overload: Too many frameworks for the same thing, too many libraries for the same thing, and libraries for doing extremely trivial stuff like left-padding :( Competition is good but IMO what we have in the JS ecosystem is just too much. It causes mental drain and unnecessary fragmentation in the community.  Over-engineering in the ecosystem: I have been doing JS for over 11 years now and I’m noticing a clear trend of over-engineering in the ecosystem. Look at the JS build tooling for example it has been over-engineered to the extend that setting that up feels like a project in itself. 4. GoGo is the simplest language I have worked with and the easiest to learn. I like Golang for quickly putting something together and Go code is quite easy to read. But I would choose Rust over go for large codebase since Go becomes annoying due to its limited features after a while especially on large codebases. IMO go is still ideal for building small system utilities and microservices. I wrote in detail about what I like and dislike about Go in this post, here is the summary 😍 Likes:  Simplicity: Go is extremely simple. You could technically learn it in a day or two. It’s easy to get started and be productive in Go. It’s also easy to read and debug as there is only one way of doing anything so you know what is going on without having to ask the person who wrote the code. It’s the easiest language for beginners as well.  Built-in tooling and style guide: Go provides built-in tooling for all the basic stuff like dependency management, testing, build and so on. It also has an opinionated formatter that saves a lot of time.  Goroutines: Goroutines are awesome. It’s so easy to use and intuitive and the best concurrency experience you can find.  Native images: It’s so easy to build native images with Go, no special setup required. It supports building for your favorite platform out of the box. 😩 Dislikes:  Simplicity: Go’s simplicity is really nice when you are getting started but it starts to feel like an annoyance once you start writing more Go code. If you come from another language, you soon start to yearn for features like generics, error handling, default values, and so on. Keeping code DRY is a challenge in Go due to the lack of generics. IMO Go would be a great language if it had generics. Also, I dread writing if err != nil in Go.  Boilerplate: Side effect of Go being too simple. In large codebases, you would be repeating stuff like finding an item from an array or map so many times that the boilerplate is just too much after a while.  Implicit interfaces: May it’s just me. I don’t like implicit interface implementations. It just feels wrong and confusing to me. ConclusionThese are my personal favorites among programming languages and doesn’t mean the likes and dislikes would apply to anyone. make your own conclusions but do use the language before making a judgment about it. In today’s IT landscape, being a polyglot developer is becoming a requirement. And personally, I believe that being a polyglot makes you a better programmer and software engineer. I previously wrote about how to be an effective polyglot developer, do check it out if you are interested. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Photo by Peter Herrmann on Unsplash "},{id:25,url:"https://deepu.tech/my-second-impression-of-rust/",title:"My second impression of Rust and why I think it's a great general-purpose language!",body:"2021/05/07 - I wrote about my first impression of Rust in November 2019. After that, I dabbled with it few times, but mostly on very simple code, and while I liked using Rust I wasn’t too amazed. On the surface, it kind of felt more or less like many other languages I have dabbled with. All that changed last month when I finally decided to build a realistic use case with Rust. Since I was quite invested in cloud computing and containers, I decided to build a terminal UI to monitor Kubernetes clusters with Rust. I know it was quite ambitious for a Rust newbie. But looking back, it was absolutely worth it and I have KDash to show for it. Introducing KDash - A simple @kubernetesio terminal dashboard built with @rustlang Still a WIP but here is an MVP I built over the last few weeks. Please try it out and provide me feedback also any contribution is welcome#rust #Kubernetes #oss https://t. co/ws9qfc9ie8 pic. twitter. com/NYkQHQwBc2 &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) April 16, 2021So it’s a pretty UI on the terminal that shows different resource data and utilization metrics for Kubernetes clusters. Kind of inspired by K9s, which is built with Go. The focus for KDash was speed and UX. With Rust, I knew that I didn’t have to worry about the speed part. The architecture was quite complex. It’s fully event-driven, concurrent, and asynchronous with different threads to handle drawing of the UI, doing network requests, streaming logs, and running commands. I used channels to pass events across threads and the application state was shared using an ARC smart pointer and a Mutex lock. I’ll write another blog with more details about the architecture choices and inspirations. While, I have implemented similar architectures in Go, Java, and JavaScript, doing it in Rust was quite challenging. I struggled the first few days to put together a basic structure and was constantly battling the compiler thanks to my habits from other languages I work with. But I started getting better at thinking in the Rust way and within a week I was having fewer compiler errors and Clippy warnings. After few weeks, I rarely get compiler errors for new code (also thanks to rust-analyzer) and I have started to polish existing code to make it better or to write better abstractions. So now that I have set the context, I feel it’s time to revisit points from my first impression and see if they still hold true. Along the way, I’ll share my thoughts on why I think Rust is the future for general-purpose languages and how it is taking over the software engineering world. I’ll try not to be biased as much as possible and wear my polyglot hat for comparisons. If you haven’t read my previous post on the topic, I encourage you to read it for better context. What I love about Rust: Ok, let’s get this out of the way first. So I love everything I originally liked about Rust. Some of them even more so after using Rust full-fledged and gaining more experience in it. So here, I’ll touch upon some important high-level stuff that I didn’t mention in the previous post. Rust throws around some buzz words in its docs, but they are not just marketing buzz, they actually mean it with full sincerity and they actually matter a lot and are indeed the biggest selling points of Rust. Safety: What does being safe mean for a language? or rather what does unsafe mean? Let’s set the context first so that we can appreciate what Rust offers. Safety can be categorized into three (or four if you count null safety)  About 70% of all CVEs at Microsoft are memory safety issues. Two-thirds of Linux kernel vulnerabilities come from memory safety issues Memory safetyThis means when you access a variable or an item in an array, you can be sure that you are indeed accessing what you meant to or are allowed to access. In other words, you will not be reading/writing into the memory of another variable or pointer by mistake regardless of what you do in your program. Why is this a big deal? Doesn’t all major programming languages ensure this? Yes, to varying extent. But some languages are unsafe by default. In C or C++, you can access the memory of another variable by mistake or you can free a pointer twice (double-free error). Such behavior is categorized as undefined behavior as they are unpredictable and hence can be abused by a hacker to take control of the program or to leak privileged information. In memory-safe languages, if you try to access an array element out of its bound, you will just crash the program with a panic/error, which is predictable behavior. This is why memory-related bugs in C/C++ systems often result in CVEs and emergency patches. There are other memory-unsafe behaviors in C or C++; accessing pointers to stack frames that have been popped, a memory that has been de-allocated, iterator invalidation, and so on.  Null safety: I list this separately under memory safety, as I come from a Java/JS background and we are so used to the concept of null (infamous for being the worst invention in programming). Garbage collected languages kind of need a concept of nothing so that a pointer can be freed. But it also leads to issues and pain. NPE anyone? Technically this falls under memory safety but most memory-safe languages still let you use null as a value leading to null pointer errorsType safetyThis means when you access a variable you access it as the correct type of data it is stored as. This gives us the confidence to work on data without having to manually check for the data type during runtime. Memory safety is required for a language to be type-safe. Thread safetyThis means you can access/modify the same memory from multiple threads at the same time without worrying about data races. This is generally achieved by using mutual exclusion locks (Mutex) or thread synchronization. Thread safety is required for optimal memory and type safety so generally languages that are memory and type-safe tend to be thread-safe as well. Now let’s see how Rust offers safety in these aspects. Memory safetyRust ensures memory safety at compile time using its innovative ownership mechanism and the borrow checker built into the compiler. The compiler just does not allow memory unsafe code unless it’s explicitly marked as unsafe in an unsafe block or function. This static compile-time analysis eliminates many types of memory bugs and with some more runtime checks, Rust guarantees memory safety.  Null safety: There is no concept of null at the language level. Instead, Rust provides the Option enum which can be used to mark the presence or absence of a value making the resulting code null safe and much easier to deal with and you will never encounter null pointer exceptions in Rust. Type safetyRust is statically typed and it guarantees type safety by strict compile-time type checks and by guaranteeing memory safety. This is not special as most modern languages are statically typed. Rust also allows some level of dynamic typing with the dyn keyword and Any type when required. But the powerful type inference and the compiler ensure type safety even in that case. Thread safetyRust guarantees thread safety using similar concepts it uses for memory safety along with providing standard library features like channels, Mutex, and ARC. The compiler makes it impossible to cause accidental data race from a shared state. This makes us confident to focus on code and let the compiler worry about shared data between threads. Now, don’t be mistaken, you will see crashes and errors in a Rust application, even array index out of bound errors, and so on. Rust is not claiming safety against errors or is going to catch wrong logic, as no compiler can guarantee against human error. It just makes errors predictable so when errors do happen you are sure that there is no security issue associated with it like in a C/C++ application. Rust also lets you write unsafe code when required by explicitly declaring unsafe blocks. This gives the flexibility when needed as there are certain low-level systems use cases that would need unsafe memory access. Rust cannot guarantee any of the above safety in unsafe code blocks as it’s for people who are sure of what they are doing and don’t want the compiler to interfere. Now, most statically typed high-level languages like Java, Go or C# also offers all of the above to varying extend, none of them provide null safety though. But they do it with the cost of having a runtime and a garbage collector. This is why Rust is special as it offers better safety than any high-level language without a runtime or memory management (garbage collection, reference counting, and so on) overhead. At the same time, it offers speed and performance, sometimes better than, of a low-level language like C/C++ Zero cost abstractions: A zero-cost abstraction means that how you write your program does not affect its performance, for example, you can choose to create or use any number of abstractions to structure your program, you could use loops or iterators, you could do functional or imperative programming and the result remains the same. The compiler will produce the best possible implementation of the machine code for the use case regardless of the flavor of code you choose.  What you don’t use, you don’t pay for. And further: What you do use, you couldn’t hand code any better.  – Bjarne Stroustrup Only very few programming languages offer this. C++ is popular for offering zero-overhead abstractions but they are not always zero cost if you consider the compile-time cost. Rust compiler, however, seems to be smarter and offers zero-cost abstractions in most cases with better developer experience which is an important factor. There could still be negligible overheads like array bound checks and so on.  A zero-cost abstraction, like all abstractions, must actually offer a better experience than the alternative.  – Saoirse Let’s see a small example to understand and appreciate the importance of this. We will compare a high-level language like Java to Rust for this. But you could technically take any other high-level language for the comparison and the results can be similar. Let’s take the below Java program. Running JMH benchmark on it gives the performance numbers for each function and its added as comments inline 12345678910111213141516171819202122232425262728// Average 10. 059 ns/oppublic long factorialForLoop(long number) {  long result = 1;  for (; number &gt; 0; number--) {    result *= number;  }  return result;}// Average 20. 689 ns/oppublic long factorialRecursive(long number) {  return number == 1 ? 1 : number * factorialRecursive(number - 1);}// Average 23. 457 ns/oppublic long factorialStream(long number) {  return LongStream. rangeClosed(1, number)      . reduce(1, (n1, n2) -&gt; n1 * n2);}/*# Run complete. Total time: 00:02:30 (JDK 11)Benchmark         Mode Cnt  Score  Error UnitsMyBenchMark. forLoop    avgt  3 10. 059 ± 1. 229 ns/opMyBenchMark. recursive   avgt  3 20. 689 ± 4. 465 ns/opMyBenchMark. stream     avgt  3 23. 457 ± 32. 424 ns/op*/As you can see, even though all three functions does the same job, they don’t have similar performance. The one with the most abstractions, stream iterations, suffers the most. Now let’s try the same three functions in Rust and run benchmarks using Criterion 1234567891011121314151617181920212223242526272829// Average 8. 5858 ns/opfn factorial_loop(mut num: usize) -&gt; usize {  let mut result = 1;  while num &gt; 0 {    result *= num;    num = num - 1;  }  return result;}// Average 8. 6150 ns/opfn factorial_recursion(num: usize) -&gt; usize {  return match num {    0 =&gt; 1,    _ =&gt; num * factorial_recursion(num - 1),  };}// Average 6. 6387 ns/opfn factorial_iterator(num: usize) -&gt; usize {  (1. . num). fold(1, |n1, n2| n1 * n2)}/*Benchmark        time:  [min    avg    max   ]factorial_loop      time:  [8. 4579 ns 8. 5732 ns 8. 7105 ns]factorial_recursion   time:  [8. 4394 ns 8. 5074 ns 8. 5829 ns]factorial_iterator    time:  [6. 4240 ns 6. 4742 ns 6. 5338 ns]*/As you can see the performance is identical for iterative and recursive approach and it’s even better for inbuilt abstractions (due to internal iterator optimizations and stuff). So in the case of Rust, it’s even fair to say that abstractions provide better performance than hand-optimized code in many cases and at worst they provide the same performance at zero cost. If you look into the assembly code generated by different flavors, in many cases, you can see the compiler produces the same assembly code. This gives us the power not to think about writing the most optimal code and instead focus on writing the most readable and reusable code. This doesn’t mean that anything in Rust is zero-cost abstractions. You can always end up writing code that does unnecessary computations and such which adds to the cost but at least it will be obvious. Some of the most notable zero-cost abstractions in Rust are  Ownership and borrowing Iterator and closure APIs Async/await and Futures Unsafe and the module boundaryFearless concurrency: We already saw that Rust is thread-safe so technically you can do all sorts of concurrency implementations in Rust without worries. Rust supports multithreading, green threads, parallel computing, and asynchronous programming either as first-class citizens or via crates like Tokio or Futures. The first realistic app I built in Rust is extremely concurrent and asynchronous and I’m yet to encounter any issues related to concurrency and Rust promises that I’ll never encounter data race issues even though I share state between multiple threads doing asynchronous operations. Honestly, I won’t be comfortable doing the same in any other language that I’m used to, maybe to an extend in Go as it’s quite good when it comes to concurrency as well. Community, Tooling &amp; Ecosystem: Rust has hands down, one of the best communities around, in my opinion. It doesn’t have the politics of Java or the bloat of JavaScript (yet 😉). Rust didn’t shy away from copying good things from other languages like package management from JavaScript, language features from Haskell, OCaml, Ruby, JavaScript, and so on. That diversity is visible in the community as well. You can see people with all sorts of backgrounds and an extremely welcoming and friendly community. Strangely the Rust community forum is more active than its stack overflow community and that says a lot. You will find people helping you and guiding you rather than being gatekeepers. Rust also guarantees backward compatibility while still improving the language constantly and has a tooling and library ecosystem that keeps up. The Rust library ecosystem feels similar to that of JavaScript and gives you the NPM vibes. The tooling is just amazing, rustc, rustup, and cargo are standard and there are tons of plugins like Clippy, rustfmt, and so on. Cargo acts as the build runner, package manager, plugin manager, and so on. It all feels well integrated and results in a great developer experience. There are some rough edges from time to time but is still way ahead of many other languages with an even longer history. Rust also provides one of the best documentation around. It is even included in the standard toolchain via the rustup docs command. Many other smaller things in Rust is awesome, I’m just skipping them for high-level stuff. What I still don’t like about Rust: These were the things I originally didn’t like about Rust and while I haven’t changed my mind on how I feel about many of these, some of these make more sense to me now. But that doesn’t mean I have to like them right. Read my original post if you want more context on these. Complexity: It seems like the complexity of the language only keeps on increasing at one end, with new language features, while some things are being simplified at the other end and I’m a bit torn about this, to be honest. I do like many features it provides and my issue about having many ways to do the same thing doesn’t seem like a big deal now that I understand the zero-cost abstractions in Rust. But once you step into advanced generics, traits, lifetimes and so on it quickly becomes mind-boggling and I can relate to newcomers as there is a steep learning curve. While I understand that this complexity is required and most of the time worth it, given the benefits of ownership, it’s hard not to yearn for a bit more simplicity. Hopefully, future versions will evolve to simplify a lot of these 🤞 Shadowing of variables in the same context: This makes more sense to me now and I end up using this a lot. With the ownership mechanism, you often have to rebind, create temporary intermediates or get value from option or transforms value so this is useful but my point about this being abused is quite true as well. There were few instances where I ended up using the same name for different content with the same type in the same context giving me an unexpected result. It didn’t cause any major issues but did affect readability and logic sometimes. So probably it would have been fine without this feature as well. However, with Clippy, it’s possible to add a lint rule to disallow this so it’s good enough I guess. Functions are not first-class citizens: Well, turns out they are first-class citizens but it’s just that functions in Rust are very complex. I understand the whys and hows and they make sense. But I would still prefer if this was more streamlined at least with some syntax sugars. But it’s not a big deal and is more of a nitpick now Implicit implementation of traits: After using Rust this makes sense and I like the way Rust does it much more than Go, for example. So this is no longer an issue for me 😸 Also, the nitpicks I had in my original post no longer bothers me 😄 Rust is the future:  Rust, not Firefox, is Mozilla’s greatest industry contribution  TechRepublic Well, first of all, let me say I’m falling in love with the language. So maybe my opinions have a small bias. I haven’t had this much fun programming in years. There is a weird feeling of satisfaction that you get when writing code in Rust. Now I understand why Rust is the most loved language for 5 years in a row from the Stack Overflow developer survey. Don’t get me wrong. Rust is not a silver bullet as there are issues like the steep learning curve and complexity and so on. But it’s the closest thing to a silver bullet in my opinion. That doesn’t mean I’ll just start using Rust for everything. I still enjoy being a polyglot developer and I’m still invested in Java, JS/TS, and Go among others. But if the use case requires speed and or concurrency or building system tools or CLIs, then I will be giving Rust the first preference, and maybe Go will take a backseat as there isn’t any advantage that Go brings over Rust for similar use cases. Normally a language would offer a choice between safety, speed, and high-level abstractions. At the very best you can pick two of those. For example with Java/C#/Go you get the safety and high-level abstractions at the cost of a runtime overhead whereas C++ gives you speed and abstractions at the cost of safety. But Rust offers all three and a good developer experience as a bonus. As our IT landscape is getting more complex and resource-hungry this combination matters a lot. Rust looks and feels like a general-purpose high-level language yet offers performance and memory efficiency of a low-level systems language. So this could be the general purpose language that doesn’t make compromises and doesn’t require a runtime and is cross-platform and is not as hard as C/C++. What is not to love here? Due to these unique characteristics, Rust is getting a foothold not just in systems programming but also in areas currently dominated by high-level languages like web applications, microservices, and CLI tools. It’s also getting more and more popular as a web-assembly language due to its light footprint and great WASM support. It has also made its way into the embedded/IoT world. There are many more use cases like serverless, JS runtime, game engines, game dev, Operating Systems, and even malwares 🤦 Rust is rapidly gaining popularity. That’s impressive given its just 5 years old. Big names like Microsoft, Google, Apple, Amazon, and Facebook are already invested in Rust and are making plans to replace C/C++ code with Rust. It will not be overnight but slowly Rust is going to replace a lot of those codes. Even Linux, the poster child of C, recently approved the use of Rust in some parts of the kernel, like driver code and so on. Unlike many general-purpose languages, which are not appropriate for some use cases due to the tradeoffs they make, Rust is uniquely positioned to work across the spectrum without any major disadvantage and be the general-purpose language for any use case, from client-side to systems programming. C/C++ and Go might be the ones to be displaced most in the short term by Rust in my opinion. I think Java/JS/TS/Python etc are safe for a long while due to their massive presence in large-scale applications, the maturity of the ecosystem and due to the migration cost. IMO, the only thing holding back Rust is the maturity of the library ecosystem which is only a matter of time to get better. I would finish off by saying this: You won’t appreciate Rust unless you spend few weeks building something in it. The initial steep learning curve could be frustrating or challenging depending on how you see it, but once past that it’s hard not to love it. It’s a toddler with superpowers after all 💗 References:  engineering. fb. com www. techrepublic. com threatpost. com blogs. gartner. com benchmarksgame-team. pages. debian. net deepu. tech www. zdnet. com www. zdnet. com boats. gitlab. io medium. com/ingeniouslysimpleIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:26,url:"https://deepu.tech/concurrency-in-modern-languages-java/",title:"Concurrency in modern programming languages: Java",body:"2021/04/30 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksConcurrency in Java:  The Java programming language and the Java virtual machine (JVM) have been designed to support concurrent programming, and all execution takes place in the context of threads    Wikipedia Java had support for concurrent programming from its early days. Prior to Java 1. 1 it even had support for green threads (virtual threads). Spoiler Alert! It’s coming back again with Project Loom. Concurrent programming has always been at the core of Java as it was aimed at multi-threaded and multi-core CPUs. While not as simple as goroutines to use, it was powerful and flexible for almost any use case. While powerful, it’s also quite complex especially when you have to access data between threads since the default mechanism in Java, due to its OOP roots, is to use shared state concurrency by synchronizing the threads. Threads are at the core of concurrent &amp; asynchronous programming in Java. From JDK 1. 1 onwards these threads would map 1:1 to OS threads. Due to its early inception, the ecosystem has really mature libraries as well, from HTTP servers to concurrent message processors and so on. Asynchronous programming caught up a bit late in Java, the building blocks were there but it was practically useable only from Java 8, but it has matured as well and now has a great ecosystem with support for reactive programming and asynchronous concurrency. Java 8 bought a lot of improvements and simplifications to make it easier to do concurrency. For example, standard Java APIs like the Stream API even provides a way to do parallel processing easily by just invoking a method call on complex and CPU intensive pipelines. With Java, it’s possible to do multi-threaded concurrency or parallel programming as well as asynchronous programming. This means as we saw in the first chapter, we can mix and match these models to get the best possible performance for any use case. Multi-threading: Java provides building blocks to create and manage OS threads as part of the standard library and it also provides implementations required for shared-state concurrency using locks and synchronization. Message-passing concurrency is not provided by default but can be done using external libraries like Akka or using an Actor model implementation. However, due to the memory model, it’s up to the developer to ensure there are no data races or memory leaks in the concurrent program. In order to make multi-threading even more efficient, Java provides ways to create thread pools and reuse those threads to increase throughput. This will become even better once Project loom is released, hopefully with Java 17 or 18. Technically Java has one of the most mature ecosystems when it comes to multi-threading and most Java frameworks that you would end up using will be making use of it internally for performance improvements. Asynchronous processing: Technically asynchronous programming is not part of concurrency but in practice, it goes hand in hand for many use cases and improves performance, and makes resource usage more efficient. In Java asynchronous programming is achieved using the same building blocks as concurrent/parallel programming. a. k. a, Threads. This wasn’t very popular in Java before Java 8 due to complexity and, let’s be honest, the lack of things like lambdas, functional programming support, CompletableFuture, and so on. The latest versions of Java provide the building blocks required for asynchronous programming with standard interfaces and implementations. But do keep in mind that using an asynchronous programming model increases the overall complexity and the ecosystem is still evolving. There are also many popular libraries and frameworks like Spring and RxJava that support asynchronous/reactive programming. Java still doesn’t have any syntax sugar for async/await though but there are alternatives like the EA Async library that’s close enough. Benchmarking: Now that we have some basic understanding of concurrency features in Java, let us build a simple concurrent web server in Java. Since Java offers multiple ways to achieve this we’ll be building two sample applications and comparing them. The Java version used is the latest (16. 0. 1) at the time of writing. Multi-threaded concurrent webserver: This example is closer to the Rust multi-threaded example we built in the rust chapter, I have omitted import statements for brevity. You can find the full example on GitHub here. We use java. net. ServerSocket for this. We are not using any external dependency in this case. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class JavaHTTPServer {  public static void main(String[] args) {    var count = 0; // count used to introduce delays    // bind listener    try (var serverSocket = new ServerSocket(8080, 100)) {      System. out. println( Server is listening on port 8080 );      while (true) {        count++;        // listen to all incoming requests and spawn each connection in a new thread        new ServerThread(serverSocket. accept(), count). start();      }    } catch (IOException ex) {      System. out. println( Server exception:   + ex. getMessage());    }  }}class ServerThread extends Thread {  private final Socket socket;  private final int count;  public ServerThread(Socket socket, int count) {    this. socket = socket;    this. count = count;  }  @Override  public void run() {    var file = new File( hello. html );    try (        // get the input stream        var in = new BufferedReader(new InputStreamReader(socket. getInputStream()));        // get character output stream to client (for headers)        var out = new PrintWriter(socket. getOutputStream());        // get binary output stream to client (for requested data)        var dataOut = new BufferedOutputStream(socket. getOutputStream());        var fileIn = new FileInputStream(file)    ) {      // add 2 second delay to every 10th request      if (count % 10 == 0) {        System. out. println( Adding delay. Count:   + count);        Thread. sleep(2000);      }      // read the request first to avoid connection reset errors      while (true) {        String requestLine = in. readLine();        if (requestLine == null || requestLine. length() == 0) {          break;        }      }      // read the HTML file      var fileLength = (int) file. length();      var fileData = new byte[fileLength];      fileIn. read(fileData);      var contentMimeType =  text/html ;      // send HTTP Headers      out. println( HTTP/1. 1 200 OK );      out. println( Content-type:   + contentMimeType);      out. println( Content-length:   + fileLength);      out. println( Connection: keep-alive );      out. println(); // blank line between headers and content, very important!      out. flush(); // flush character output stream buffer      dataOut. write(fileData, 0, fileLength); // write the file data to output stream      dataOut. flush();    } catch (Exception ex) {      System. err. println( Error with exception :   + ex);    }  }}As you can see we bind a TCP listener using ServerSocket to port 8080 and listen to all incoming requests. Each request is processed in a new thread. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 1234567891011121314151617181920212223242526272829303132333435❯ ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 326 secondsComplete requests:   10000Failed requests:    0Total transferred:   2600000 bytesHTML transferred:    1760000 bytesRequests per second:  491. 98 [#/sec] (mean)Time per request:    203. 262 [ms] (mean)Time per request:    2. 033 [ms] (mean, across all concurrent requests)Transfer rate:     124. 92 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  1  1. 5   0   13Processing:   0 201 600. 0   1  2023Waiting:    0 201 600. 0   0  2023Total:     0 202 600. 0   1  2025Percentage of the requests served within a certain time (ms) 50%   1 66%   2 75%   4 80%   6 90%  2000 95%  2001 98%  2003 99%  2006 100%  2025 (longest request)As you can see the request handler thread sleeps for 2 seconds for every 10th request. In a real-world scenario, the thread pool itself could become the bottleneck and you may not be able to set so many threads as the OS may not be able to provide so many thus creating increased resource usage and bottleneck. In this simple use case, since each thread spawns and processes the request really fast we won’t encounter an issue. So let’s see if we can have another solution without such a bottleneck. Asynchronous concurrent webserver: This example is closer to the asynchronous example from the rust chapter, I have omitted import statements for brevity. You can find the full example on GitHub here. Notice that we are using java. nio. channels. AsynchronousServerSocketChannelhere and no external dependencies. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class JavaAsyncHTTPServer {  public static void main(String[] args) throws Exception {    new JavaAsyncHTTPServer(). start();    Thread. currentThread(). join(); // Wait forever  }  private void start() throws IOException {    // we shouldn't use try with resource here as it will kill the stream    var server = AsynchronousServerSocketChannel. open();    server. bind(new InetSocketAddress( 127. 0. 0. 1 , 8080), 100); // bind listener    server. setOption(StandardSocketOptions. SO_REUSEADDR, true);    System. out. println( Server is listening on port 8080 );    final int[] count = {0}; // count used to introduce delays    // listen to all incoming requests    server. accept(null, new CompletionHandler&lt;&gt;() {      @Override      public void completed(final AsynchronousSocketChannel result, final Object attachment) {        if (server. isOpen()) {          server. accept(null, this);        }        count[0]++;        handleAcceptConnection(result, count[0]);      }      @Override      public void failed(final Throwable exc, final Object attachment) {        if (server. isOpen()) {          server. accept(null, this);          System. out. println( Connection handler error:   + exc);        }      }    });  }  private void handleAcceptConnection(final AsynchronousSocketChannel ch, final int count) {    var file = new File( hello. html );    try (var fileIn = new FileInputStream(file)) {      // add 2 second delay to every 10th request      if (count % 10 == 0) {        System. out. println( Adding delay. Count:   + count);        Thread. sleep(2000);      }      if (ch != null &amp;&amp; ch. isOpen()) {        // Read the first 1024 bytes of data from the stream        final ByteBuffer buffer = ByteBuffer. allocate(1024);        // read the request fully to avoid connection reset errors        ch. read(buffer). get();        // read the HTML file        var fileLength = (int) file. length();        var fileData = new byte[fileLength];        fileIn. read(fileData);        // send HTTP Headers        var message = ( HTTP/1. 1 200 OK\n  +             Connection: keep-alive\n  +             Content-length:   + fileLength +  \n  +             Content-Type: text/html; charset=utf-8\r\n\r\n  +            new String(fileData, StandardCharsets. UTF_8)        ). getBytes();        // write the to output stream        ch. write(ByteBuffer. wrap(message)). get();        buffer. clear();        ch. close();      }    } catch (IOException | InterruptedException | ExecutionException e) {      System. out. println( Connection handler error:   + e);    }  }}As you can see we bind an asynchronous listener to port 8080 and listen to all incoming requests. Each request is processed in a new task provided by AsynchronousServerSocketChannel. We are not using any thread pools here and all the incoming requests are processed asynchronously and hence we don’t have a bottleneck for maximum connections. But one thing you may immediately notice is that the code is much more complex now. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 123456789101112131415161718192021222324252627282930313233343536ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 243 secondsComplete requests:   10000Failed requests:    0Total transferred:   2770000 bytesHTML transferred:    1760000 bytesRequests per second:  494. 00 [#/sec] (mean)Time per request:    202. 431 [ms] (mean)Time per request:    2. 024 [ms] (mean, across all concurrent requests)Transfer rate:     133. 63 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 6   0    5Processing:   0 201 600. 0   0  2026Waiting:    0 201 600. 0   0  2026Total:     0 202 600. 0   0  2026Percentage of the requests served within a certain time (ms) 50%   0 66%   1 75%   3 80%   4 90%  2000 95%  2001 98%  2002 99%  2003 100%  2026 (longest request)We got almost identical results here, this one is even faster by 100ms. Hence this version seems much more efficient than the multi-threaded version for this particular use case. However at the cost of added complexity. Conclusion: As I explained in the first part of this serious, this simple benchmarking is not an accurate representation for all concurrency use cases. It’s a simple test for a very particular use case, a simple concurrent web server that just serves a file. The idea is to see the differences in solutions and to understand how concurrency works in Java. And for this particular use case, asynchronous solutions do seem to be the best choice. So stay tuned for the next post where we will look at concurrency in Kotlin and build the same use case in Kotlin. References:  blogs. oracle. com dzone. com www. vogella. com dzone. com www. baeldung. com dzone. com www. baeldung. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Evgeniya Litovchenko on Unsplash "},{id:27,url:"https://deepu.tech/the-state-of-linux-on-desktops/",title:"The state of Linux as a daily use OS in 2021",body:"2021/03/24 - I got my first PC when I was in high school, in 2003. It was a DIY Intel Pentium 4 PC with 512MB of RAM, running Windows XP. This got me hooked to computers and I even started assembling PCs for others and selling my service as a Windows OS installer (pirated, of course, the middle class in India back then couldn’t afford to buy software). This also made me quite an expert in the Windows XP OS which I loved. I used to get occasional bootable CDs of a Linux distro from my Chip magazine subscription which I would try then forget as they weren’t as compelling or user-friendly as Windows back then. Then came Windows Vista, which I didn’t like much, and Windows 8, which I didn’t care for, and Windows 10, which seemed decent but still didn’t feel as good as XP. Fast forward to 2016 I made a bold jump to Ubuntu from Windows 10, after 13 years of being a loyal Windows user. I was on Ubuntu for ~6 months then switched to Fedora and never looked back. Now in 2021, I’m still using Fedora as my daily driver. I have macOS from work which I use for work stuff and a Windows 10 PC for VR gaming. Ok, now that I have nerded out about my fascination with computers, let’s get into the actual topic and see if Linux is good enough as a daily driver even for regular users who just wants a working computer and are not tech-savvy and never want to visit the terminal. Throughout the article, I’ll be making some comparisons with the world’s most popular OS, Microsoft Windows, and Apple’s macOS which, IMO, is unavoidable. State of Linux: Linux has come a long way. For those of you, who still think of a terminal and an outdated UI(I know people who still love those UI) when you hear the word Linux, I think you probably haven’t seen or worked on a Linux distro in the last 10 years. When we talk about Linux on Desktop(or Laptop) we are really talking about a Linux distribution, like Ubuntu or Fedora, and hence we need to understand what makes a distribution (distro). A Linux distribution is an Operating System made up of mainly the below:  The Linux Kernel - This is the core of the operating system and it is used by all Linux operating systems, including Android and Chrome OS Windowing system - Responsible for managing input peripherals and driving output displays. This is either the legacy X11 or the newer Wayland on almost all desktop Linux distros. More and more distros are using Wayland by default now instead of the legacy X11 server. All distros normally support both and you can configure the display sever you prefer as the default Desktop Environment (DE) - This forms the GUI shell for the operating system and there is a multitude of options like Gnome, KDE, Xfce and so on. You can find most distros providing multiple versions each having a different DE. The DE can also include a Window manager like KWin or Mutter that works with the windowing system Package manager - Every family of Linux distributions provides a package manager to find and install software and the package manager is unique for every distribution family. For example APT, DNF, pacman and so on.  Software and tools - The distributions generally also come with a set of useful default software like a terminal emulator, a shell, a browser, file browser, and so on. So let’s look into different aspects of Linux and see if it is ready for daily use. I hope by the end of this post, you will give Linux serious consideration. Flexibility: Flexibility is a boon and a curse at the same time for Linux. IMO it is the reason many people get into Linux and many people get scared of Linux. I’m in the first group though. Flexibility and freedom is the core of Linux, you can see this theme in everything else, from the choice of distribution to choosing a DE there is a plethora of choice and even after making a choice, there are so many things you can customize to your exact preference, be it look and feel, UX or performance, everything is configurable. Window decoration customization in KDE Plasma While such flexibility is great for power users and tech-savvy users, it could be daunting and annoying to people who just want to use the computer to get some work done. Linux in the past is known to scare off average users as they either came pretty bare metal out of the box or came with highly opinionated defaults, like unity desktop in Ubuntu. Having sane defaults was not a thing in the Linux world and that’s why people associate Linux users to someone working only on a terminal as you would end up using the terminal to do something as it’s the fastest way in Linux. This situation is changing though, for example Ubuntu, with Gnome, or Kubuntu, with KDE, Elementary and Pop!_OS are very user friendly from the beginning and comes with sane defaults and a pretty decent set of default applications. They also offer nice app stores for users to find more software.  With great power comes great responsibility    Uncle Ben When it comes to freedom Linux is not a walled garden like the Apple ecosystem or configurable but annoying for power users like Windows. ✅ This level of flexibility makes it easy to break things if you don’t know what you are doing. But IMO the advantages outweigh the disadvantages and the default setup in Linux distros keeps getting better. It’s already possible to get on a distro like Ubuntu and get work done without having to configure anything. UI/UX: First of all, UI and UX is not the same thing. A polished UI doesn’t mean good UX and vice versa. While UI/UX is subjective for most people, they would mostly agree on what good UX is and what bad UX is. Linux didn’t use to care much about either of those two decades ago and it still didn’t care about UI a decade ago. Fast forward to 2020 and you will find some of the most beautiful and slick-looking UIs and one of the best UX on Linux DEs. Look at the customized KDE plasma [video] on the header image of this post for example. KDE Plasma with default Dragonized theme on Garuda OS The UI/UX on Linux is entirely up to the DE, which unlike in Windows/macOS is separate from the OS, and you can freely and easily switch between different DEs. You can even install multiple DEs and pick one for each login session. Is it as polished as a macOS though? You may ask. Probably not but it makes up for that in terms of flexibility and better UX, IMO. Having used a macOS for over a year now, I do like how the UI looks but I hate the UX of a macOS. It’s not very intuitive, unless you grew up with a macOS, IMO. It’s either the macOS way or the highway, there isn’t much flexibility in terms of what you can change to fit your preferences, and in the end, you just end up adapting to whatever the UX designers at Apple thought is good for the rest of us. I like the UX of Windows way better than that of macOS even if the UI of Windows is a bit dated. Gnome 3. 36 with Arc theme on Fedora 32 With Linux, you can make your UI look and feel like macOS [video] or Windows [video] if that’s what you like. You can also go full CyberPunk [video] on it if you feel like it or anything else for that matter. The possibilities are unlimited. It doesn’t mean you have to customize things to get a good experience, the default setup in many of the DEs is quite good for most people. KDE plasma 5. 20 default Breeze twilight theme on Fedora 34 I’m using Gnome for over 4 years and recently decided I will switch to KDE when I refresh my laptop. There are a ton of DEs in the Linux world but in my personal opinion, Gnome, KDE, Pantheon (Elementary OS) and Deepin provides the best UX out of the box and has slick UIs as well. ✅ So I think it’s fair to say Linux is better than or on par with, depending on personal preferences, macOS/Windows in terms of UI/UX Performance: I don’t think there is much to worry about when it comes to performance. There is a reason Linux is used as the default choice for servers and infrastructure. Performance has never been an issue in the Linux world and I don’t think that was ever a factor in its adoption so I’m not gonna go into details here. Mainstream distributions based on Ubuntu, Fedora, and Arch Linux provide very good performance out of the box and DEs like Gnome and KDE have evolved to become much smoother and faster. You could even run Linux with fairly good performance on very outdated hardware thanks to lightweight distributions like Xubuntu or Lubuntu that use lightweight DEs like Xfce and LXQt macOS comes quite close to Linux in terms of overall performance and it still has the advantage of being built by the same company as its hardware thus making it possible to tune the OS for specific hardware without worrying about supporting other hardware combinations. Whereas in Linux &amp; Windows, it has to work on a wide range of hardware. Also when it comes to individual application performances, some are just better tuned for macOS/Windows than for Linux Graphics performance used to be an issue for Linux in the past but the latest drivers for both NVidia and AMD have bridged the gap a lot and you can get good GPU performance these days in Linux and even play AAA titles via Steam on Linux with performance that is on par with Windows. Battery life on Linux laptops are still not as good as macOS or Windows but it’s getting better and is not bad enough to be a deal-breaker ✅ Overall, Linux is better than Windows and macOS in overall performance. Linux also has the advantage of having more powerful hardware to choose from compared to macOS. Stability &amp; software support: Unfortunately this is a weak point for Linux distros. While the Linux kernel itself is very stable, the distros on the other hand tend to be less stable compared to macOS or even Windows sometimes. While the overall stability of Linux distros has improved a lot, depending on the specific distro you are using, the stability aspect could still be an issue. In Linux, you choose between stability and bleeding-edge based on your preference. For example;  Ubuntu prioritizes stability over bleeding edge software and hence has long-interval release cycles and long-term support for versions. You won’t get all software automatically updated in Ubuntu and might have to either do it manually or wait for the next release.  Arch Linux on the other hand prioritizes providing the latest and greatest software over stability. It hence has a rolling release and you always get the latest software as soon as it’s released. This is definitely for power users who knows what they are doing Fedora on the other hand choose a compromise to provide stable software at more frequent release cycles. So you get releases often which keeps everything up to date but versions are not supported as long as in Ubuntu. Another issue with the Linux ecosystem is the availability &amp; stability of software. Some software vendors just don’t care about Linux users, I’m looking at you Adobe, and doesn’t bother publishing software for Linux. Some publish software that is worse/unstable/buggy than their macOS/Windows counterparts. The same goes for hardware support. Linux sometimes lacks support for certain hardware that just works in Windows/macOS. While the hardware support and drivers keep getting better they will never match up with what you will find in Windows or macOS unless the vendors start caring about Linux, which will only happen if Linux gains more market share in Desktop Note: If a certain Windows software is needed on Linux, there is also the possibility of getting it working with Wine. Distributions like Ubuntu and Fedora are quite stable and have better support for third-party software and hardware drivers, and are getting better at staying stable. You can also find many variations of these based on your preferences. There are also many distributions in between all these, like Manjaro which is a more stable version of Arch, and so on. I have been using Fedora for close to 5 years now and I’m quite happy with its stability and shorter release cycles. If stability is very important for you, then probably Ubuntu is better for you as it has longer release cycles and a much more stable ecosystem with wider software choice. When it comes to updates and upgrades, Linux easily wins as it’s the easiest to upgrade and most updates won’t even require a reboot. ✅ macOS and Windows score better in terms of stability and software/hardware support and the gap is shrinking. For daily use you already have everything required for most user personas, we will see more about these later, and with the correct distro you will not be missing out on anything. Security &amp; Privacy: If you are privacy and security-focused then Linux is the best OS for you. macOS would come second but it’s not even close and I think we all can agree that Windows is the worst when it comes to Security and Privacy. For Linux, you don’t need to create any online accounts or use any proprietary software or send your data to anyone if you don’t want to. There are also Linux distributions, like Qubes OS and PureOS, which are entirely privacy and security-focused. Linux is also the least intrusive among the three. On the security front, you don’t have to worry about malware and junkware on Linux like in Windows. Most Linux software being open source also means they are more trustworthy and has a community around who will find and fix vulnerabilities much faster than Windows or macOS ✅ Linux is easily the best when it comes to privacy and security without sacrificing your freedom Installation &amp; Availability: This probably was the hardest part of Linux in the past and it still is quite difficult in some distros like Arch, but there are many distros like Ubuntu, Fedora, Elementary and so on that are so easy to install. Most stable and popular distros provide an easy installation wizard that is easy enough for most people, but it still requires some technical knowledge and hence is not for everyone. This is where Windows takes the pie as it comes pre-installed on 95% of PCs and the same for macOS. Thankfully this situation has started to change. When I switched to Linux in 2016, I started looking for a laptop that either came pre-installed with Linux or was certified to work well with Linux and I couldn’t find many options. The only decent choice was the Dell Precision line that came with Ubuntu. Fast forward to today and there are a lot of great companies that are building PCs and laptops specifically for Linux. Even bigger names like Dell, Lenovo and Asus has started selling hardware with Linux pre-installed. Some of the good options for Linux out of the box are:  System76 - They make highly configurable and performant PCs and Laptops specifically for Linux that come with Pop!_OS or Ubuntu Tuxedo, Clevo - They make highly configurable Laptops and PCs for Linux and can pre-install Ubuntu Slimbook - They make highly configurable Ultrabooks and PCs for Linux and can pre-install Ubuntu, Kubuntu, Ubuntu Mate, Debian, Elementary, Mint, Manjaro, Zorin, and KDE Neon. They even do dual boot setup with Windows.  Kubuntu Focus - They make highly performant Linux Laptops with long term premium support option for Kubuntu.  Purism - They make PCs, Laptops, and Mobile phones that comes with PureOS, a privacy-focused Linux distro Dell XPS/Precision developer editions Lenovo ThinkPadThis list is a good source to find if your existing hardware works for your distro of choice. ✅ The availability of pre-installed Linux laptops is still not that great and choices are very limited. But if you are Ok to do the installation yourself then there is a plethora of modern hardware to choose from and with the support of an amazing community, you can get up and running in few hours. Support: When it comes to support, Linux is either the best or worst depending on how you see it. For example, I would argue that Linux has the best support system due to the fact it’s FOSS and there is a great community that helps you along your way. Bugs are fixed faster and security patches are available immediately when ready. No other OS has the kind of community support that Linux offers. Now if you are expecting professional paid support then probably it’s not as great as what is available in Windows/macOS. While professional support is great for server distributions like RHEL very few consumer distributions, like Ubuntu, provide official support plans. Of course, many independent organizations provide support for most consumer Linux distros. ✅ This boils down to preference. For most average users community support would be good enough but if you are looking for enterprise-level support it could be a bit more work to acquire. Decision fatigue: Now that we looked at different aspects of the OS that we need to care about, let’s look at another boon/curse in the Linux world, which in my opinion scares away regular users. When it comes to Linux distributions they can be grouped into families based on the software packaging format they support. After that there are specific distributions and way too many derivatives of them, I’m only going to mention the popular ones here so that the blog doesn’t end up being a book. You can find the full list here  RPM-based: Fedora and derivatives, CentOS/RHEL and derivatives, openSUSE, and so on Debian-based     Ubuntu and derivatives like Pop!_OS, Kubuntu, Lubuntu, Xubuntu, KDE Neon, Elementary OS, Linux Mint, and so on   Kali Linux, PureOS, Deepin, and so on    Pacman-based: Arch Linux, Manjaro, Garuda, BlackArch and so on Gentoo-based: Gentoo and its derivativesDistro hopping is a term popular with Linux enthusiasts, but it’s not for everyone. Average users just need a working OS and having so many options will only cause decision fatigue. I’m not gonna go into the debate of, if it’s good or bad to have so many distros, instead I’ll make some personal suggestions for newcomers to Linux so it might help with a decision Linux for everyone: These are my suggestion for people who are new to Linux or for those who are annoyed with macOS/Windows but are terrified of Linux to give it a chance For an average computer user: For an average computer user who just wants a working desktop/laptop with which you can browse the internet, use some productivity tools, make video calls, watch videos or listen to music and do other general-purpose stuff without worrying about using the terminal for anything and have descent stability:  Ubuntu: It’s the easiest distro to get started and comes pre-installed in most Linux laptop options. It’s also the most widely used Linux distro. It’s easy to install and easy to use thanks to Gnome DE. It has a great community, long-term support, excellent software, and hardware support. This is the most beginner-friendly Linux distro out there that comes with a good set of default software. If you don’t like Gnome or if you are coming from Windows you can choose variants like Kubuntu or Linux Mint. If you have very old hardware then go for Xubuntu Elementary OS: If you come from macOS, you will feel at home with elementary. It’s Ubuntu-based and hence is super stable and user-friendly. Tip: You can find all the officially supported laptops/desktops for Ubuntu here. They should work for all Ubuntu derivatives as well. For developers: For developers, who want a fast and productive operating system for anything from web development to kernel development the below are a great choice  Fedora: Provides a stable OS with shorter release cycles thus keeping libraries and tools up to date. Has great community and software support. Comes with required toolchain for most languages pre-installed. I have been using Fedora for over 4 years and I’m very happy with it. Fun fact, Linus Torvalds also uses Fedora. It comes with Gnome DE as default and you can choose from different variants if you like KDE or Xfce DE instead Manjaro: If you don’t mind living on the edge then Manjaro can be a good option as well, but beware that it’s Arch-based and hence does rolling updates which could break something from time to time. But since it’s Arch-based, it’s also extremely flexible and powerful. You can find Manjaro with all popular DE options. If you are a computer science student, I would also recommend Manjaro as you can learn a lot from an Arch-based rolling distribution. Tip: You can find some OOB Manjaro hardware here For designers and multimedia professionals: For people who are into graphics design, 3D modeling, video editing, audio production, and other media-related work, Linux could be a great choice unless your entire workflows are based on Adobe or some other software suite with poor Linux support. So first see if your favorite software suite has Linux support. If you feel a bit brave then you will be surprised with the free and OSS software alternatives like GIMP, Kdenlive, Blender and so on  Ubuntu Studio: This is a great choice as Ubuntu is very stable and this variant comes with a lot of multi-media software like Gimp, Inkscape and Blender, and so on.  Fedora design suite: Another great choice if you want a bit more bleeding edge. This also comes with a lot of OSS multi-media software AV Linux: This is crafted specifically for audio production and comes with all optimizations and software required for audio/video production. For gamers: A decade ago you wouldn’t even think of Linux when it comes to Gaming, that has changed thanks to the growing community of indie game makers and Valve, makers of Steam. Steam made it possible to run AAA titles on Linux which runs almost as well as in Windows. A great testament to this is SteamOS from Valve which powers their Steam Machine line of gaming PCs. There is also Steam Play that makes it possible to run Windows games on Linux Windows still dominate when it comes to gaming. Largely due to huge collection of games that are only supported on Windows and due to better Graphics performance. But things are looking good on the Linux side as well and Linux might be the best alternative to Windows for gamers. There are great options for Gamers on Linux  Pop!_OS: This is an Ubuntu-based distribution from System76 that comes optimized for gaming SteamOS or GamerOS: If you want a console-like GUI with great controller support for gaming on SteamConclusion: Linux is undisputed on the mobile/server/infra space. It has the largest installed base of all general-purpose operating systems mainly due to Android. Did you know that 85% of all smartphones are powered by Linux? TBH, Linux has been touted as the future since the 90s, and we have been hearing “Year of the Linux Desktop” for quite some time. On the desktop space, however, it never really materialized. But from what I have observed, it finally seems like Linux is actually taking hold and getting there in the last decade. WSL from Microsoft is a testament to that, IMO. The fact that we are seeing more and more laptops shipping Linux OOB is also a testament to the fact that Linux on the desktop is finally happening, albeit a bit slower than we expected.  I wouldn’t be surprised if one day Microsoft just ditches windows and ships their own Linux distro. Well, it’s probably a distant dream as Windows still holds around 75% market share in desktop OS and macOS stands around 16% with Linux and derivatives taking the remaining 9%. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Linux Scoop "},{id:28,url:"https://deepu.tech/what-about-svelte/",title:"What about Svelte? Should you care!",body:"2021/03/18 - Svelte is one of the latest cool-and-shiny client-side frameworks in the JavaScript world. Svelte lets you write blazing fast web applications with minimal boilerplate, reduced complexity, and smaller bundle size. Yes, we have heard the same promise many times before as well from other cool-and-shiny projects like Angular, React, Vue, and others. So how is Svelte any different and why should we care? Let’s dive into that What is Svelte?:  Svelte is a radical new approach to building user interfaces. Whereas traditional frameworks like React and Vue do the bulk of their work in the browser, Svelte shifts that work into a compile step that happens when you build your app.  Instead of using techniques like virtual DOM diffing, Svelte writes code that surgically updates the DOM when the state of your app changes.  – Svelte docs As per Svelte docs, Svelte is a component framework similar to React or Vue. But the most unique thing about Svelte is that it’s a compiler first and a UI framework second. You basically provide template files containing standard-compliant HTML, CSS, and JS/TS for your components with a . svelte extension. This is compiled by Svelte into highly-optimized vanilla JavaScript at build time that runs in a browser with minimal framework overhead. Frameworks like React revolutionized client-side engineering by popularizing the concept of the Virtual DOM. This made web applications faster, more responsive, and slick. Gone were the days of JQuery and DOM manipulation hell. But this also meant doing a lot more work for the reconciliation of the DOM on the browser and using a lot more memory. This in turn means having to ship the library, like React, as well on your production code making bundles bigger and downloads slower. Svelte on the other hand took a different route and introduced a compiler that does all of the heavy liftings during the build phase. It converts components into efficient imperative vanilla JS code that surgically updates the DOM at runtime. This removed the need for a middle man, like Virtual DOM, at the runtime making web applications faster and lighter. There are still some Svelte specific code that ends up on your application on the browser but it’s very minimal when compared to React, Vue, or Angular So let’s see why you should give Svelte a chance Why Svelte?: Svelte may not be as revolutionary as AngularJS or React during our JQuery days, as in the end, it pretty much does the same job that most other MVVM frameworks like, React, Vue, or Angular does. To be fair, Svelte builds upon lessons learned from its predecessors. But that’s not the point. What makes it different is the approach it takes and the advantages it hence provides which could be subtle or prominent based on your exact use case. Advantages: Let us see what are the advantages Svelte offers: Compiler vs Virtual DOM: Being a compiler and getting rid of the VirtualDOM is the most important advantage of Svelte that facilitates many of the other advantages we will see below. The concept is becoming so popular that Angular and Ember have started moving towards compilation in their recent versions. Lightweight &amp; Performant: Svelte produces highly optimized vanilla JS with a very minimal overhead at runtime. This means small bundle sizes, a low memory footprint, and a fast-loading and fast-running application. Check the performance benchmarks here to see the difference. All this is out of the box without even having to do any tuning and there are many ways to improve performance even further. Less boilerplate: With Svelte there is no need for adding glue code like hooks or complex state management and so on. The boilerplate required for components is very minimal and almost close to vanilla HTML/JS. Svelte also supports optional two-way bindings making it easier to build forms. Below is a simple component in Svelte with a two-way input binding, it can’t get simpler than this! 123456789101112&lt;style&gt; h1 {  color: blue; }&lt;/style&gt;&lt;script&gt; let name =  world ;&lt;/script&gt;&lt;input bind:value= {name}  /&gt;&lt;h1&gt;Hello {name}!&lt;/h1&gt;Truly reactive: Svelte is reactive by default. the DOM is automatically updated on state changes in any top-level variable on a component. You don’t even have to add any special code for that. Only direct top-level assignments work this way and reference mutations like array. push won’t work. This means mutations would be more explicit and easier to understand in my opinion. Svelte also supports derived declarations and statements that are recomputed on state change with a special label ($:). Here is an example: 123456789101112131415161718&lt;script&gt; let count = 0; // reactive top-level variable $: doubled = count * 2; // derived declaration // derived statement $: {  console. log(`the count is ${count}`); } // when variable is updated the DOM is updated accordingly function handleClick() {  count += 1; }&lt;/script&gt;&lt;button on:click= {handleClick} &gt; &lt;p&gt;{count} doubled is {doubled}&lt;/p&gt;&lt;/button&gt;Low learning curve: Unlike React or Angular, the learning curve for Svelte is quite low. There is no special syntax like JSX to learn or complex APIs like Angular to remember. Everything is written using standard-compliant JS/TS, CSS, and HTML with some additional syntax sugar for directives and template logic. The component API is simple and straightforward. The documentation is also quite good and easy to follow. For example, it took me just a few days to get comfortable with Svelte even for advanced concepts like life cycles, composition, and so on. Whereas it took months for me to get really comfortable in React and I still don’t know half of the Angular APIs even after using it for almost a year. Of course, knowing React or Angular does help in making it easier to learn Svelte as there are a lot of concepts that are similar. Components pattern: Svelte follows a component first pattern which makes it ideal for building new web applications or for adding web components to existing applications. Styles are scoped to components by default making Svelte ideal for web components. Built-in animations and effects: Svelte provides built-in animations and effects which makes it easier to build slick user interfaces and interactive visualizations. Well, the framework was originally created for building interactive graphics for The Guardian. This approach provides a much nicer developer experience than something like React and is way easier to use. Here is a simple example of using a transition effect: 12345678910111213&lt;script&gt; import { fade } from  svelte/transition ; let visible = true;&lt;/script&gt;&lt;label&gt; &lt;input type= checkbox  bind:checked= {visible}  /&gt; visible&lt;/label&gt;{#if visible}&lt;p transition:fade&gt;Fades in and out&lt;/p&gt;{/if}Built-in Reactive store: Svelte provides both mutable and immutable reactive stores out of the box making it easier to do more complex state management in your application. The stores support manual and automatic subscriptions and two-way bindings making them very flexible. The implementation also makes it possible to switch to another state management solution like RxJS for example. Let’s see an example of a writable store: 1234567891011121314151617&lt;script&gt; // you can also use readable or derived stores import { writable } from  svelte/store ; // ideally you should do this in a different file export const count = writable(0); // using manual subscription let count_value; const unsubscribe = count. subscribe((value) =&gt; {  count_value = value; });&lt;/script&gt;&lt;h1&gt;The count is {count_value}&lt;/h1&gt;&lt;!-- The same can be done with auto subscription like below --&gt;&lt;h1&gt;The count is {$count}&lt;/h1&gt;Multiple output targets: Being a compiler, it is easy to change output targets without having to change your component’s code. For example, Svelte supports server-side rendering out of the box by providing a compiler mode for it (dom vs ssr). There is even a NativeScript integration for Svelte that makes use of this flexibility to produce targets beyond dom and ssr. There is also the Sapper framework, soon to be SvelteKit, from Svelte team, which is similar to Next. js but optimized to work with Svelte’s philosophy. Sapper supports SSR, Progressive Web Apps, code-splitting, and so on. Disadvantages: All those advantages we saw above don’t mean there are no downsides, every framework makes tradeoffs. The major downsides of Svelte are: Young framework: Svelte is very young and that means it is not as battle-tested as React or Angular and you might run into some walls at times. This means it’s probably not suitable for very complex or mission-critical applications that are expected to scale. This might not be a long-term problem as the framework is exploding in popularity and the introduction of Sapper has helped with the scaling concerns. Though, the confusion around Sapper vs SvelteKit could have been avoided, in my opinion. Smaller community and ecosystem: Being a young framework means it has a smaller community and user base along with a smaller ecosystem. So you may not find as many tools or libraries as in React or as much help on Stack Overflow when you are stuck on some complex problem. Compilation heavy: If you are involved in the front-end development space as well for a while, like me, you could look back and see that UX has improved miles, but the toolchain to produce that has grown to be very complex. For example, at JHipster, we try to provide the best in class production-grade set up for a full stack web app with React/Vue/Angular front end and Java/Kotlin/. NET/NodeJS backend. When you create a new app and compile it, you will see that the front-end takes 10x more time than the backend to compile. This is the new normal in any full-stack web apps these days and Svelte also has the same issue. It’s compiler heavy and the more complex your app becomes the more complex and time-consuming the build will become. This also means you can’t just drop a JS file into a webpage and expect to make it a Svelte app like you can do with Vue. By the way, JHipster also has Svelte support. Quirks: Svelte is web standards compliant, it doesn’t introduce anything new like JSX. But it does change some of the standard semantics to work in a different way and this could be confusing for new users. For example, it uses export keyword differently and there are quirks like having to use on:click instead of onClick and so on. But those are almost unavoidable in any framework. It also uses a JS label ($:) to make derived statements/declarations work, it could look alien as some JS developers probably don’t even know that labels exist in JS as we rarely use it. Conclusion: Of course, all this just scratches the surface of what Svelte offers. If you are coming from other frameworks like React, Angular, or Vue, you will find that Svelte provides a lot of similar features that you are already familiar with. I’m glad that Svelte didn’t just try to re-invent the wheel on everything. I have built complex-mission-critical apps on production with JQuery, AngularJS, Angular, and React. I have also dabbed with Vue and few other lesser-known frameworks in the past. Having matured into a polyglot developer, I have stopped marrying languages/frameworks and hence I have no blind loyalty to any frameworks. I just pick what I think is the best fit for the problem at hand. I used to pick React for random stuff in the past but I think I’ll give Svelte a chance next time. In my opinion, React would continue to be a big challenger for Svelte, especially with the new concurrent mode that promises non-blocking rendering and hence no dropped frames. If you already have an app on an MVVM framework like React, Vue, or Angular, there is no justifiable benefit for you to switch to Svelte as those frameworks can also get the job done with very similar performance. Each framework has its own pros and cons which could be situational based on your use case. But if you are going to build a new application or add web components to an existing application then Svelte could be a great choice, especially for web components due to Svelte’s simple and lightweight nature. If you are convinced and want to dive in here is a great resource to get started. you can also get started directly with Sapper if you like. So keep Svelte in your mind and consider giving it a chance for your next greenfield project. References:  svelte. dev www. codecademy. com blog. logrocket. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:29,url:"https://deepu.tech/concurrency-in-modern-languages-ts/",title:"Concurrency in modern programming languages: TypeScript on Deno",body:"2021/02/19 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksIf you are new to Deno, check this post out to get an idea.                               Forget NodeJS! Build native TypeScript applications with Deno 🦖:             18-Feb-2020                           #typescript #javascript #nodejs #deno                                    Concurrency in Deno: Concurrency in TypeScript is exactly the same as in JavaScript as TypeScript is a strict superset of JavaScript. So I suggest you read the below post first to get an idea of concurrency in JavaScript. I’ll be covering only the differences between NodeJS and Deno here.                               Concurrency in modern programming languages: JavaScript on NodeJS:             05-Feb-2021                           #languages #concurrency #nodejs #javascript                                    So if you use TypeScript with NodeJS it’s exactly the same as using JavaScript on NodeJS as NodeJS doesn’t run TypeScript natively and we have to transpile it down to JavaScript so let’s focus on TypeScript on Deno since we already covered NodeJS. Unlike NodeJS, Deno can run TypeScript natively, it’s transpiled to JS behind the scenes. As we saw with NodeJS, Deno is also focused on non-blocking IO with an aim of improving/fixing issues in NodeJS. This means you can do everything that you can do with NodeJS and JavaScript on Deno as well, with nicer APIs and less code sometimes. Like in JS you rely on the event loop, Callbacks, Promises, and Async/Await to achieve concurrency in TypeScript. And the Deno APIs are async by default and promote using async/await a lot.  The default for concurrency in Deno is an asynchronous programming model using callbacks, Promise or async/await. Like in JavaScript, it’s possible to do some level of multi-threaded concurrency and parallelization with TypeScript on Deno as well and since Deno is built on Rust may be in the future concurrency performance might be better than that on NodeJS. Multi-threading: JavaScript is single-threaded hence the only way to achieve multi-threading is by spinning up multiple instances of the JS Engine hence the same goes for TypeScript as well. We looked at the Web Worker API in JS and similar support in NodeJS so when it comes to Deno, it also supports the Web Worker API. This means it is possible to offload heavy computations to a separate thread thus freeing up the main thread. These workers and the main thread communicate using events and a worker thread can spawn other worker threads. Unfortunately, Deno doesn’t offer something similar to the NodeJS worker_threads or the cluster module yet and using web workers makes things more complicated as Deno only supports modules as workers, which means you can only invoke a JS/TS file from a worker. Hence some of the advanced multi-threading concepts possible in NodeJS is not feasible with Deno yet. It’s also worth noting that Deno supports Web Assembly out of the box hence paving way for some advanced multi-threading using a language like Rust. Asynchronous processing: As we saw for NodeJS, Deno is also heavily geared towards non-blocking and asynchronous programming and it improves upon the concepts and makes asynchronous APIs cleaner and easier to use. Deno provides a Promises-based API rather than using callbacks which is a differentiator when compared to NodeJS. Deno even supports concepts like top-level await which reduces clutter and makes the code cleaner. 123456789// sequential (you don't need to wrap this in an async function in Deno)const users = await getUsers();const posts = await getPosts();const messages = await getMessages();// do stuff// concurrent (you don't need to wrap this in an async function in Deno)const [users, posts, messages] = await Promise. all([getUsers(), getPosts(), getMessages()]);// do stuffBenchmarking: Now that we have some basic understanding of concurrency features in Deno for TypeScript, let us build a simple concurrent web server in TypeScript. Since asynchronous concurrency is the best way to achieve this in Deno we’ll build a sample application using the standard http module. The Deno version used is the latest (1. 18. 1) at the time of writing. Async HTTP concurrent webserver: This example is closer to the Rust Asynchronous example we built in the second chapter. You can find the full example on GitHub here. We are only using standard Deno modules in this case. 12345678910111213141516171819202122232425262728293031import { serve } from  https://deno. land/std/http/server. ts ;let count = 0;const handleRequest = async (request: Request): Promise&lt;Response&gt; =&gt; { count++; // add 2 second delay to every 10th request if (count % 10 === 0) {  console. log( Adding delay. Count:  , count);  await sleep(2000); } // read html file const body = await Deno. readTextFile( . /hello. html ); const res = {  status: 200,  headers: new Headers(), }; res. headers. set( Connection ,  keep-alive ); return new Response(body, res); // send data to client side};// sleep function since NodeJS doesn't provide onefunction sleep(ms: number) { return new Promise((resolve) =&gt; {  setTimeout(resolve, ms); });}// set listen port and listen to all incoming requestsawait serve(handleRequest, { hostname:  0. 0. 0. 0 , port: 8080 });console. log(`HTTP webserver running at: http://localhost:8080/`);As you can see we create an HTTP server and bind it to port 8080 and listen to all incoming requests in a for await loop. Each request is processed in a function that internally uses async/await. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 12345678910111213141516171819202122232425262728293031323334353637383940ab -k -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Server Software:Server Hostname:    127. 0. 0. 1Server Port:      8080Document Path:     /Document Length:    174 bytesConcurrency Level:   100Time taken for tests:  20. 393 secondsComplete requests:   10000Failed requests:    0Total transferred:   3150000 bytesHTML transferred:    1740000 bytesRequests per second:  490. 37 [#/sec] (mean)Time per request:    203. 927 [ms] (mean)Time per request:    2. 039 [ms] (mean, across all concurrent requests)Transfer rate:     150. 85 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 3   0    5Processing:   0 202 600. 3   1  2029Waiting:    0 202 600. 3   1  2029Total:     0 202 600. 3   1  2029Percentage of the requests served within a certain time (ms) 50%   1 66%   1 75%   2 80%   3 90%  2001 95%  2002 98%  2002 99%  2003 100%  2029 (longest request)One minor thing to note is that I had to pass the -k flag to ApacheBench for this to run all requests. I still couldn’t figure out why it behaves this way. A very similar implementation in every other language worked fine without the flag. So if someone has an idea let me know. Conclusion: As I explained in the first part of this serious, this simple benchmarking is not an accurate representation for all concurrency use cases. It’s a simple test for a very particular use case, a simple concurrent web server that just serves a file. The idea is to see the differences in solutions and to understand how concurrency works in JavaScript/TypeScript specifically on Deno. And for this particular use case,since there is no clean way to do a multi-threaded server in Deno the http module provided by the standard library with asynchronous programming seems to be the way to go. So stay tuned for the next post where we will look at concurrency in JVM and build the same use case in Java. References:  morioh. com deno. landIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Jeffrey Brandjes on Unsplash "},{id:30,url:"https://deepu.tech/concurrency-in-modern-languages-js/",title:"Concurrency in modern programming languages: JavaScript on NodeJS",body:"2021/02/05 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksConcurrency in JavaScript &amp; NodeJS:  JavaScript has a concurrency model based on an event loop, which is responsible for executing the code, collecting and processing events, and executing queued sub-tasks. This model is quite different from models in other languages like C and Java.  – MDN Web Docs Concurrency in JavaScript is quite different from other languages we are looking at. The biggest difference is that JavaScript is single-threaded and hence multi-threading and parallelism is out of the question – at least not traditional multi-threading or parallelism like in other languages, we will come to that later. But concurrency on the other hand is the heart and soul of the JavaScript event loop. It is how JavaScript is able to bridge the gap of multi-threading making JavaScript a serious contender in an arena filled with multi-threaded languages like Java, Go, Python, Rust, and so on. Read the introduction chapter to see why the difference matters. The JavaScript event loop relies on message passing concurrency to execute items on its stack in a non-blocking way, more like, it gives a perception of being non-blocking, because, in reality, it does block for a short moment since its single-threaded and execution is interleaved. This makes it perfect for most of the concurrency use cases you would encounter and you will rarely miss having multi-threading and true parallelism support and in many use cases, you can go head to head with a fully multi-threaded language. While languages like Rust offer flexibility and power and Go offers simplicity and performance, JavaScript was never meant for concurrency but that didn’t stop the awesome people behind NodeJS to come up with a platform that was fully focused on concurrency and non-blocking I/O. While JavaScript event loop already paved the way for it, NodeJS made JS a truly viable option for server-side concurrency. All this is made possible thanks to the event loop, Callbacks, Promises, and Async/Await support in JavaScript. Unlike in many other languages, maybe except Go, concurrency is used a lot in JavaScript by developers, sometimes without even realizing it, especially in NodeJS applications. It’s due to the programming model followed by NodeJS which relies heavily on callbacks and Promises and also because it is extremely easy to do so due to the flexible nature of JavaScript. The NodeJS standard library also uses concurrency where ever possible and there is no much overhead in doing so.  The default for concurrency in JavaScript is an asynchronous programming model using callbacks, Promise or async/await. With JavaScript, it’s possible to do some level of multi-threaded concurrency and parallelization. It’s not as flexible as Rust or as simple as Go, but still, you can achieve almost everything possible in other languages and still get quite decent performance for most of those use cases. Multi-threading: JavaScript is single-threaded hence the only way to achieve multi-threading is by spinning up multiple instances of the JS Engine. But then how do you communicate between these instances? That is where Web Workers come in.  Web Workers makes it possible to run a script operation in a background thread separate from the main execution thread of a web application  – MDN Web Docs With the help of web workers, it is possible to offload heavy computations to a separate thread hence freeing up the main thread. These workers and the main thread communicate using events and a worker thread can spawn other worker threads. Now when it comes to NodeJS, there are few ways to spawn additional threads and processes. There is the classical child_process module, the more modern worker_threads module which is quite similar to web workers, and the cluster module for creating clusters of NodeJS instances.  Workers (threads) are useful for performing CPU-intensive JavaScript operations. They do not help much with I/O-intensive work.  – NodeJS docs Be it web workers or worker threads, they are not as flexible or easy as the multi-threading implementations in other languages and has many limitations and hence these are mostly used only when there are CPU intensive tasks or background tasks to be performed for other use cases concurrency using asynchronous processing would be sufficient. JavaScript doesn’t provide access to OS threads or green threads, the same applies for NodeJS however worker threads and the cluster comes close, and hence advanced multi-threading is not feasible. Message-passing concurrency is possible and is used by the JS event loop itself and can be used for both workers and the standard concurrency model in JS. Shared-state concurrency is possible in the standard concurrency model and with workers using array buffers. Asynchronous processing: Technically asynchronous programming is not part of concurrency but in practice, it goes hand in hand for many use cases and improves performance, and makes resource usage more efficient. Since the JavaScript event loop is non-blocking and asynchronous, it’s perfect for asynchronous programming and that’s why the concept is so popular among JS developers than in other languages and is heavily used especially in NodeJS and on the client-side with modern SPA frameworks. In a way, asynchronous programming was made popular by JavaScript and NodeJS. JavaScript provides 3 ways to do asynchronous programming, Callbacks, Promises, and Async/Await. In recent times Async/Await is becoming more widely used instead of or together with promises and callbacks due to its cleaner approach (Read, no callback hell). In NodeJS all I/O operations are async(non-blocking) by default, you would have to use sync alternatives provided to do synchronous I/O. This makes it easy to write concurrent applications in NodeJS using these constructs. 12345678910111213// sequentialasync function load() { const users = await getUsers(); const posts = await getPosts(); const messages = await getMessages(); // do stuff}// concurrentasync function load() { const [users, posts, messages] = await Promise. all([getUsers(), getPosts(), getMessages()]); // do stuff}Benchmarking: Now that we have some basic understanding of concurrency features in JavaScript &amp; NodeJS, let us build a simple concurrent web server in NodeJS. Since asynchronous concurrency is the best way to achieve this in NodeJS we’ll build a sample application using the standard http module and then improve it using the cluster module. The NodeJS version used is the latest (15. 8. 0) at the time of writing. Async HTTP concurrent webserver: This example is closer to the Rust Asynchronous example we built in the second chapter. You can find the full example on GitHub here. We are not using any external dependency in this case. 123456789101112131415161718192021222324252627282930313233343536const http = require( http );const fs = require( fs ). promises;let count = 0;// set routerconst server = http. createServer((req, res) =&gt; { count++; requestListener(req, res, count);});const host =  localhost ;const port = 8080;// set listen portserver. listen(port, host, () =&gt; { console. log(`Server is running on http://${host}:${port}`);});const requestListener = async function (req, res, count) { // add 2 second delay to every 10th request if (count % 10 === 0) {  console. log( Adding delay. Count:  , count);  await sleep(2000); } const contents = await fs. readFile(__dirname +  /hello. html ); // read html file res. setHeader( Connection ,  keep-alive ); res. writeHead(200); // 200 OK res. end(contents); // send data to client side};function sleep(ms) { return new Promise((resolve) =&gt; {  setTimeout(resolve, ms); });}As you can see we create an HTTP server and bind it to port 8080 and listen to all incoming requests. Each request is processed in a callback function that internally uses async/await. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 12345678910111213141516171819202122232425262728293031323334353637383940ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Server Software:Server Hostname:    127. 0. 0. 1Server Port:      8080Document Path:     /Document Length:    174 bytesConcurrency Level:   100Time taken for tests:  21. 329 secondsComplete requests:   10000Failed requests:    0Total transferred:   2540000 bytesHTML transferred:    1740000 bytesRequests per second:  468. 85 [#/sec] (mean)Time per request:    213. 286 [ms] (mean)Time per request:    2. 133 [ms] (mean, across all concurrent requests)Transfer rate:     116. 30 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 9   0   11Processing:   0 207 599. 9   4  2048Waiting:    0 206 600. 0   3  2038Total:     0 208 599. 9   4  2048Percentage of the requests served within a certain time (ms) 50%   4 66%   8 75%   13 80%   19 90%  2000 95%  2004 98%  2012 99%  2017 100%  2048 (longest request)Let’s see if there is a difference in performance with multi-threading using the cluster package. Multi-threaded webserver cluster: This example is quite similar to the previous one except that we are using the cluster module to fork into master and worker threads, one worker per CPU thread. We are still using the http module and callbacks here. You can find the full example on GitHub here. We are not using any external dependency in this case as well. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152const http = require( http );const fs = require( fs ). promises;const cluster = require( cluster );const numCPUs = require( os ). cpus(). length;let count = 0;// set routerconst server = http. createServer((req, res) =&gt; { count++; requestListener(req, res, count);});const host =  localhost ;const port = 8080;if (cluster. isMaster) { console. log(`Master ${process. pid} is running`); // Fork workers.  for (let i = 0; i &lt; numCPUs; i++) {  cluster. fork(); } cluster. on( exit , (worker, code, signal) =&gt; {  console. log(`worker ${worker. process. pid} died`); });} else { // set listen port, TCP connection is shared by all workers server. listen(port, host, () =&gt; {  console. log(`Worker ${process. pid}: Server is running on http://${host}:${port}`); });}const requestListener = async function (req, res, count) { // add 2 second delay to every 10th request if (count % 10 === 0) {  console. log( Adding delay. Count:  , count);  await sleep(2000); } const contents = await fs. readFile(__dirname +  /hello. html ); // read html file res. setHeader( Connection ,  keep-alive ); res. writeHead(200); // 200 OK res. end(contents); // send data to client side};// sleep function since NodeJS doesn't provide onefunction sleep(ms) { return new Promise((resolve) =&gt; {  setTimeout(resolve, ms); });}As you can see we used the same code from previous example and moved the server. listen to the worker nodes. The cluster module forks into master and workers. We assign a callback function to handle each request which internally calls the requestListener method. The code here is a bit more complex compared to the previous one. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 12345678910111213141516171819202122232425262728293031323334353637383940ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Server Software:Server Hostname:    127. 0. 0. 1Server Port:      8080Document Path:     /Document Length:    174 bytesConcurrency Level:   100Time taken for tests:  21. 075 secondsComplete requests:   10000Failed requests:    0Total transferred:   2540000 bytesHTML transferred:    1740000 bytesRequests per second:  474. 50 [#/sec] (mean)Time per request:    210. 747 [ms] (mean)Time per request:    2. 107 [ms] (mean, across all concurrent requests)Transfer rate:     117. 70 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 8   0   11Processing:   0 206 600. 1   4  2047Waiting:    0 205 600. 1   3  2045Total:     1 206 600. 1   4  2047Percentage of the requests served within a certain time (ms) 50%   4 66%   8 75%   11 80%   14 90%   88 95%  2005 98%  2012 99%  2016 100%  2047 (longest request)We got almost identical results here. If you look close, the multi-threaded one is slightly faster but then the added complexity might not be worth it for all use cases. So it might be preferable to just use the http package without clustering for such use cases as it’s cleaner. Conclusion: As I explained in the first part of this serious, this simple benchmarking is not an accurate representation for all concurrency use cases. It’s a simple test for a very particular use case, a simple concurrent web server that just serves a file. The idea is to see the differences in solutions and to understand how concurrency works in JavaScript specifically on NodeJS. And for this particular use case, an asynchronous server using the http module provided by the standard library does seem to be the best choice. So stay tuned for the next post where we will look at concurrency in Deno and build the same use case in TypeScript. References:  developer. mozilla. org tsh. io medium. com/@onejohi blog. logrocket. com blog. logrocket. com medium. com/ideas-at-igeniusIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Michał Parzuchowski on Unsplash "},{id:31,url:"https://deepu.tech/concurrency-in-modern-languages-go/",title:"Concurrency in modern programming languages: Golang",body:"2021/01/21 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksConcurrency in Go:  Do not communicate by sharing memory; instead, share memory by communicating.  – Go docs Go supports concurrency as a first-class citizen with its goroutines. Go takes the concept of coroutines to a whole new level by making it much simpler and the preferred way of doing almost anything in Go. The semantic and syntax are made so easy that even a Go newbie will be able to start using goroutines from the get-go easily. All this without sacrificing performance. While languages like Rust offers flexibility and power over simplicity, Go focuses on simplicity and performance. While languages like Rust provide you with building blocks required for concurrency Go provides implementations that can be used easily to achieve most of the concurrency use cases. The Go standard library also uses goroutines where ever possible. This allows us to use goroutines and improve performance even for trivial use cases as there is not much overhead from the complexity of using concurrency semantics.  The default for concurrency in Go is an asynchronous programming model and hence there are no explicit language features like async/await. Using concurrency with synchronous programming would be way more difficult in Go With Go, it’s possible to do multi-threaded concurrency and parallelization with goroutines and goroutines work in an asynchronous way hence making use of both multi-threading and asynchronous programming efficiently. It’s not as flexible as Rust as we saw in the previous chapter, but still, you can achieve almost everything that is possible in Rust and still get the best possible performance for those use case. Multi-threading: Multi-threading is achieved with goroutines that provide green threads (virtual threads that are scheduled by a runtime or virtual machine and not managed by the operating system) and there are no ways to use OS threads directly like in Rust for example. Go also provides implementations required for message-passing concurrency using channels and shared-state concurrency using mutexes and WaitGroups, though Go discourages shared state concurrency. Parallelization is also possible using goroutines. Asynchronous processing: Technically asynchronous programming is not part of concurrency but in practice, it goes hand in hand for many use cases and improves performance, and makes resource usage more efficient. This is the default behavior in Go when using goroutines and hence there is no special syntax around these so synchronous programming with goroutines is not commonly done and would require using WaitGroups and such. Benchmarking: Now that we have some basic understanding of concurrency features in Golang, let us build a simple concurrent web server in Go. Since goroutines are the only way to achieve this we’ll build two sample applications using the http package and plain tcp. The Go version used is the latest (1. 15. 6) at the time of writing. TCP concurrent webserver: This example is closer to the Rust Asynchronous example we built in the previous chapter. I have omitted import statements for brevity. You can find the full example on GitHub here. We are not using any external dependency in this case. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func main() {\tvar count = 0\tl, err := net. Listen( tcp ,  127. 0. 0. 1:8080 ) // set listen port\tif err != nil {\t\tlog. Fatal( Error listening:  , err)\t}\tdefer l. Close() // close connection when done\tfor {\t\tcount++\t\t// Listen for an incoming connection. \t\tif conn, err := l. Accept(); err != nil {\t\t\tlog. Fatal( Error accepting:  , err)\t\t} else {\t\t\t// Handle connections in a new goroutine. \t\t\tgo handleConnection(conn, count)\t\t}\t}}func handleConnection(conn net. Conn, count int) {\t// Close the connection when you're done with it. \tdefer conn. Close()\t// Read the incoming connection into a buffer. \tbuf := make([]byte, 1024)\tif _, err := conn. Read(buf); err != nil {\t\tlog. Fatal( Error reading: , err)\t}\t// add 2 second delay to every 10th request\tif (count % 10) == 0 {\t\tprintln( Adding delay. Count:  , count)\t\ttime. Sleep(2 * time. Second)\t}\thtml, _ := ioutil. ReadFile( hello. html ) // read html file\t// Send a response back\theader := `HTTP/1. 0 200 OKConnection: keep-aliveContent-Length: 174Content-Type: text/html; charset=utf-8\t`\tres := fmt. Sprintf( %s\r\n\r\n%s , header, string(html))\tconn. Write([]byte(res))}As you can see we bind a TCP listener to port 8080 and listen to all incoming requests in an endless loop. Each request is processed in a new goroutine by using the go statement. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 123456789101112131415161718192021222324252627282930313233343536ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    174 bytesConcurrency Level:   100Time taken for tests:  20. 208 secondsComplete requests:   10000Failed requests:    0Total transferred:   2780000 bytesHTML transferred:    1740000 bytesRequests per second:  494. 86 [#/sec] (mean)Time per request:    202. 075 [ms] (mean)Time per request:    2. 021 [ms] (mean, across all concurrent requests)Transfer rate:     134. 35 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  1  1. 0   0    9Processing:   0 201 600. 0   0  2012Waiting:    0 201 600. 0   0  2011Total:     0 201 600. 0   0  2015Percentage of the requests served within a certain time (ms) 50%   0 66%   1 75%   2 80%   3 90%  2000 95%  2001 98%  2001 99%  2002 100%  2015 (longest request)Let’s see if there is a difference in performance when using the http package. HTTP concurrent webserver: This example is quite similar to the previous one except that we are using an HTTP server here and we are not invoking goroutines directly instead we pass a callback function to http. HandleFunc and it internally executes it as a goroutine. I have omitted import statements for brevity. You can find the full example on GitHub here. We are not using any external dependency in this case as well and http is part of the Go standard library. 1234567891011121314151617181920212223242526func main() {\tvar count = 0\t// set router\thttp. HandleFunc( / , func(w http. ResponseWriter, r *http. Request) {\t\tdefer r. Body. Close()\t\tcount++\t\thandleConnection(w, count)\t})\t// set listen port\terr := http. ListenAndServe( :8080 , nil)\tif err != nil {\t\tlog. Fatal( ListenAndServe:  , err)\t}}func handleConnection(w http. ResponseWriter, count int) {\t// add 2 second delay to every 10th request\tif (count % 10) == 0 {\t\tprintln( Adding delay. Count:  , count)\t\ttime. Sleep(2 * time. Second)\t}\thtml, _ := ioutil. ReadFile( hello. html ) // read html file\tw. Header(). Add( Connection ,  keep-alive )\tw. WriteHeader(200)      // 200 OK\tfmt. Fprintf(w, string(html)) // send data to client side}As you can see we created an HTTP server bound to port 8080 and listen to all incoming requests. We assign a callback function to handle each request which internally calls the handleConnection method. The code here is much more clean and concise compared to the previous one. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 1234567891011121314151617181920212223242526272829303132333435363738ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    174 bytesConcurrency Level:   100Time taken for tests:  20. 232 secondsComplete requests:   10000Failed requests:    0Total transferred:   2910000 bytesHTML transferred:    1740000 bytesRequests per second:  494. 27 [#/sec] (mean)Time per request:    202. 319 [ms] (mean)Time per request:    2. 023 [ms] (mean, across all concurrent requests)Transfer rate:     140. 46 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  1  0. 9   0    6Processing:   0 201 600. 0   1  2013Waiting:    0 201 600. 0   0  2013Total:     0 202 600. 0   1  2018WARNING: The median and mean for the initial connection time are not within a normal deviation    These results are probably not that reliable. Percentage of the requests served within a certain time (ms) 50%   1 66%   1 75%   2 80%   3 90%  2000 95%  2001 98%  2002 99%  2003 100%  2018 (longest request)We got almost identical results here. So it might be preferable to just use the HTTP package for such use cases as it’s cleaner. Conclusion: As I explained in the first part of this serious, this simple benchmarking is not an accurate representation for all concurrency use cases. It’s a simple test for a very particular use case, a simple concurrent web server that just serves a file. The idea is to see the differences in solutions and to understand how concurrency works in Golang. And for this particular use case, the http package provided by the standard library does seem to be the best choice. So stay tuned for the next post where we will look at concurrency in NodeJS and build the same use case in JavaScript. References:  medium. com/@damithadayananda medium. com/@gauravsingharoyIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Peggy Anke on Unsplash "},{id:32,url:"https://deepu.tech/concurrency-in-modern-languages-rust/",title:"Concurrency in modern programming languages: Rust",body:"2020/12/24 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below and I’ll try to publish them weekly.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksConcurrency in Rust:  Handling concurrent programming safely and efficiently is another of Rust’s major goals.  – Rust docs Efficient and memory safe concurrency is one of the major goals of Rust and these are not just plain words, the language offers great features for concurrent programming and when combined with the best in class memory safety model makes it a great choice for concurrency use cases. As with everything else in Rust, the idea is that you spend more time upfront (read compile-time) fixing issues rather than spending time fixing issues in production (read runtime). So if you are new to Rust it might look like more time spent on writing code but it will considerable effort later on by avoiding a lot of issues that generally pop up in languages with not-so-great memory safety. The Rust team calls this “fearless concurrency”.  As with everything else in Rust the idea is that you spend more time upfront (read compile-time) fixing issues rather than spending time fixing issues in production (read runtime). There are other languages like Go, which offers simpler and equally performant solutions for concurrency but they aren’t as powerful as Rust due to the flexibility offered by Rust. Basically, Rust provides you with building blocks required for concurrent, parallel, and asynchronous programming and you can extend or implement different solutions as you see fit or use a solution offered by the community. This allows for one to use the best possible solution for the use case rather than using the same hammer for all jobs. With Rust, it’s possible to do multi-threaded concurrency or parallel programming as well as asynchronous programming. This means as we saw in the previous chapter, we can mix and match these models to get the best possible performance for any use case. Multi-threading: Rust provides building blocks to create and manage OS threads as part of the standard library and it also provides implementations required for message-passing concurrency (similar to Go) using channels and shared-state concurrency using Mutexes and Smart pointers. Rust’s type system and ownership model helps to avoid common concurrency issues like data race, locks, etc. Asynchronous processing: Technically asynchronous programming is not part of concurrency but in practice, it goes hand in hand for many use cases and improves performance, and makes resource usage more efficient. The latest versions of Rust provides building blocks and language features required for asynchronous programming with the async/. await syntax. But do keep in mind that using an asynchronous programming model increases the overall complexity and the ecosystem is still evolving. While Rust provides the language features required the standard library doesn’t provide any implementations needed and hence you would have to use an external crate like Futures to be able to use the asynchronous programming model effectively. Benchmarking: Now that we have some basic understanding of concurrency features in Rust, let us build a simple concurrent webserver in Rust. Since Rust offers multiple ways to achieve this we’ll be building three sample applications and comparing them. The Rust version used is the latest (1. 48. 0) at the time of writing. Multi-threaded concurrent webserver: This example is closer to the example from the official Rust book, I have omitted import statements for brevity. You can find the full example on GitHub here. The ThreadPool struct is exactly the same as in the Rust book. We are not using any external dependency in this case. 12345678910111213141516171819202122232425262728293031323334353637383940fn main() {  let listener = TcpListener::bind( 127. 0. 0. 1:8080 ). unwrap(); // bind listener  let pool = ThreadPool::new(100); // same number as max concurrent requests  let mut count = 0; // count used to introduce delays  // listen to all incoming request streams  for stream in listener. incoming() {    let stream = stream. unwrap();    count = count + 1;    pool. execute(move || {      handle_connection(stream, count); // spawning each connection in a new thread    });  }}fn handle_connection(mut stream: TcpStream, count: i64) {  // Read the first 1024 bytes of data from the stream  let mut buffer = [0; 1024];  stream. read(&amp;mut buffer). unwrap();  // add 2 second delay to every 10th request  if (count % 10) == 0 {    println!( Adding delay. Count: {} , count);    thread::sleep(Duration::from_secs(2));  }  let header =  HTTP/1. 0 200 OKConnection: keep-aliveContent-Length: 174Content-Type: text/html; charset=utf-8   ;  let contents = fs::read_to_string( hello. html ). unwrap();  let response = format!( {}\r\n\r\n{} , header, contents);  stream. write(response. as_bytes()). unwrap(); // write response  stream. flush(). unwrap();}As you can see we bind a TCP listener to port 8080 and listen to all incoming requests. Each request is processed in a new thread provided by a ThreadPool. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 123456789101112131415161718192021222324252627282930313233343536ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 173 secondsComplete requests:   10000Failed requests:    0Total transferred:   2830000 bytesHTML transferred:    1760000 bytesRequests per second:  495. 72 [#/sec] (mean)Time per request:    201. 726 [ms] (mean)Time per request:    2. 017 [ms] (mean, across all concurrent requests)Transfer rate:     137. 00 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 9   0    7Processing:   0 201 600. 0   0  2014Waiting:    0 200 600. 0   0  2013Total:     0 201 600. 0   0  2017Percentage of the requests served within a certain time (ms) 50%   0 66%   1 75%   1 80%   3 90%  2000 95%  2001 98%  2001 99%  2002 100%  2017 (longest request)As you can see the request handler sleeps for 2 seconds for every 10th request hence if we set a realistic thread pool number of 8 for example it will limit us to a maximum of (8 x 10) / 2 = 40 requests per second and hence we set a thread pool of 100 here to match the maximum concurrent requests, setting a value higher would not make any difference. I guess you can already see the problem here. The thread pool itself becomes the bottleneck. In a real use case, you may not be able to set so many threads as the OS may not be able to provide so many thus creating increased resource usage and bottleneck. In this simple use case, since each thread spawns and processes the request really fast we won’t encounter an issue. So let’s see if we can have another solution without such a bottleneck. Asynchronous concurrent webserver: This example is closer to the example from the Rust async docs, I have omitted import statements for brevity. You can find the full example on GitHub here. The TcpListener, TcpStream, and task are from the async_std crate and async-std is the only external dependency used in this case. 1234567891011121314151617181920212223242526272829303132333435363738#[async_std::main]async fn main() {  let listener = TcpListener::bind( 127. 0. 0. 1:8080 ). await. unwrap(); // bind listener  let mut count = 0; // count used to introduce delays  loop {    count = count + 1;    // Listen for an incoming connection.     let (stream, _) = listener. accept(). await. unwrap();    // spawn a new task to handle the connection    task::spawn(handle_connection(stream, count));  }}async fn handle_connection(mut stream: TcpStream, count: i64) {  // Read the first 1024 bytes of data from the stream  let mut buffer = [0; 1024];  stream. read(&amp;mut buffer). await. unwrap();  // add 2 second delay to every 10th request  if (count % 10) == 0 {    println!( Adding delay. Count: {} , count);    task::sleep(Duration::from_secs(2)). await;  }  let header =  HTTP/1. 0 200 OKConnection: keep-aliveContent-Length: 174Content-Type: text/html; charset=utf-8   ;  let contents = fs::read_to_string( hello. html ). unwrap();  let response = format!( {}\r\n\r\n{} , header, contents);  stream. write(response. as_bytes()). await. unwrap(); // write response  stream. flush(). await. unwrap();}As you can see we bind an asynchronous TCP listener to port 8080 and listen to all incoming requests. Each request is processed in a new task provided by async_std. We are not using any thread pools here and all the incoming requests are processed asynchronously and hence we don’t have a bottleneck for maximum connections. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 1234567891011121314151617181920212223242526272829303132333435363738ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 186 secondsComplete requests:   10000Failed requests:    0Total transferred:   2830000 bytesHTML transferred:    1760000 bytesRequests per second:  495. 38 [#/sec] (mean)Time per request:    201. 863 [ms] (mean)Time per request:    2. 019 [ms] (mean, across all concurrent requests)Transfer rate:     136. 91 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  1  0. 8   0    6Processing:   0 201 600. 0   0  2010Waiting:    0 201 600. 0   0  2010Total:     0 201 600. 0   1  2014WARNING: The median and mean for the initial connection time are not within a normal deviation    These results are probably not that reliable. Percentage of the requests served within a certain time (ms) 50%   1 66%   1 75%   2 80%   3 90%  2000 95%  2001 98%  2001 99%  2003 100%  2014 (longest request)We got almost identical results here. Hence this version seems much more efficient than the multi-threaded version for this particular use case. Similar solutions can be built using other crates like smol, hyper, tokio, and so on. You can find some of them in this repo. Let’s see if we can combine the two to create an asynchronous multi-threaded version. Asynchronous multi-threaded concurrent webserver: This example uses an async ThreadPool. I have omitted import statements for brevity. You can find the full example on GitHub here. The ThreadPool struct is from the futures crate and it’s the only external dependency used in this case. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546fn main() {  let listener = TcpListener::bind( 127. 0. 0. 1:8080 ). unwrap(); // bind listener  let mut pool_builder = ThreadPoolBuilder::new();  pool_builder. pool_size(100);  let pool = pool_builder. create(). expect( couldn't create threadpool );  let mut count = 0; // count used to introduce delays  // Listen for an incoming connection.   for stream in listener. incoming() {    let stream = stream. unwrap();    count = count + 1;    let count_n = Box::new(count);    // spawning each connection in a new thread asynchronously    pool. spawn_ok(async {      handle_connection(stream, count_n). await;    });  }}async fn handle_connection(mut stream: TcpStream, count: Box&lt;i64&gt;) {  // Read the first 1024 bytes of data from the stream  let mut buffer = [0; 1024];  stream. read(&amp;mut buffer). unwrap();  // add 2 second delay to every 10th request  if (*count % 10) == 0 {    println!( Adding delay. Count: {} , count);    thread::sleep(Duration::from_secs(2));  }  let header =    HTTP/1. 0 200 OK  Connection: keep-alive  Content-Length: 174  Content-Type: text/html; charset=utf-8     ;  let contents = fs::read_to_string( hello. html ). unwrap();  let response = format!( {}\r\n\r\n{} , header, contents);  stream. write(response. as_bytes()). unwrap(); // write response  stream. flush(). unwrap();}This is very similar to the first Threadpool example except for the async invocation. Unfortunately, we have the same bottleneck from the thread pool in this case as well hence we set a thread pool of 100 here to match the maximum concurrent requests. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 123456789101112131415161718192021222324252627282930313233343536ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 161 secondsComplete requests:   10000Failed requests:    0Total transferred:   3030000 bytesHTML transferred:    1760000 bytesRequests per second:  496. 00 [#/sec] (mean)Time per request:    201. 615 [ms] (mean)Time per request:    2. 016 [ms] (mean, across all concurrent requests)Transfer rate:     146. 76 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  0  0. 8   0    5Processing:   0 201 600. 0   0  2007Waiting:    0 200 600. 0   0  2007Total:     0 201 600. 0   0  2010Percentage of the requests served within a certain time (ms) 50%   0 66%   1 75%   2 80%   2 90%  2000 95%  2000 98%  2001 99%  2002 100%  2010 (longest request)It does seem slightly faster by some milliseconds compared to previous solutions. Asynchronous multi-threaded concurrent webserver with Tokio: This is another version of asynchronous multi-threaded webserver using Tokio and it was contributed by Remco Bloemen. I have omitted import statements for brevity. You can find the full example on GitHub here. 1234567891011121314151617181920212223242526272829303132333435363738#[tokio::main()] // Tokio uses a threadpool sized for number of cpus by defaultasync fn main() {  let listener = TcpListener::bind( 127. 0. 0. 1:8080 ). await. unwrap(); // bind listener  let mut count = 0; // count used to introduce delays  // Listen for an incoming connection.   loop {    count = count + 1;    let (socket, _) = listener. accept(). await. unwrap();    // spawning each connection in a new tokio thread asynchronously    tokio::spawn(async move { handle_connection(socket, Box::new(count)). await });  }}async fn handle_connection(mut stream: TcpStream, count: Box&lt;i64&gt;) {  // Read the first 1024 bytes of data from the stream  let mut buffer = [0; 1024];  stream. read(&amp;mut buffer). await. unwrap();  // add 2 second delay to every 10th request  if (*count % 10) == 0 {    println!( Adding delay. Count: {} , count);    sleep(Duration::from_secs(2)). await;  }  let header =    HTTP/1. 0 200 OK  Connection: keep-alive  Content-Length: 174  Content-Type: text/html; charset=utf-8     ;  let contents = read_to_string( hello. html ). await. unwrap();  let response = format!( {}\r\n\r\n{} , header, contents);  stream. write_all(response. as_bytes()). await. unwrap(); // write response}This is very similar to the previous example but works with less number of thread pools and uses async invocation. We do not have the bottleneck from the previous thread pool example in this case. Let us run a benchmark using ApacheBench. We will make 10000 requests with 100 concurrent requests. 123456789101112131415161718192021222324252627282930313233343536ab -c 100 -n 10000 http://127. 0. 0. 1:8080/This is ApacheBench, Version 2. 3 &lt;$Revision: 1879490 $&gt;. . . Document Path:     /Document Length:    176 bytesConcurrency Level:   100Time taken for tests:  20. 569 secondsComplete requests:   10000Failed requests:    0Total transferred:   3030000 bytesHTML transferred:    1760000 bytesRequests per second:  486. 17 [#/sec] (mean)Time per request:    205. 688 [ms] (mean)Time per request:    2. 057 [ms] (mean, across all concurrent requests)Transfer rate:     143. 86 [Kbytes/sec] receivedConnection Times (ms)       min mean[+/-sd] median  maxConnect:    0  1  2. 4   0   22Processing:   0 202 600. 3   1  2013Waiting:    0 202 600. 3   1  2012Total:     0 203 600. 3   2  2029Percentage of the requests served within a certain time (ms) 50%   2 66%   3 75%   5 80%   7 90%  2000 95%  2003 98%  2006 99%  2008 100%  2029 (longest request)It does seem slightly slower by some milliseconds compared to previous solution. Conclusion: As I explained in the first part of this serious, this simple benchmarking is not an accurate representation for all concurrency use cases. It’s a simple test for a very particular use case, a simple concurrent web server that just serves a file. The idea is to see the differences in solutions and to understand how concurrency works in Rust. And for this particular use case, asynchronous solutions do seem to be the best choice. So stay tuned for the next post where we will look at concurrency in Golang and build the same use case in Go. References:  klau. si hackmd. io doc. rust-lang. org rust-lang. github. ioIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Jacob Mejicanos on Unsplash "},{id:33,url:"https://deepu.tech/concurrency-in-modern-languages/",title:"Concurrency in modern programming languages: Introduction",body:"2020/11/27 - This is a multi-part series where I’ll be talking about concurrency in modern programming languages and will be building and benchmarking a concurrent web server, inspired by the example from the Rust book, in popular languages like Rust, Go, JavaScript (NodeJS), TypeScript (Deno), Kotlin and Java to compare concurrency and its performance between these languages/platforms. The chapters of this series are as below and I’ll try to publish them weekly.  Introduction Concurrent web server in Rust Concurrent web server in Golang Concurrent web server in JavaScript with NodeJS Concurrent web server in TypeScript with Deno Concurrent web server in Java with JVM Comparison and conclusion of benchmarksWhat is concurrency:  Concurrency is one of the most complex aspects of programming and depending on your language of choice the complexity can be anywhere from “that looks confusing” to “what black magic is this”. Concurrency is the ability where multiple tasks can be executed in overlapping time periods, in no specific order without affecting the final outcome. Concurrency is a very broad term and can be achieved by any of the below mechanisms.  To simplify this we can say that concurrency is the problem and multi-threading, parallelism &amp; asynchronous processing are the solutions for that problem. Parallelism:  Parallelism != Concurrency Parallelism is when multiple tasks or several parts of a unique task literally run at the same time, e. g. on a multi-core processor. True parallelism is not possible on single-core processors. Applications running in multi-core processors can achieve concurrency by parallelism as well. Multi-threading and parallelism can work together and when doing multi-threading in multi-core environments the operating system can provide parallelism depending on resource usage. Multi-threading: Multi-threading is when a program appears to be doing several things at the same time even when it’s running on a single-core machine. Within a single-core, this is done by context switching or interleaving (only one thread gets CPU time at any given moment). On multi-core processors, multi-threading can work similarly to parallelism at the discretion of the operating system and hence can be more efficient. Most modern multi-core processors offer two threads per core hence enabling both multi-threading and parallelism. This is how concurrency is commonly achieved in multi-threaded languages like Java. If you are interested in learning more about how resources are used in threads check out this blog series                               🚀 Demystifying memory management in modern programming languages:             08-Jan-2020                           #languages #programming #computerscience                                    Asynchronous processing: Asynchronous processing means that your program performs non-blocking operations and can provide some level of concurrency though not as efficient as parallelism or multi-threading for some use cases. This is how concurrency is mainly achieved in single-threaded languages like JavaScript. Asynchronous programming in a multi-core, multi-threaded environment is a way to achieve parallelism. The asynchronous process can be spawned in separate threads in multi-threaded applications.  Though this might look similar to concurrency and parallelism the main difference is that concurrency and parallelism are ways tasks are executed and synchronous and asynchronous are programming models. When is concurrency needed: Concurrency has its pros and cons and hence is better to be implemented when there is a use case for it. The advantages of concurrency are faster processing times, better performance &amp; throughput, and better load handling. Now knowing when to use concurrency is very important. Some of the common use cases are as below and can serve as a baseline:  Web servers &amp; web services: These should be able to handle multiple requests at the same time and is a prime candidate for concurrency and asynchronous processing. Every popular web frameworks in every language will provide a concurrent implementation for this.  Large computation problems: CPU intensive algorithms, data processing, data science, aggregations and Map-reduce like workload can benefit from concurrency I/O intensive problems: When there is a lot of I/O(disk, DB, network, terminal, etc) involved, depending on the specific use case concurrency can help to speed it up. For example, writing a file can be offloaded to a thread or asynchronous process while the main program can read the next file or do some other calculation.  Distributed use cases: Message queues, pub-sub kind of use cases, running tests are best suited for concurrency and asynchronous processingWhen concurrency should be avoided: The disadvantages of concurrency is the complexity introduced in code and more computing resources being used. hence it’s also important to know when not to use it. Some cases where it should be avoided are:  Speed up small computation problems: The complexity introduced in splitting up a small function to be concurrent is not worth the increase in performance if any, which in most cases will be negligible for small computations and the cost of initializing resources for synchronization will end up being costlier in many cases.  When you don’t understand how concurrency and other related principles work: There is no shame in this. Concurrency is one of the most complex aspects of programming and depending on your language of choice the complexity can be anywhere from “that looks confusing” to “what black magic is this”. So trying to split a problem using concurrency without understanding can do more harm than good. Some of the common problems you might face are race conditions, memory leaks, data corruption, locks, and so on. Benchmarking &amp; comparison: We will be building a very simple web server using out of the box language features as much as possible. The web server will support concurrent requests. If the language supports both asynchronous and multi-threaded concurrency we will try both and a combination of both and pick the best performer. The complexity of the application will hence depend on language features and language complexity. We will try to use whatever the language provides to make concurrency performance as good as possible without over-complicating stuff. The web server will just serve one endpoint and it will add a sleep of two seconds on every tenth request. This will simulate a more realistic load, IMO. We will use promises, thread pools, and workers if required, and if the language supports it. We won’t use any unnecessary I/O in the application. Disclaimer: I’m not claiming this to be an accurate scientific method or the best benchmark for concurrency. I’m pretty sure different use cases will have different results and real-world web servers will have more complexity that requires communication between concurrent processes affecting performance. I’m just trying to provide some simple base comparisons for a simple use case. Also, my knowledge of some languages is better than others so I might miss some optimizations here and there. So please don’t shout at me. If you think the code for a particular language can be improved out of the box to improve concurrency performance let me know. If you think this benchmark is useless, well, please suggest a better one :) Benchmarking conditions: These will be some of the conditions I’ll be using for benchmark  The latest versions of language/runtimes available are used.  We will be using external dependencies only if that is the standard recommended way in the language.  We are not gonna look at improving concurrency performance using any configuration tweaks We will use ApacheBench for the benchmarks with a concurrency factor of 100 requests and 10000 total requests. The benchmark will be done 5 times for each language with and without warmup and the best result will be used.  All the benchmarks are run on the same machine running Fedora 32 on a quad-core Xeon processor with 8 threads and 32GB memory. Comparison parameters: I’ll be comparing the below aspects related to concurrency as well  Performance, based on benchmark results Perceived performance for complex use cases from the community Ease of use Simplicity, especially for complex use cases External libraries and ecosystem for concurrency LimitationsSo stay tuned for the remaining blog posts. References:  Stack Overflow question Stack Overflow question Stack Overflow questionIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Matt Bero on Unsplash "},{id:34,url:"https://deepu.tech/most-important-things-i-learned-in-a-decade/",title:"8 important things I learned as a software developer over a decade.",body:"2020/09/01 - I started my IT career, accidentally as I don’t have a Computer Science background and instead got an engineering degree in Electrical and Electronics, in March of 2010 and this year marks my 10th anniversary. Before I start giving out any advice let me try to justify my background. In a decade-long professional software development career, I have worn several hats like Jr. /Sr. /Principal Developer, Tech Lead, Project Manager, Business Analyst, Pre-sales Engineer, DevOps Engineer, Cloud Engineer, OSS contributor, and finally Developer Advocate. I have also had the pleasure of co-leading an amazing open-source community behind JHipster over the past 5 years. During this period I started out as a Java/JavaScript developer and went on to work with many other languages like Go, Rust, Python, Kotlin, C#, and so on and I now consider myself a polyglot developer. I have built web applications, enterprise apps, Microservices, Cloud Native apps, CLIs, System tools, Libraries, Android and Hybrid mobile apps, and so on. Hence I hope this gives me enough experience to talk about some of the important things I learned and practiced over the last decade. I hope this is beneficial especially to beginners and junior developers who are starting out their careers. 1. Do not get married to a technology:  When you marry a technology you start living with it in a bubble and you miss out on all those smart alternatives out there This, in my humble opinion, is the most important thing to learn for anyone starting their career. Do not get married to a programming Language, Framework, or Methodology. Getting attached to a programming language or a framework or methodology like OOP/FP will do more harm than good in the long run. Try to be pragmatic and use technology as it was intended, as a tool in your tool belt. Change is the only constant and it fits the IT industry better than others. Languages will die out, Frameworks and methodologies will come and go but the Industry will remain and anyone who is not capable of adapting to the changes may not fare well so be smart and let go of things that are dying out and learn what seems like the future. 2. Avoid being a fanboy/fangirl: Throughout your career, you will encounter technology that is amazing and you will fall in love with some of them. That is perfectly fine but avoid being a fanboy/fangirl and be ready to recognize something better might replace it in the future and when it does accept and move on. Trying to cling on to what you love blindly is not gonna do anyone any good. I still see fanboys of a dying language or framework making the effort to discredit the newer ones on the block, please don’t be like them its counterproductive to you and the industry. 3. Learn the basics of programming and become a polyglot developer:  A good programmer writes great code in a language. A great programmer is language independent. When learning a language learn its semantics rather than syntax. Learn programming basics. Learn about objects, collections, memory models, concurrency, and data structures. This knowledge can be applied to almost every language and the concepts around these don’t change that often. This will help you to become a true polyglot developer which has its own merits and this will help you follow the above two points as well and trust me it will make you a better developer. 4. Write for humans:  Any fool can write code that a computer can understand. Good programmers write code that humans can understand.  ‒ Martin Fowler Write simple and clean code and avoid showing off how smart you are in code. I know that it is tempting, especially when you are starting your career, to show off that fancy new language construct or functional magic in your code so that your colleagues can see how smart you are but trust me writing simple and easy to understand code will take you further than writing smart and complex code. Always choose the simplest solution for a problem. The best programmers are those who write the simplest code 5. Write docs, comments, and tests: Never discount the importance of writing good comments, documentation, and tests for your code. I know you might be tired of hearing this but I cannot stress the importance of this enough, not because I always do this but because it’s not easy to do always and I end up sacrificing this sometimes and every time I do that I have regretted that in the future. Again this is not easy but you don’t have to be 100% perfect you just need to try your best at least 6. There is no perfect code:  There is always a better way to do something and someone smarter to figure it out I used to spend a lot of time polishing my code and rewriting stuff to make it perfect, but over years I started realizing that there is nothing called perfect code, there is always a better way to do something and there is always someone smarter than you who will figure it out. I lost count of times when someone has suggested better solutions to code that I thought was perfect. Hence solve the problem at hand and avoid premature optimizations and over-engineering. One good thing about our industry is that the source code is ever-evolving and there is always tomorrow to improve it if needed. Don’t get me wrong, I’m not saying you should write sub-par code, when I say solve the problem at hand, I mean writing code that solves the issue along with all edge cases that could be thrown at it and code that is secure and with acceptable performance. I still polish my code when I feel something can be improved but I do not obsess about it or lose my sleep over it. I first get the problem solved and then if there is spare time and if an improvement helps to simplify the code or increase performance I do it. But mostly its when the polish makes code simpler. 7. Don’t fall for trends: We are all guilty of this, especially in our junior years. We see a hype or a trend and we get on that bandwagon without thinking through. For example, if you have React or Angular app, there is no justifiable reason IMO to switch to Vue. The same goes for many such choices which are purely based on trends, hype, or personal preferences. Don’t do something just because it’s hyped or is trendy, do what makes sense for your problem and remember something that works for a specific team need not work for another. Most companies build fancy frameworks to solve their own problem and doesn’t mean you have to blindly adapt it for no reason. 8. Do not reinvent the wheel without a good reason:  Not invented here (NIH) syndrome is common in IT companies and it does more harm than good Last but not the least make use of the collective wisdom from the industry, make use of OSS frameworks and libraries instead of building everything from scratch. There is an abundance of OSS libraries, frameworks, and tools out there for your language/platform of choice so make use of them for complex problems. Sometimes it’s better to slightly change your approach to fit a library than developing a huge library in-house just to fit your minor deviation in solution. Having said that there is also a time when you should avoid using a library, for example, it’s better to just write few lines of code as util rather than using a library for simple stuff like left pad or trim. If you want to use utils from a library go for a util library like Lodash which provides multiple utilities. ConclusionThere are many other things I learned over the years that have helped me but these are the important ones. Some are more common sense but its hard to recognize them especially in your initial years. The root idea is being pragmatic and using the right tools for the right job. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Martin Katler on Unsplash "},{id:35,url:"https://deepu.tech/the-problem-with-hiring-in-it/",title:"Let's face it, we have a broken technical interview process in our industry",body:"2020/08/12 - This topic has been discussed and debated for a long time and there have been countless articles written about it and even websites dedicated to it. But still, the problem remains. I wanted to write about these for a while but was procrastinating on it. Today I decided to finally pick it up after seeing the below tweet &quot;Whiteboard technical interviews appear to favor men over women. . . no women successfully solved the problem in the public setting, whereas all women solved it correctly in the private setting&quot;. Interesting findings from NCSU and Microsoft 🤔 https://t. co/CedqwKlquI &mdash; Antonia Forster (@AntoniaRForster) July 16, 2020Broken process: So I guess you got an idea of what I’m talking about. Yes, the technical interview process in the IT industry is broken. It is biased, it’s based on the old mentality of how a software developer has to be and based on unrealistic expectations. I guess we are all partly to blame as we didn’t push back against this enough. Below are the most common practices during a technical interview these days. Whiteboarding: Solving a problem on a whiteboard including writing pseudocode or actual code on a whiteboard or drawing process/technical diagrams. Most of the IT companies, including most of the big names out there follow this outdated process when evaluating someone’s technical credibility. Live coding: Coding on the spot while someone watches over you or via a remote session when someone monitors what you do. Most of the time, these assignments are asked to be done in an IDE or environment that you are not familiar with. Coding challenges: Solving coding challenges on a platform like HackerRank or Codility. This normally requires you to complete some short coding problems within a given time and most of these platforms can track your step-by-step input and mostly will have automated tests that may not be visible to you. These often require you to work in environments that are not familiar to you. Coding challenges are not great as it can also cause anxiety and doesn’t have a lot of scopes to ask questions or to understand problems at hand but at least they are not as bad as whiteboarding and live coding, as you still have the ability to do this at your convenience and use external resources to look up. Take home assignment: You are given a small application or a problem to be solved at your own convenience, often with a max time limit. You might have back and forth with the company and you might have to explain your solution in an interview. While take-home assignments are better than other options in this list, it still is time-intensive and if you are applying to multiple jobs that require this then you will spend a lot of time doing assignments. Why is Whiteboarding &amp; Live coding bad: While coding challenges and take-home assignments are not particularly great, they are not as bad and biased as whiteboarding and live coding. These require you to work in an unfamiliar environment without resources like an IDE or code editor. And most importantly you don’t have the best tools in a developers toolbelt, Google and Stack overflow :) How many of us can honestly say that we can perform our best without our favorite IDE, tools, and Google. Like it or not, this is the fact, so why should a technical interview be any different.  Give your future employees the same comfort and tools that your current employees have access to when interviewing them as well. The results could be amazing. Anxiety &amp; pressure: These methods are designed to cause anxiety and pressure. Most people will not be comfortable performing complex tasks in front of strangers who are evaluating your every move. Of course, there will be few who can ace these but what about the remaining? Being able to perform under pressure is not a quality of a great programmer. You are not working as a hacker for an intelligence agency (Remember the popular NSFW scene from the movie Swordfish?).  You are not Hugh Jackman from Swordfish or a hacker working for an intelligence agency Most of us never work under such pressure and performance anxiety in real life (I’m only talking about the case of someone watching over your shoulder and judging you) you might be under unrelated anxiety and pressure and those are the times we won’t be able to do our work properly and we might take sick days and stuff, so what is the point of expecting someone to perform these tasks under pressure for a job. An engineer is a problem solver not a fact memorizer. This is why I like coding assignments rather then whiteboarding.  https://t. co/Z3KU1CWSNj &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) August 31, 2019I generally do well in interviews and don’t have any issues solving coding challenges and stuff, once during a tech interview for a Developer Advocate position for a well-known company, the interviewer suddenly asked me to fix a factorial code snippet on the spot over zoom call without ever hinting beforehand that this will be part of the process. I have solved factorials a thousand times, hell; I use them as examples for recursion all the time. Despite that, I completely froze and my brain just shut down, I couldn’t even read the four lines of code and make any sense of it as my brain was just frozen and I was feeling so much anxiety and I started behaving like a 1st grader, and was blurting nonsense for the next few minutes. The anxiety was so unbearable that I asked them to end the interview. I would have never accepted the interview if I had known that this was part of it, as I don’t think putting myself through such pressure is worth the job. Imposter syndrome: Imposter syndrome is very common in the software industry and people who struggle with it are the worst hit from these outdated hiring practices. Having to do whiteboarding or live coding will only make the situation worse and if you happen to fail in those, then it will make things even worse. You will lose confidence and will start to believe that you are not good enough. Often times this goes hand in hand with the above points about anxiety. Bias: A recent research from North Carolina State University (NCSU) and Microsoft suggests that whiteboard interviews favor men over women psychologically and are ‘anti-women psychological stress examinations’. These interview processes test for stage fright rather than coding competency. Apparently, these processes resemble ‘Trier Social Stress Test’ a gold standard procedure used by psychologists to induce stress. So the process is not just biased toward women, but also biased towards people susceptible to social or performance anxiety, imposter syndrome, and so on. Conclusion: In the beginning, I said we are partly to blame. Why? Because in most companies, still doing these interview techniques, it’s not something mandated by the company management or some standards, it’s mostly people who are in charge of hiring (hiring managers, developers, team leads) who decide the process. So if you are in a position to influence hiring practice and didn’t try to change this, then you are part of the problem as well. So what can we do to change this?  Talk to hiring managers and people in charge of hiring in your company, explain to them issues of whiteboarding and live coding interviews, pressure them to change the process. If you are a team lead, make sure your hiring process doesn’t have these.  Turn down interviews that have whiteboarding or live coding in their process and explain to them why you turned it down. I have done this many times in the past. Of course, this may not be practical if you are desperate for a job. In that case, ask them if they are willing to do something like a coding challenge or take-home assignment instead and explain why you are asking for that. If they still insist, provide honest feedback on how it felt after the final hiring decision is made.  If you are in some position of power or influence in your company, please voice out against these practices A company that treats an interviewee well is a company that will treat its employees better. Resources http://they. whiteboarded. me/ https://www. theregister. com/AMP/2020/07/15/it_hiring_whiteboard/ https://news. ncsu. edu/2020/07/tech-job-interviews-anxiety/If you like this article, please leave a like. I would love to hear your thoughts and experiences in the comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Scott Graham on Unsplash "},{id:36,url:"https://deepu.tech/how-to-be-an-effective-polyglot-developer/",title:"How to become an effective Polyglot Developer",body:"2020/06/29 - A polyglot developer is someone who can work with multiple languages with ease. In its strict interpretation, the vast majority of developers are polyglot as they would have worked with at least two languages in their lifetime, not counting markup/config/SQL languages. So in the IT industry when someone says s/he is a polyglot developer they mean they can work with a handful of languages at least and they don’t have any particular strong preference for a single language. I started my career as a Java &amp; JavaScript developer but I never considered myself a polyglot developer until a few years ago. Why the shift you may ask! Even as a Java &amp; JavaScript developer, I had a strong affinity towards JavaScript as I used to enjoy building UI/UX. Later in my career, I started working a little bit with more languages like Groovy, Scala, TypeScript, and Python. Even then I didn’t consider myself a polyglot developer as I didn’t dive deep into any of those languages. I was mostly working on existing codebases where I had to do some work in that particular language and I always considered that a nuisance. Another reason was, I naively believed that since I already knew JS and Java quite well, and since they are among the most popular languages, I didn’t need to learn anything else or improve my skills further.  A programming language is a tool that has profound influence on our thinking habits.  ‒ E. Dijkstra Things changed when I started working full-time on a project written in Go and I started learning Go, which was extremely easy to learn, I realized how easy it was to learn a new language and enjoy working in it when I slightly changed my mindset and approach towards a programming language. Then I went on to learn, still learning, Rust, Kotlin, and so on. I now consider myself a polyglot developer and I can comfortably work in Java, Kotlin, Groovy, JS/TS, Go, and Rust. I can also get by in Python, Scala, Bash, Ruby, and C#. Now I love deep diving into languages and enjoy working with various languages simultaneously. In this post, I’m gonna share how my change in mindset helped and how you can also be an effective polyglot developer if you would like that. Advantages &amp; Disadvantages of being polyglot: Before we proceed lets quickly see the advantages and disadvantages of being polyglot Advantages:  You are not married to a single language and hence you have a bigger tool-belt with a more appropriate solution for the problem at hand Wider Job market and options to choose from. Hence you have multiple career path and opportunities Less fanboyism (Trust me it’s a good thing). You also become less biased. You also won’t get bored with one language &amp; its community You don’t have to worry about your favorite language dying out and you losing your job as you are not too invested in one ecosystem You become more pragmatic and opt for simplicity as you are not trying to use every fancy feature of a language. And in the end, you will become a better developer as you become better at concepts and semantics rather than syntax and you will get rid of programming habits more common to people working predominantly in a single language. Disadvantages:  You probably won’t be an expert in one particular language as you switch between languages Some jobs might require a long time of experience in one particular language You might have to refer to language docs (let’s be honest, Google it) more often than usual as you might not remember details of language APIs or certain syntax Developer fatigue. More things to learn and keep up to as it’s not something everyone would enjoy doing.  Mixing concepts from one language to anotherHow to approach a language: Most programming languages are quite similar at their core. Any mainstream Turing complete language will have some basic semantic features like objects, conditionals, loops, variables, functions, and operations. When you encounter a new language, the best way to learn it would be to learn semantics rather than syntax. This is where your pattern recognition skills are needed. When you start learning the semantics of a language you quickly realize how similar it is to another language you already know, you start to see patterns. Once you familiarize yourself and understand that, learning the syntax becomes easier and in many cases, syntax might change and you will end up looking up syntax anyway.  Programming is an art of seeing patterns in the world, the same pattern recognition can be applied to programming languages to transfer your knowledge from one language to another. So when learning a new language I follow the below approach  Learn the goal and purpose of the language, so that you know when to use this language, this helps with making you more pragmatic. You stop treating a language like a hammer and every problem as a nail.  Learn semantics. See what basic features the language offers and compare it with what you already know. For example, I start by looking at how variables work, how are conditionals designed, how loops and iterators work, and finally how functions work Learn about additional features that might be unique to the language. Sometimes they are a variation of a feature you are familiar with, like Goroutines &amp; coroutines or something unique like the Ownership model in Rust. Learn the similarities and differences of these features compared to languages you already know.  If the language uses a programming paradigm that you are not familiar with, learn that so that you can do justice to the language. No point using a pure functional language and trying to emulate OOP or imperative style with it. Even if the language is multi-paradigm, it is still beneficial to learn different paradigms and mix them based on the use case.  Learn about memory management and memory model of the language as it will help you have a deeper understanding. You will be surprised how similar the concepts are between languages. One thing not to do is learning a language by learning a particular framework, for example, learning Ruby on Rails rather than learning Ruby or Learning Spring rather than Java or learning React rather than JavaScript. This will do more harm than good as you will be learning the semantics of the framework more than the language. Being effective in a language: Now, learning a language and being effective in a language are two different things. One issue is that you might unconsciously try to emulate something from one language in another when the other language might have a better way to do it. For example, when I started with Go, I was trying to emulate JS callbacks in Go, instead of using Goroutines which can do a better job. One way to avoid this is to keep things as simple as possible.  Any fool can write code that a computer can understand. Good programmers write code that humans can understand.  ‒ Martin Fowler Some of the things you can try are:  Stick to simple constructs as much as possible. Write clean code. Use advanced language features only when required. What matters is you solve the problem at hand simply and understandably without sacrificing efficiency or causing bugs.  Don’t fall into the paradigm trap, for example, do not try to go fully functional on an OOP or imperative language. Use what gets the job done, remember, unlike what some people might say, there is no problem with doing OOP or imperative code. Most of the time a simple for loop is better than recursion as long as you are not doing nested loops and stuff like that.  Choose the appropriate language for the situation. Don’t pick a language for a problem because it’s cool, pick based on the use case, team composition, maintenance requirement, and business needs. Don’t impose a language on others in the team. Using the wrong tool for the job. #programming pic. twitter. com/5RdVqGuZoj &mdash; Rory Preddy☕ (@rorypreddy) June 24, 2019keeping up: One of the issues with being a polyglot developer is keeping up with multiple language trends and communities. You might end up with Developer fatigue. It is important to keep up but it doesn’t mean that you have to keep up with stuff like frameworks and libraries in a language.  Googling something doesn’t make you a bad developer, blindly copy-pasting from the first result does. Here are some tips that could help  Learn programming basics. Learn about objects, collections, memory models, concurrency, and data structures. This knowledge can be applied to almost every language and the concepts around these don’t change that often.  Use a good IDE/Editor/Plugins to take care of language syntax and style for you. A programmer shouldn’t be worrying about inserting semicolons and commas or adding the right amount of tabs or spaces. It’s 2020, these things should be the least of your concerns, let the IDEs or tools like prettier &amp; linters take care of those. For the last time, it friggin doesn’t matter if you use tab or space.  Before you start any project, choose the language you are going to use and spend few hours catching up on what is new in the language and what are reliable frameworks/libraries that you might need.  Docs, Google, and forums are your friend. There is no shame in googling something when you are in doubt or when you have a bad memory like me. Remember, we are problem solvers, not memorizers. Trust me everyone does it and the ones who say they don’t are lying. I still, almost always end up googling for array/string methods in almost all the languages I work with. The only thing to keep in mind is not to blindly copy the code from the first result. Read and compare at least two or three results from Google if you looking for a solution for a particular issue. If it’s a stack overflow answer, read the entire thread, don’t just read the answer. If it’s a blog, spend a few minutes and read it fully.  Contribute to some OSS projects if you have spare time or if your employer allows it during work hours. Doesn’t mean you have to take your work home, do it only if you think it’s beneficial to you in some way. It’s absolutely individual preferences. It is a great way to learn and keep up without having to actively track it. Conclusion:  A good programmer writes great code in a language. A great programmer is language-independent. As the saying goes don’t keep all your eggs in a single basket. There are considerable effort and some disadvantages to being a polyglot developer. But the advantages outweigh the disadvantages in my humble opinion. Knowing more than one programming language does more good than bad. IMO the effort is worth it. Most of you are already polyglot developers even if you don’t realize it, so just stop hating the other language you often use and instead embrace it and appreciate the good parts, keep in mind the bad parts and use the language when it is a good fit for a use case. The IT landscape is changing fast. New languages are being invented more rapidly languages are also dying out faster. Being a polyglot will be a very valuable skill in the future. So don’t fight it, come to the dark side. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Andrés Gómez on Unsplash "},{id:37,url:"https://deepu.tech/my-vr-gaming-pc-build/",title:"My PC build for VR gaming",body:"2020/06/12 - I built my first PC when I was in high school, in 2003. It was an Intel Pentium 4 system, not high specs or anything, just whatever I could afford at that time. I loved that PC like a child and it served me quite well for the next few years. I used it for Photoshop, 3DS max, some light gaming, and so on. Most importantly building that PC got me hooked to computers and allowed me to pursue that as a part-time business where I was building custom PCs for friends and others through word of mouth marketing. I was in college by then and this served as a channel of income for me. I probably built anywhere from 70 to 80 PCs in 4 years. I stopped building PCs when I started my IT career and moved onto using Windows Laptops and later to Linux Laptops. Flash forward to 2020 and the PC landscape has changed quite a lot. I recently purchased an Oculus Quest VR headset and got hooked to VR gaming so as a natural progression I wanted to play the best VR game out there, yes Half-Life: Alyx. But this meant I needed a Windows Gaming PC which I don’t have since my primary workstation is a Dell Precision running Fedora, you can read about that setup here, and my work laptop is a MacBook. So I decided to awake the PC nerd in me and build myself a gaming PC 😁 Configuration: Initially, I thought of building a budget machine for under 600€ since I was pretty sure I probably won’t use that machine for anything other than VR gaming but as usual, the PC nerd in me won and I ended up building a pretty good machine for around 1000€ So without further adieu, let’s jump into the configuration       Item   Model   Cost         CPU   AMD Ryzen 5 3600   €169       CPU Cooler   Arctic Freezer 34 eSports   €27       Motherboard   ASRock B450M Steel Legend   €89       RAM   Corsair CMK16 vengeance LPX 2x8GB   €68       GPU   Gigabyte Radeon RX 5700 XT GAMING OC 8GB   €449       SSD   Toshiba NVME M. 2 512GB   -       HDD   Seagate 500GB 2. 5” SATA   -       WiFi   TP-Link WLAN Bluetooth AC7265   €35       Case   Fractal Design Meshify C Mini Dark TG Micro ATX   €90       PSU   Cooler Master MWE Bronze 750 V2   €85       Fans   Cooler master Masterfab MF120L x 2 &amp; Sharkoon SHARK Blades RGB   €20       VR Headset   Oculus Quest 64GB   €453   Here are the parts when they arrived I already had an NVME SSD and a 2. 5” SATA drive so I decided to use those, its just 1TB combined for now, but I’m planning to add a 2 or 4 TB SATA HDD later based on storage needs. I already have a 34” widescreen monitor as well. I also bought a windows 10 pro license and some rechargeable batteries for the Oculus controllers. The entire setup cost me around 1500€ and that is not including software and game license costs Build: After almost a week of waiting for the components to arrive, I finally started the most exciting part of the process. CPU &amp; Motherboard: I always liked AMD processors when I was building PCs back in the days, but then AMD started loosing out to Intel and I was sad to see that. Now AMD is back in the game and is the best choice when it comes to CPUs. The Ryzen series is just amazing, I would have never dreamed of the 64 core Threadripper processors, let alone the 16 core Ryzen 9s. I decided to stick to the Budget-friendly Ryzen 5 series as they were powerful and provided the best value for money. Intel couldn’t even compete in that category IMO. I choose the 6 core Ryzen 5 3600 as it fit my needs perfectly. Originally I was planning to use the stock cooler, but after the build, I found the CPU temperature quite high on high load and the stock cooler fan was too loud so I changed it with an Arctic Freezer 34 tower cooler. I choose ASRock B450M Steel Legend as the motherboard as it was the best value for money option. It had excellent quality, a lot of ports, great reviews, and a great price. I considered getting an x570 chipset but then didn’t see enough value in spending 100€ more on that I choose to start with a dual-channel 16GB RAM from Corsair. I might add another 16GB later if needed. Since my workroom doesn’t have a LAN connection, I also bought a PCIe WiFi card from TP-Link which came with integrated Bluetooth 4. Graphics card: The GPU is undoubtedly the most important part of a gaming machine and hence I decided to settle for one of the best GPU from AMD. I did consider some Nvidia cards but again the AMD RX 5700 XT was more value for money and can handle demanding games and VR perfectly fine for the next few years. It didn’t make sense to pay 50% more for a 10% performance increase from Nvidia.  I specifically got the Gigabyte Radeon RX 5700 XT GAMING OC 8GB as it had the best reviews and great thermal efficiency. It is huge though compared to NVidia cards in the same category Case &amp; PSU: For the PC case, I initially choose a Cooler Master Q300L but then found out it had some cooling issues and hence bought a Fractal Design Meshify C Mini. It looks beautiful and I love the quality. It has good thermal efficiency, ample space, and great cable management. It also came with 2 fans.  To power the RX 5700XT and Ryzen CPU, I choose to go with a 750W power supply from Cooler Master. It’s 150W more than the recommended 600W but I guess this gives me some space for expansion later on. I also bought 3 additional fans so that I can have 3 intake fans and two exhaust fans creating a positive pressure inside the case, which is good for cooling and avoids dust build-up. I’m not too much into RGB but was curious so I bought one RGB fan for the front intake and I quite liked the result. VR Headset: So originally I bought the Oculus Quest 64GB, the 128GB is not worth the premium as the only advantage it offers is the memory bump which is not that important IMO, as I wanted a standalone headset without having to build a PC. I didn’t even look into the possibility of using it for PC VR. Then when I started considering PC VR, luckily Oculus also announced the Link support for Quest which lets you use a USB-C cable to connect it to a PC. It was great news and since then the Link support is getting better and better. IMO Quest is a great choice at this price point as you can use it for PC VR and also for standalone games on the go. Of course, it might not be having the display quality of the Valve Index but it has very decent quality.  I tried to play PC VR using the included USB-C 2. 0 cable and it was ok but I bought a 5-meter USB-C 3. 0 cable for better bandwidth and it works quite well. I tried using Virtual Desktop to play PC VR wirelessly on Quest but since I’m on full WiFi which is shared with so many devices in my house the latency was quite bad, maybe I’ll add a dedicated WiFi terminal for this in the future. A friend of mine was able to play Alyx on ultra settings through Virtual Desktop so its possible with proper network settings. Edit: Someone on Reddit pointed out that the Virtual Desktop issue could be due to an AMD driver issue, so I rolled back to the previous version of the driver and Virtual Desktop works better now, the latency is acceptable and I was able to play Alyx fine. Though the visual quality was still not as good as the Link cable, maybe if I’m on a wired LAN connection for the PC it could be better. Performance: Now that I have finished the build and installed a fresh Windows 10 OS, its time for some benchmarks. I used standard benchmarking tools like Cinebench, 3DMark, and so on. Here are the results. Cinebench R20 benchmark for CPU gave the best score of 3577 which is pretty good for the configuration and I’m happy with it I got a pretty good score for the configuration in 3DMark as well Steams VR performance test gave a high score as well The Superposition benchmark for GPU has the below scores for 4k and VR gaming which is pretty good as well VR Gameplay: Oculus Quest is perfectly fine for games like Beatsaber, Superhot, and Vader immortal and I prefer those in the standalone Quest version. I’m using the Quest tethered to the PC via a USB-3 cable for Half-Life: Alyx and oh boy its the best gaming experience I ever had and with this PC configuration I’m able to play both high and ultra display settings, though I mostly play with high settings as I get more frames and I don’t notice much difference in quality and also ultra settings cause some jitter once a while may be due to the USB cable length that reduces some bandwidth. Oculus link works great, though you might see a jitter or two rarely and that’s acceptable for me at this price point. Once I’m done with Alyx, I’m planning to try PC VR titles like Walking Dead: Saints &amp; Sinners, Asgard’s Wrath and Lone Echo I also played non-VR titles like Monster Hunter and Resident Evil 3 and even with the 34” 4k screen I’m getting pretty good frame rates and amazing graphics Conclusion: So far I’m super happy with the build and it serves the purpose very well. I might add an extra hard disk later on and might also use the machine for Photo editing and maybe a little bit of video editing. I’m hoping there will be more VR games similar to Half-Life: Alyx that will justify me building a PC for that :P If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. The Amazon links in the post are affiliate links and if you make a purchase using those I might get a referral fee. "},{id:38,url:"https://deepu.tech/avoiding-memory-leaks-in-nodejs/",title:"Avoiding Memory Leaks in NodeJS: Best Practices for Performance",body:"2020/05/06 - This is a post by me originally published in AppSignal blog as a guest author. This is just a excerpt from the same. Memory leaks are something every developer has to eventually face. They are common in most languages, even if the language automatically manages memory for you. Memory leaks can result in problems such as application slowdowns, crashes, high latency, and so on. In this blog post, we will look at what memory leaks are and how you can avoid them in your NodeJS application. Though this is more focused on NodeJS, it should generally apply to JavaScript and TypeScript as well. Avoiding memory leaks helps your application use resources efficiently and it also has performance benefits. Read full article If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: AppSignal "},{id:39,url:"https://deepu.tech/memory-management-in-rust/",title:"🚀 Visualizing memory management in Rust",body:"2020/03/31 - In this multi-part series, I aim to demystify the concepts behind memory management and take a deeper look at memory management in some of the modern programming languages. I hope the series would give you some insights into what is happening under the hood of these languages in terms of memory management. In this chapter, we will look at the memory management of the Rust programming language. Rust is a statically typed &amp; compiled systems programming language like C &amp; C++. Rust is memory &amp; thread-safe and does not have a runtime or a garbage collector. I previously also wrote about my first impressions of Rust. If you haven’t read the first part of this series, please read it first as I explained the difference between the Stack and Heap memory there which would be useful to understand this chapter.  This post is based on Rust 1. 41 official implementation and concept details might change in the future versions of Rust Compared to other languages we saw until now in this series, Rust is quite unique let us see how. Rust internal memory structureFirst, let us take a look at the internal memory structure of Rust. Rust doesn’t have a defined memory model in the language specifications as of now and the memory structure is quite straightforward. Each Rust program process is allocated some virtual memory by the Operating System(OS), this is the total memory that the process has access to.  This is quite simple compared to the memory structure we saw in the previous chapters for JVM, V8 and Go. As you can see there is no generational memory or any complex substructures since Garbage Collection(GC) is not involved. The reason for this is that Rust manages memory as part of program execution during runtime using the Ownership model rather than using any kind of GC. Let us see what the different memory are: Heap: This is where all dynamic data(any data for which size cannot be calculated at compile time) is stored. This is the biggest block of memory and the part managed by Rust’s Ownership model.  Box: The Box type is an abstraction for a heap-allocated value in Rust. Heap memory is allocated when Box::new is called. A Box&lt;T&gt; holds the smart-pointer to the heap memory allocated for type T and the reference is saved on the Stack. Stack: This is the Stack memory area and there is one Stack per thread. This is where static values are allocated by default. Static data(data size known at compile time) includes function frames, primitive values, Structs and pointers to dynamic data in Heap. Rust memory usage (Stack vs Heap)Now that we are clear about how memory is organized let’s see how Rust uses Stack and Heap when a program is executed. Let’s use the below Rust program, the code is not optimized for correctness hence ignore issues like unnecessary intermediatory variables and such, the focus is to visualize Stack and Heap memory usage. 123456789101112131415161718192021222324252627282930313233343536struct Employee&lt;'a&gt; {  // The 'a defines the lifetime of the struct. Here it means the reference of `name` field must outlive the `Employee`  name: &amp;'a str,  salary: i32,  sales: i32,  bonus: i32,}const BONUS_PERCENTAGE: i32 = 10;// salary is borrowedfn get_bonus_percentage(salary: &amp;i32) -&gt; i32 {  let percentage = (salary * BONUS_PERCENTAGE) / 100;  return percentage;}// salary is borrowed while no_of_sales is copiedfn find_employee_bonus(salary: &amp;i32, no_of_sales: i32) -&gt; i32 {  let bonus_percentage = get_bonus_percentage(salary);  let bonus = bonus_percentage * no_of_sales;  return bonus;}fn main() {  // variable is declared as mutable  let mut john = Employee {    name: &amp;format!( {} ,  John ), // explicitly making the value dynamic    salary: 5000,    sales: 5,    bonus: 0,  };  // salary is borrowed while sales is copied since i32 is a primitive  john. bonus = find_employee_bonus(&amp;john. salary, john. sales);  println!( Bonus for {} is {} , john. name, john. bonus);}All values in Rust are allocated on the Stack by default. There are two exceptions to this:  When the size of the value is unknown, i. e Structs like String and Vectors which grows in size over time or any other dynamic value When you manually create a Box&lt;T&gt; value like Box::new( Hello ). A box is a smart pointer to a heap-allocated value of type T. When a box goes out of scope, its destructor is called, the inner object is destroyed, and the memory on the Heap is freed. In both exception cases, the value will be allocated on Heap and its pointer will live on the Stack. Let us visualize this. Click on the slides and move forward/backward using arrow keys to see how the above program is executed and how the Stack and Heap memory is used:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck. As you can see:  Main function is kept in a “main frame” on the Stack Every function call is added to the Stack memory as a frame-block All static variables including arguments and the return value is saved within the function frame-block on the Stack All static values regardless of type are stored directly on the Stack. This applies to global scope as well All dynamic types created on the Heap and is referenced from the Stack using smart pointers. This applies to the global scope as well. Here we explicitly made the name dynamic to avoid it going to the Stack as having a fixed-length string value will do that The struct with static data is kept on the Stack and any dynamic value in it is kept on the Heap and is referenced via pointers Functions called from the current function is pushed on top of the Stack When a function returns its frame is removed from the Stack Unlike Garbage collected languages, once the main process is complete, the objects on the Heap are destroyed as well, we will see more about this in the following sectionsThe Stack as you can see is automatically managed and is done so by the operating system rather than Rust itself. Hence we do not have to worry much about the Stack. The Heap, on the other hand, is not automatically managed by the OS and since its the biggest memory space and holds dynamic data, it could grow exponentially causing our program to run out of memory over time. It also becomes fragmented over time slowing down applications. This is where Rust’s ownership model steps in to automatically manage the Heap memory Note: you can find the code I used to identify where a value ends up here Rust Memory management: OwnershipRust has one of the most unique ways of managing Heap memory and that is what makes Rust special. It uses a concept called ownership to manage memory. It is defined by a set of rules  Every value in Rust must have a variable as its owner There must be only one owner for a variable at any given time When the owner goes out of scope the value will be dropped freeing the memoryThe rules are applicable regardless of the value being in Stack or Heap memory. For example, in the below example the value of foo is dropped as soon as the method execution completes and the value of bar is dropped right after the block execution. 12345678910111213fn main() {  let foo =  value ; // owner is foo and is valid within this method  // bar is not valid here as its not declared yet  {    let bar =  bar value ; // owner is bar and is valid within this block scope    println!( value of bar is {} , bar); // bar is valid here    println!( value of foo is {} , foo); // foo is valid here  }  println!( value of foo is {} , foo); // foo is valid here  println!( value of bar is {} , bar); // bar is not valid here as its out of scope}These rules are checked by the compiler at compile-time and the freeing of memory happens at runtime along with program execution and hence there is no additional overhead or pause times here. So by scoping variables carefully, we can make sure the memory usage is optimized and that is also why Rust lets you use block scope almost everywhere. This might sound simple but in practice, this concept has deep implications in how you write Rust programs and it takes some getting used to. The Rust compiler does a great job of helping you along the way as well. Due to the strict ownership rules, Rust lets you change the ownership from one variable to another and is called a move. This is automatically done when passing a variable into a function or when creating a new assignment. For static primitives, a copy is used instead of move. There are a few more concepts related to memory management that play along with Ownership to make it effective RAII: RAII stands for Resource acquisition is initialization. This is not new in Rust, this is borrowed from C++. Rust enforces RAII so that when a value is initialized the variable owns the resources associated and its destructor is called when the variable goes out of scope freeing the resources. This ensures that we will never have to manually free memory or worry about memory leaks. Here is an example 1234567891011121314151617181920212223fn create_box(i: u32) {  // Allocate a string on the heap  let _var_i = Box::new(format!( Hello {} , i));  // `_var_i` is destroyed here, and memory gets freed}fn main() {  // Allocate an integer on the heap  let _var_1 = Box::new(5u32);  // A nested scope:  {    // Allocate a string on the heap    let _var_2 = Box::new( Hello 2 );    // `_var_2` is destroyed here, and memory gets freed  }  // Creating lots of boxes  // There's no need to manually free memory!  for i in 0u32. . 1_000 {    create_box(i);  }  // `_var_1` is destroyed here, and memory gets freed}Borrowing &amp; Borrow checker: In Rust we can pass a variable by either value or by reference and passing a variable by reference is called borrowing. Since we can have only one owner for a resource at a time, we have to borrow a resource to use it without taking ownership of it. Rust compiler has a borrow checker that statically ensures that references point to valid objects and ownership rules are not violated. Here is a simplified version of the Rust official example. 123456789101112131415161718192021222324252627282930313233343536// This function takes ownership of the passed valuefn take_ownership(value: Box&lt;i32&gt;) {  println!( Destroying box that contains {} , value);}// This function borrows the value by referencefn borrow(reference: &amp;i32) {  println!( This is: {} , reference);}fn main() {  // Create a boxed and a stacked variable  let boxed = Box::new(5_i32);  let stacked = 6_i32;  // Borrow the contents of the box. Ownership is not taken,  // so the contents can be borrowed again.   borrow(&amp;boxed);  borrow(&amp;stacked);  {    // Take a reference to the data contained inside the box    let _ref_to_boxed: &amp;i32 = &amp;boxed;    // Error!    // Can't destroy `boxed` while the inner value is borrowed later in scope.     take_ownership(boxed);    // Attempt to borrow `_ref_to_boxed` after inner value is destroyed    borrow(_ref_to_boxed);    // `_ref_to_boxed` goes out of scope and is no longer borrowed.   }  // `boxed` can now give up ownership to `take_ownership` method and be destroyed  take_ownership(boxed);}Variable Lifetimes: The lifetime of variables is another very important concept to make the ownership model work. It is a construct used by the borrow checker in order to ensure that all references to an object is valid. This is checked during compile time. The lifetime of a variable begins when its initialized and ends when it is destroyed. Lifetime is not the same as the scope. This might sound straight forward but lifetimes get much more complex once functions and structs with references come into play and once they do then we would need to start using lifetime annotations to let the borrow checker know how long references are valid. Sometimes the compiler can infer lifetimes, but not always. I’m not going to details here as its not in the scope of this article Smart pointers: Pointers are nothing but a reference to a memory address on the Heap. Rust has support for pointers and lets us reference and dereference them using &amp; and * operators. Smart pointers in Rust are like pointers but with additional metadata capabilities. Like RAII this is another concept taken from C++. Unlike pointers which are references that only borrow data, smart pointers own the data they point to. Box, String and Vec are examples of smart pointers in Rust. You can also write your own smart pointers using structs. Ownership visualizationNow that we have seen different concepts used for Ownership, let us visualize it, unlike other languages where we visualized the data in Heap, here it is much easier if we look at the code itself.  ConclusionThis post should give you an overview of the Rust memory structure and memory management. This is not exhaustive, there are a lot more advanced concepts and the implementation details keep changing from version to version. Unlike Garbage collected languages, where you need not understand the memory management model to use the language, in Rust, it is required to understand how Ownership works in order to write programs. This post is just a starting step, I recommend that you dive into the Rust documentation to learn more about these concepts. I hope you had fun learning this, stay tuned for the next post in the series. References doc. rust-lang. org www. youtube. com pcwalton. github. io zee-nix. blogspot. com hacks. mozilla. org os. phil-opp. com/heap-allocation msrc-blog. microsoft. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image inspired by https://hacks. mozilla. org/2019/01/fearless-security-memory-safety/ "},{id:40,url:"https://deepu.tech/deno-runtime-for-typescript/",title:"Forget NodeJS! Build native TypeScript applications with Deno 🦖",body:"2020/02/18 - Have you heard of Deno? If not you should check it out. Deno is a modern JavaScript/TypeScript runtime &amp; scripting environment. Deno is what NodeJS should have been according to Ryan Dahl who created NodeJS. Deno was also created by Ryan Dahl in 2018 and is built with V8, Rust and Tokio with a focus on security, performance, and ease of use. Deno takes many inspirations from Go and Rust. In this post let us see what Deno offers and how it compares to NodeJS. You can also watch the same in a talk format I did for Devoxx Ukraine below                         Let us install Deno before we proceed. Install DenoThere are multiple ways to install Deno. If you are on Mac or Linux, you can install it via Homebrew. On Windows, you can use Chocolatey. 12345# Mac/Linuxbrew install deno# windowschoco install denoCheck the official doc for other installation methods  Please note that Deno is still under active development and hence may not be ready for production use Now that we have Deno installed, let us look at its features. Features TypeScript supported out of the box without any transpiling setup Can execute remote scripts Secure by default. No file, network, or environment access by default unless explicitly enabled Provides curated standard modules Supports only ES modules. Modules are cached globally and are immutable Built-in tooling (format, lint, test, bundle and so on) Deno applications can be browser compatible Promise based API(async/await supported) and no callback hell Top-level await support Sub-process using web workers WebAssembly support Lightweight multi-platform executable(~10MB) Deno does not use NPM for dependency management and hence there is no node_modules hell to deal with, which IMO is a huge selling point TypeScript support: Deno has native support for TypeScript and JavaScript. You can write Deno applications directly in TypeScript and Deno can execute them without any transpiling step from your side. Let us try it 12345function hello(person: string) { return  Hello,   + person;}console. log(hello( John ));Save this to hello. ts file and execute deno hello. ts. You will see Deno compiles the file and executes it. Deno supports the latest version of TypeScript and keeps the support up to date. Remote script execution: With Deno, you can run a local or remote script quite easily. Just point to the file or HTTP URL of the script and Deno will download and execute it 1deno https://deno. land/std/examples/welcome. tsThis means you can just point to a raw GitHub URL to execute a script, no hassle of installing something. The default security model Deno is applied to remote scripts as well. Secure by default: By default, a script run with Deno cannot access the file system, network, sub-process, or environment. This creates a sandbox for the script and the user has to explicitly provide permissions. This puts control in the hands of the end-user.  Granular permissions Permissions can be revoked Permissions whitelist supportThe permissions can be provided via command-line flags during execution or programmatically when using sub-processes. The available flags are: 12345678--allow-all | -A--allow-env--allow-hrtime--allow-read=&lt;whitelist&gt;--allow-write=&lt;whitelist&gt;--allow-net=&lt;whitelist&gt;--allow-plugin--allow-run Please note that flags must be passed before the filename like deno -A file. ts or deno run -A file. ts. Anything passed after the filename will be considered as program arguments. Let us see an example that creates a local HTTP server: 1234567console. info( Hello there! );import { serve } from  https://deno. land/std/http/server. ts ;const server = serve( :8000 );console. info( Server created! );The snippet tries to use the network and hence when you run the program with Deno it will fail with an error To avoid the error we need to pass the --allow-net or --allow-all flag when running the program. You can also grant access to specific ports and domains as well using a whitelist. For example deno --allow-net=:8000 security. ts Standard modules: Deno provides standard modules like NodeJS, Go or Rust. The list is growing as newer versions are released. Currently available modules are:  archive - TAR archive handling colors - ANSI colors on console datetime - Datetime parse utilities encoding - Encode/Decode CSV, YAML, HEX, Base32 &amp; TOML flags - CLI argument parser fs - Filesystem API http - HTTP server framework log - Logging framework media_types - Resolve media types prettier - Prettier formatting API strings - String utils testing - Testing utils uuid - UUID support ws - Websocket client/serverThe standard modules are available under https://deno. land/std namespace and are tagged in accordance with Deno releases. 1import { green } from  https://deno. land/std/fmt/colors. ts ;ES Modules: Deno supports only ES Modules using a remote or local URL. This keeps dependency management simple and light. Unlike NodeJS, Deno doesn’t try to be too smart here, which means:  require() is not supported, so no confusion with import syntax No “magical” module resolution Third-party modules are imported by URL(Local and remote) Remote code is fetched only once and cached globally for later use Remote code is considered immutable and never updated unless --reload flag is used Dynamic imports are supported Supports import maps Third-party modules are available in https://deno. land/x/ NPM modules can be used, if required, as simple local file URL or from jspm. io or pika. devHence we can any import any library that is available from a URL. Let’s build on our HTTP server example 12345678910111213141516import { serve } from  https://deno. land/std/http/server. ts ;import { green } from  https://raw. githubusercontent. com/denoland/deno/master/std/fmt/colors. ts ;import capitalize from  https://unpkg. com/lodash-es@4. 17. 15/capitalize. js ;const server = serve( :8000 );console. info(green(capitalize( server created! )));const body = new TextEncoder(). encode( Hello there\n );(async () =&gt; { console. log(green( Listening on http://localhost:8000/ )); for await (const req of server) {  req. respond({ body }); }})();The import paths can be made nicer by using an import map below 1234567{  imports : {   http/ :  https://deno. land/std/http/ ,   fmt/ :  https://raw. githubusercontent. com/denoland/deno/master/std/fmt/ ,   lodash/ :  https://unpkg. com/lodash-es@4. 17. 15/  }}Now we can simplify the paths as below 12345678910111213141516import { serve } from  http/server. ts ;import { green } from  fmt/colors. ts ;import capitalize from  lodash/capitalize. js ;const server = serve( :8000 );console. info(green(capitalize( server created! )));const body = new TextEncoder(). encode( Hello there\n );(async () =&gt; { console. log(green( Listening on http://localhost:8000/ )); for await (const req of server) {  req. respond({ body }); }})();Run this with the --importmap flag deno --allow-net=:8000 --importmap import-map. json server. ts. Please note that the flags should be before the filename. Now you can access http://localhost:8000 to verify this. Built-in tooling: Deno takes inspiration from Rust and Golang to provide built-in tooling, this IMO is great as it helps you get started without having to worry about setting up testing, linting and bundling frameworks. The below are tools currently available/planned  Dependency inspector (deno info): Provides information about cache and source files Bundler (deno bundle): Bundle module and dependencies into a single JavaScript file Installer (deno install): Install a Deno module globally, the equivalent of npm install Test runner (deno test): Run tests using the Deno built-in test framework Type info (deno types): Get the Deno TypeScript API reference Code formatter (deno fmt): Format source code using Prettier Linter (planned) (deno lint): Linting support for source code Debugger (planned) (--debug): Debug support for Chrome Dev toolsFor example, with Deno, you can write test cases easily using provided utilities Let’s say we have factorial. ts 123export function factorial(n: number): number { return n == 0 ? 1 : n * factorial(n - 1);}We can write a test for this as below 1234567891011import { test } from  https://deno. land/std/testing/mod. ts ;import { assertEquals } from  https://deno. land/std/testing/asserts. ts ;import { factorial } from  . /factorial. ts ;test(function testFactorial(): void { assertEquals(factorial(5), 120);});test(function t2(): void { assertEquals( world ,  worlds );});Browser compatibility: Deno programs or modules can be run on a browser as well if they satisfy the below conditions  The program must are written completely in JavaScript and should not use the global Deno APIs If the program is written in Typescript, it must be bundled as JavaScript using deno bundle and should not use the global Deno APIsFor browser compatibility Deno also supports window. load and window. unload events. load and unload events can be used with window. addEventListener as well. Let us see below sample, this can be run using deno run or we can package it and execute in a browser 123456789import capitalize from  https://unpkg. com/lodash-es@4. 17. 15/capitalize. js ;export function main() { console. log(capitalize( hello from the web browser ));}window. onload = () =&gt; { console. info(capitalize( module loaded! ));};We can package this using deno bundle example. ts browser_compatibility. js and use the browser_compatibility. js in an HTML file and load it in a browser. Try it out and look at the browser console. Promise API: Another great thing about Deno is that all of its API is Promise based which means, unlike NodeJS we do not have to deal with callback hells. Also, the API is quite consistent across standard modules. Let us see an example: 1234567const filePromise: Promise&lt;Deno. File&gt; = Deno. open( dummyFile. txt );filePromise. then((file: Deno. File) =&gt; { Deno. copy(Deno. stdout, file). then(() =&gt; {  file. close(); });});But we said no callbacks right, the good thing about Promise API is that we can use async/await syntax, so with that, we can rewrite above 123456const filePromise: Promise&lt;Deno. File&gt; = Deno. open( dummyFile. txt );filePromise. then(async (file: Deno. File) =&gt; { await Deno. copy(Deno. stdout, file); file. close();});Run deno -A example. ts to see it in action, don’t forget to create the dummyFile. txt with some content Top-level await: The above code still uses a callback, what if we can use await for that as well, luckily Deno has support for the top-level await proposal(Not supported by TypeScript yet). With this, we can rewrite the above 1234567const fileName = Deno. args[0];const file: Deno. File = await Deno. open(fileName);await Deno. copy(Deno. stdout, file);file. close();Isn’t that neat? Run it as deno -A example. ts dummyFile. txt Subprocess using web workers: Since Deno uses the V8 engine which is single-threaded, we have to use a sub-process like in NodeJS to spawn new threads(V8 instance). This is done using service workers in Deno. Here is an example, we are importing the code we used in the top-level await example in the subprocess here. 123456789101112131415161718const p = Deno. run({ args: [ deno ,  run ,  --allow-read ,  top_level_await. ts ,  dummyFile. txt ], stdout:  piped , stderr:  piped ,});const { code } = await p. status();if (code === 0) { const rawOutput = await p. output(); await Deno. stdout. write(rawOutput);} else { const rawError = await p. stderrOutput(); const errorString = new TextDecoder(). decode(rawError); console. log(errorString);}Deno. exit(code);You can run any CMD/Unix command as a subprocess like in NodeJS WebAssembly support: WebAssembly is one of the most innovative features to have landed on the JavaScript world. It lets us use programs written in any compatible language to be executed in a JS Engine. Deno has native support for WebAssembly. Let us see an example. First, we need a WebAssembly(WASM) binary. Since we are focusing on Deno here, let’s use a simple C program. You can also use Rust, Go or any other supported language. In the end, you just need to provide a compiled . wasm binary file. 123int factorial(int n) {\treturn n == 0 ? 1 : n * factorial(n - 1);}We can convert this to WASM binary using the online converter here and import it in our TypeScript program below 123456const mod = new WebAssembly. Module(await Deno. readFile( fact_c. wasm ));const { exports: { factorial },} = new WebAssembly. Instance(mod);console. log(factorial(10));Run deno -A example. ts and see the output from the C program. A Deno application in actionNow that we have an overview of Deno features, let’s build a Deno CLI app  Run deno --help and deno run --help to see all options that can be passed when you run a program. You can learn more about Deno features and API in the Deno website and manual Let’s build a simple proxy server that can be installed as a CLI tool. This is a really simple proxy, but you can add more features to make it smarter if you like 1234567891011121314151617181920212223console. info( Proxy server starting! );import { serve } from  https://deno. land/std/http/server. ts ;import { green, yellow } from  https://deno. land/std/fmt/colors. ts ;const server = serve( :8000 );const url = Deno. args[0] ||  https://deepu. tech ;console. info(green( proxy server created! ));(async () =&gt; { console. log(green(`Proxy listening on http://localhost:8000/ for ${url}`)); for await (const req of server) {  let reqUrl = req. url. startsWith( http ) ? req. url : `${url}${req. url}`;  console. log(yellow(`URL requested: ${reqUrl}`));  const res = await fetch(reqUrl);  req. respond(res); }})();Run deno --allow-net deno_app. ts https://google. com and visit http://localhost:8000/. You can now see all the traffic on your console. You can use any URL you like instead of Google. Lets package and install the app. 1deno install --allow-net my-proxy deno_app. tsIf you want to override the file use deno install -f --allow-net my-proxy deno_app. ts. You can also publish the script to an HTTP URL and install it from there. Now just run my-proxy https://google. com and viola we have our own proxy app. Isn’t that simple and neat. ConclusionLet us see how Deno compares against NodeJS and why I believe it has great potential Why is Deno better than NodeJS: I consider Deno to be better than NodeJS for the following reasons. The creator of NodeJS thinks the same I guess  Easy to install - Single lightweight binary, built-in dependency management Secure by default - Sandboxed, Fine-grained privileges and user-controlled Simple ES module resolution - No smart(confusing) module system like NodeJS Decentralized and globally cached third-party modules - No node_modules hell, efficient No dependency on package managers or package registries(No NPM, No Yarn, No node_modules) Native TypeScript support Follows web standards and modern language features Browser compatibility - Ability to reuse modules in browser and Deno apps Remote script runner - Neat installation of scripts and tools Built-in tooling - No hassle of setting up tooling, bundlers and so onWhy does it matter: Why does it matter, why do we need another scripting environment? Isn’t JavaScript ecosystem already bloated enough  NodeJS ecosystem has become too heavy and bloated and we need something to break the monopoly and force constructive improvements Dynamic languages are still important especially in the below domains     Data science   Scripting   Tooling   CLI    Many Python/NodeJS/Bash use cases can be replaced with TypeScript using Deno     TypeScript provides better developer experience   Consistent and documentable API   Easier to build and distribute   Does not download the internet all the time   More secure   Challenges: This is not without challenges, for Deno to succeed it still has to overcome these issues  Fragmentation of libraries and modules Not compatible with many of the NPM modules already out there Library authors would have to publish a Deno compatible build(Not difficult but en extra step) Migrating existing NodeJS apps will not be easy due to incompatible API Bundles are not optimized so might need tooling or improvements there Stability, since Deno is quite new (NodeJS is battle-tested) Not production-readyIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Random image from the internet "},{id:41,url:"https://deepu.tech/memory-management-in-golang/",title:"🚀 Visualizing memory management in Golang",body:"2020/02/17 - In this multi-part series, I aim to demystify the concepts behind memory management and take a deeper look at memory management in some of the modern programming languages. I hope the series would give you some insights into what is happening under the hood of these languages in terms of memory management. In this chapter, we will look at the memory management of the Go programming language(Golang). Go is a statically typed &amp; compiled language like C/C++ and Rust. Hence Go does not need a VM and Go application binaries include a small runtime embedded in them to take care of language features like Garbage collection, scheduling &amp; concurrency. If you haven’t read the first part of this series, please read it first as I explained the difference between the Stack and Heap memory there which would be useful to understand this chapter.  This post is based on Go 1. 13 default official implementation and concept details might change in the future versions of Go Go internal memory structureFirst, let us see what the internal memory structure of Go is.  Go Runtime schedules Goroutines (G) onto Logical Processors (P) for execution. Each P has a machine (M). We will use P, M &amp; G throughout this post. If you’re not familiar with the Go scheduler read Go scheduler: Ms, Ps &amp; Gs first.  Each Go program process is allocated some virtual memory by the Operating System(OS), this is the total memory that the process has access to. The actual memory that is used within the virtual memory is called Resident Set. This space is managed by the internal memory constructs as below: This is a simplified view based on the internal objects used by Go, In reality, Go divides and groups memory into pages as described in this great article. This is quite different from the memory structure we saw in the previous chapters for JVM and V8. As you can see there is no generational memory here. The main reason for this is the TCMalloc(Thread-Caching Malloc), which is what Go’s own memory allocator was modeled upon. Let us see what the different constructs are: Page Heap(mheap): This is where Go stores dynamic data(any data for which size cannot be calculated at compile time). This is the biggest block of memory and this is where Garbage Collection(GC) takes place. The resident set is divided into pages of 8KB each and is managed by one global mheap object.  Large objects(Object of Size &gt; 32kb) are allocated directly from mheap. These large requests come at an expense of central lock, so only one P’s request can be served at any given point in time. mheap manages pages grouped into different constructs as below:    mspan: mspan is the most basic structure that manages the pages of memory in mheap. It’s a double-linked list that holds the address of the start page, span size class, and the number of pages in the span. Like TCMalloc, Go also divides Memory Pages into a block of 67 different classes by size starting at 8 bytes up to 32 kilobytes as in the below image     Each span exists twice, one for objects with pointers (scan classes) and one for objects with no pointers (noscan classes). This helps during GC as noscan spans need not be traversed to look for live objects.     mcentral: mcentral groups spans of same size class together. Each mcentral contains two mspanList:      empty: Double linked list of spans with no free objects or spans that are cached in a mcache. When a span here is freed, it’s moved to the nonempty list.    non-empty: Double linked list of spans with a free object. When a new span is requested from mcentral, it takes that from the nonempty list and moves it into the empty list.     When mcentral doesn’t have any free span, it requests a new run of pages from mheap.     arena: The heap memory grows and shrinks as required within the virtual memory allocated. When more memory is needed, mheap pulls them from the virtual memory as a chunk of 64MB(for 64-bit architectures) called arena. The pages are mapped to spans here.     mcache: This is a very interesting construct. mcache is a cache of memory provided to a P(Logical Processor) to store small objects(Object size &lt;=32Kb). Though this resembles the thread stack, it is part of the heap and is used for dynamic data. mcache contains scan and noscan types of mspan for all class sizes. Goroutines can obtain memory from mcache without any locks as a P can have only one G at a time. Hence this is more efficient. mcache requests new spans from mcentral when required.  Stack: This is the stack memory area and there is one stack per Goroutine(G). This is where static data including function frames, static structs, primitive values, and pointers to dynamic structs are stored. This is not the same as mcache which is assigned to a P Go memory usage (Stack vs Heap)Now that we are clear about how memory is organized let’s see how Go uses Stack and Heap when a program is executed. Let’s use the below Go program, the code is not optimized for correctness hence ignore issues like unnecessary intermediatory variables and such, the focus is to visualize stack and heap memory usage. 1234567891011121314151617181920212223242526272829package mainimport  fmt type Employee struct { name  string salary int sales int bonus int}const BONUS_PERCENTAGE = 10func getBonusPercentage(salary int) int { percentage := (salary * BONUS_PERCENTAGE) / 100 return percentage}func findEmployeeBonus(salary, noOfSales int) int { bonusPercentage := getBonusPercentage(salary) bonus := bonusPercentage * noOfSales return bonus}func main() { var john = Employee{ John , 5000, 5, 0} john. bonus = findEmployeeBonus(john. salary, john. sales) fmt. Println(john. bonus)}One major difference Go has compared to many garbage collected languages is that many objects are allocated directly on the program stack. The Go compiler uses a process called escape analysis to find objects whose lifetime is known at compile-time and allocates them on the stack rather than in garbage-collected heap memory. During compilation, Go does the escape analysis to determine what can go into Stack(static data) and what needs to go into Heap(dynamic data). We can see these details during compilation by running go build with -gcflags '-m' flag. For the above code, it will output something like the below: 123456789101112❯ go build -gcflags '-m' gc. go# command-line-argumentstemp/gc. go:14:6: can inline getBonusPercentagetemp/gc. go:19:6: can inline findEmployeeBonustemp/gc. go:20:39: inlining call to getBonusPercentagetemp/gc. go:27:32: inlining call to findEmployeeBonustemp/gc. go:27:32: inlining call to getBonusPercentagetemp/gc. go:28:13: inlining call to fmt. Printlntemp/gc. go:28:18: john. bonus escapes to heaptemp/gc. go:28:13: io. Writer(os. Stdout) escapes to heaptemp/gc. go:28:13: main []interface {} literal does not escape&lt;autogenerated&gt;:1: os. (*File). close . this does not escapeLet us visualize this. Click on the slides and move forward/backward using arrow keys to see how the above program is executed and how the stack and heap memory is used:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck. As you can see:  Main function is kept in a “main frame” on the Stack Every function call is added to the stack memory as a frame-block All static variables including arguments and the return value is saved within the function frame-block on the Stack All static values regardless of type are stored directly on the Stack. This applies to global scope as well All dynamic types are created on the Heap and is referenced from the Stack using Stack pointers. Objects of size less than 32Kb go to the mcache of the P. This applies to global scope as well The struct with static data is kept on the stack until any dynamic value is added at that point the struct is moved to the heap Functions called from the current function is pushed on top of the Stack When a function returns its frame is removed from the Stack Once the main process is complete, the objects on the Heap do not have any more pointers from Stack and becomes orphanThe Stack as you can see is automatically managed and is done so by the operating system rather than Go itself. Hence we do not have to worry much about the Stack. The Heap, on the other hand, is not automatically managed by the OS and since it’s the biggest memory space and holds dynamic data, it could grow exponentially causing our program to run out of memory over time. It also becomes fragmented over time slowing down applications. This is where garbage collection comes in. Go Memory managementGo’s memory management involves automatic allocation when memory is needed and garbage collection when memory is not needed anymore. It’s done by the standard library. Unlike C/C++ the developer does not have to deal with it and the underlying management done by Go is well optimized and efficient. Memory Allocation: Many programming languages that employ Garbage collection use a generational memory structure to make collection efficient along with compaction to reduce fragmentation. Go takes a different approach here, as we saw earlier, Go structures memory quite differently. Go employs a thread-local cache to speed up small object allocations and maintains scan/noscan spans to speed up GC. This structure along with the process avoids fragmentation to a great extent making compact unnecessary during GC. Let’s see how this allocation takes place. Go decides the allocation process of an object based on its size and is divided into three categories: Tiny(size &lt; 16B): Objects of size less than 16 bytes are allocated using the mcache’s tiny allocator. This is efficient and multiple tiny allocations are done on a single 16-byte block.  Small(size 16B ~ 32KB): Objects of size between 16 bytes and 32 Kilobytes are allocated on the corresponding size class(mspan) on mcache of the P where the G is running.  In both tiny and small allocation if the mspan’s list is empty the allocator will obtain a run of pages from the mheap to use for the mspan. If the mheap is empty or has no page runs large enough then it allocates a new group of pages (at least 1MB) from the OS. Large(size &gt; 32KB): Objects of size greater than 32 kilobytes are allocated directly on the corresponding size class of mheap. If the mheap is empty or has no page runs large enough then it allocates a new group of pages (at least 1MB) from the OS.  Note: You can find the above GIF images as slideshow here Garbage collection: Now that we know how Go allocates memory, let us see how it automatically collects the Heap memory which is very important for the performance of an application. When a program tries to allocate more memory on the Heap than that is freely available we encounter out of memory errors. An incorrectly managed heap could also cause a memory leak. Go manages the heap memory by garbage collection. In simple terms, it frees the memory used by orphan objects, i. e, objects that are no longer referenced from the Stack directly or indirectly(via a reference in another object) to make space for new object creation. As of version 1. 12, the Golang uses a non-generational concurrent tri-color mark and sweep collector. The collection process roughly looks like the below, I don’t want to go into details as it changes from version to version. However, if you are interested in those, then I recommend this great series. The process starts when a certain percentage(GC Percentage) of heap allocations are done and the collector does different phases of work:  Mark Setup (Stop the world): When GC starts, the collector turns on write barriers so that data integrity can be maintained during the next concurrent phase. This step needs a very small pause as every running Goroutine is paused to enable this and then continues.  Marking (Concurrent): Once write barriers are turned on the actual marking process is started in parallel to the application using 25% of the available CPU capacity. The corresponding Ps are reserved until marking is complete. This is done using dedicated Goroutines. Here the GC marks values in the heap that is alive(referenced from the Stack of any active Goroutines). When collection takes longer the process may employ active Goroutine from application to assist in the marking process. this is called Mark Assist.  Mark Termination (Stop the world): Once marking is done every active Goroutine is paused and write barriers are turned off and clean up tasks are started. The GC also calculates the next GC goal here. Once this is done the reserved Ps are released back to the application.  Sweeping (Concurrent): Once the collection is done and allocations are attempted, the sweeping process starts to reclaim memory from the heap that is not marked alive. The amount of memory swept is synchronous to the amount being allocated. Let us see these in action for a single Goroutine. The number of objects is kept small for brevity. Click on the slides and move forward/backward using arrow keys to see the process:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck.  We are looking at a single Goroutine, the actual process does this for all active Goroutines. The write barriers are turned on first.  The marking process picks a GC root and colors it black and traverses pointers from it in a depth-first tree-like manner, it marks each object encountered grey When it reaches an object in a noscan span or when an object has no more pointers it finishes for the root and picks up the next GC root object Once all GC roots are scanned, it picks up a grey object and continues to traverse its pointers in a similar fashion If there are any pointer changes to an object when write barriers are on, the object gets colored grey so that GC re-scans it When there are no more grey objects left, the marking process is complete and the write barrier is turned off Sweeping will take place when allocations startThis has some stop-the-world process but it’s generally very fast that it is negligible most of the time. The coloring of objects takes place in the gcmarkBits attribute on the span. ConclusionThis post should give you an overview of the Go memory structure and memory management. This is not exhaustive, there are a lot more advanced concepts and the implementation details keep changing from version to version. But for most Go developers this level of information would be sufficient and I hope it helps you write better code, considering these in mind, for more performant applications, and keeping these in mind would help you to avoid the next memory leak issue you might encounter otherwise. I hope you had fun learning this, stay tuned for the next post in the series. References blog. learngoprogramming. com www. ardanlabs. com povilasv. me medium. com/a-journey-with-go medium. com/a-journey-with-go hub. packtpub. com making. pusher. com segment. com/blog go101. orgIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image inspired by a-journey-with-go "},{id:42,url:"https://deepu.tech/polyglot-showdown-with-graalvm/",title:"Polyglot inception with GraalVM. Why? Because it's fun 🏄",body:"2020/02/06 - Have you heard of GraalVM? If you haven’t you should check it out. It is an exciting technology, you know the kind that gets a polyglot developer going 😉 From the website:  GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Groovy, Kotlin, Clojure, and LLVM-based languages such as C and C++. GraalVM is one of its kind. It is a polyglot VM developed at Oracle and apart from its polyglot capabilities it also has been proven to be quite performant and has a smaller memory footprint. It has support for building native images and some modern Java microservice frameworks like Micronaut and Quarkus support GraalVM as it provides faster startup and smaller footprint which is ideal in microservice architectures. So what are the capabilities of GraalVM? Let us take a quick look GraalVM features:  Drop in JDK replacement - Some benchmarks show GraalVM to be faster and less memory hungry than other JDK vendors, I haven’t personally run any benchmarks Drop in NodeJS replacement - Uses GraalVM instead of V8 as the engine for NodeJS Faster runtime for Ruby and R than the default implementations Ahead-of-time(AOT) compiled native images Polyglot capabilities - Java(Any JVM language), JavaScript, Ruby, Python, R, C/C++/Rust(LLVM) along with language interoperability The Truffle Language Implementation framework to implement third-party language supportInstall GraalVM: Before we start, let us setup GraalVM. I used SDKMAN, you can also follow this if you like to install it manually.  First install SDKMAN if you don’t have it already12345678sdk list java# you can use a newer version if availablesdk install java 19. 3. 1. r11-grlsdk use java 19. 3. 1. r11-grl# Check everythingjava -versionnode -vlli --versionThe above will install GraalVM and set it up as java, node and lli context. Please note: If you start a new terminal session, you would have to run sdk use java 19. 3. 1. r11-grl again.  Install LLVM toolchain, Python and Ruby support12345gu install llvm-toolchainexport LLVM_TOOLCHAIN=$(lli --print-toolchain-path)gu install pythongu install ruby Install Rust1curl https://sh. rustup. rs -sSf | shThat’s it we are ready to roll! Lets have some funAs a polyglot developer, GraalVM is quite interesting to me as I can use many languages I love together making use of the best parts from each. Let’s explore the polyglot capabilities offered by GraalVM, please note that support for Python, Ruby, R, and Rust are still in the experimental stage and hence your mileage may vary. We will build a program today using Java, JavaScript, Ruby, Rust, Python, and C++.  I wanted to use Rust and Go as well. While Rust mostly works via the GraalVM lli command line, it has a lot of limitations when embedded in polyglot mode. After a lot of fiddling around, I did manage to get it working. For Golang, it might be possible with this Go LLVM compiler as shown here, but it’s having its own set of issues as well when I tried. So I have given up on Golang for now. Let me know if any of you got it working. We will have a simple(silly 😉) application written in Java that composes methods from different languages for each step from within Java  Python: Filters out the Fibonacci numbers from the given input array JavaScript: Find the cube of each number in the output array from the previous step C++: Get the sum of the numbers in the output array from the previous step Rust: Find the cube-root of the number from the previous step Ruby: Find factorial of the number from the previous step Java: Finally print the result(this is also the wrapper program)If you prefer a more complex example, check this out. Step 1: Java: Let’s start with our Java wrapper program Polyglot. java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java. io. *;import org. graalvm. polyglot. *;class Polyglot {  // We create a polyglot context to evaluate source files  static Context polyglotCtx = Context. newBuilder(). allowAllAccess(true). build();  // Utility method to load and evaluate a source file  static Value loadSource(String language, String fileName) throws IOException {    File file = new File(fileName);    Source source = Source. newBuilder(language, file). build();    return polyglotCtx. eval(source);  }  // Utility method to convert arrays between languages  static int[] getIntArrayFromValue(Value val) {    int[] out = new int[(int) val. getArraySize()];    if (val. hasArrayElements()) {      for (int i = 0; i &lt; val. getArraySize(); i++) {        out[i] = val. getArrayElement(i). asInt();      }    }    return out;  }  public static void main(String[] args) throws IOException {    int[] input = new int[] { 4, 2, 8, 5, 20, 1, 40, 13, 23 };    /* PYTHON: Get the Fibonacci numbers from the array */    loadSource( python ,  pythonpart. py );    Value getFibonacciNumbersFn = polyglotCtx. getBindings( python ). getMember( getFibonacciNumbers );    int[] fibNumbers = getIntArrayFromValue(getFibonacciNumbersFn. execute(input));    /* JAVASCRIPT: Find cube of numbers in the output array */    loadSource( js ,  jspart. js );    Value findCubeOfNumbersFn = polyglotCtx. getBindings( js ). getMember( findCubeOfNumbers );    int[] sqrNumbers = getIntArrayFromValue(findCubeOfNumbersFn. execute(fibNumbers));    /* C++: Get the sum of the numbers in the output array */    loadSource( llvm ,  cpppart );    Value getSumOfArrayFn = polyglotCtx. getBindings( llvm ). getMember( getSumOfArray );    int sum = getSumOfArrayFn. execute(sqrNumbers, sqrNumbers. length). asInt();    /* Rust: Find the cube root of sum */    Value cubeRootFn = loadSource( llvm ,  rustpart. bc ). getMember( cube_root );    // println! macro does not work from Rust when embedded, some issue with mangling    System. out. println( Rust =&gt; Find cube root of the number );    Double cubeRoot = cubeRootFn. execute(sum). asDouble();    /* RUBY: Find factorial of the number */    Value factorialFn = loadSource( ruby ,  rubypart. rb );    long out = factorialFn. execute(cubeRoot). asLong();    System. out. println( Sum:   + sum);    System. out. println( Cube Root:   + cubeRoot);    System. out. println( Factorial:   + out);  }}The utility functions are to simplify the code, now let’s look at each step where it composes the functions. Step 2: Python: We are executing the getFibonacciNumbers function located in file pythonpart. py and passing it our input array. 1234/* PYTHON: Get the Fibonacci numbers from the array */loadSource( python ,  pythonpart. py );Value getFibonacciNumbersFn = polyglotCtx. getBindings( python ). getMember( getFibonacciNumbers );int[] fibNumbers = getIntArrayFromValue(getFibonacciNumbersFn. execute(input));Let’s look at pythonpart. py, it is a standard python program that takes an array and filters out the Fibonacci numbers from it and returns the resulting array. 123456789101112131415161718192021222324import mathdef isPerfectSquare(num):  n = int(math. sqrt(num))  return (n * n == num)# Function to check if the number is in Fibonacci or notdef getFibonacciNumbers(array):  print( Python =&gt; Filtering Fibonacci number from the array );  out = []  n = len(array)  count = 0  for i in range(n):    if (isPerfectSquare(5 * array[i] * array[i] + 4) or      isPerfectSquare(5 * array[i] * array[i] - 4)):      out. append(array[i]);      count = count + 1  if (count == 0):    print( None present );  return outStep 3: JavaScript: We are executing the findCubeOfNumbers function located in file jspart. js and passing the result from the Python function. 1234/* JAVASCRIPT: Find cube of numbers in the output array */loadSource( js ,  jspart. js );Value findCubeOfNumbersFn = polyglotCtx. getBindings( js ). getMember( findCubeOfNumbers );int[] sqrNumbers = getIntArrayFromValue(findCubeOfNumbersFn. execute(fibNumbers));Let’s look at jspart. js, it is a standard JavaScript function that takes an array and maps over the elements and returns the array. But we had to call Array. prototype. map. call instead of just array. map as the array passed by Java is not standard JS array. 12345function findCubeOfNumbers(array) { console. log( JS =&gt; Getting cube of numbers in the array ); return Array. prototype. map. call(array, (it) =&gt; Math. pow(it, 3));}Step 4: C++: We are executing the getSumOfArray function located in the cpppart binary file. We pass the result from JS function and the length of the array here. We have to use compiled binary here unlike Python, Ruby, and JavaScript which are interpreted languages. 1234/* C++: Get the sum of the numbers in the output array */loadSource( llvm ,  cpppart );Value getSumOfArrayFn = polyglotCtx. getBindings( llvm ). getMember( getSumOfArray );int sum = getSumOfArrayFn. execute(sqrNumbers, sqrNumbers. length). asInt();The source of the binary is in cpppart. cpp file. Which is compiled using the below 12export LLVM_TOOLCHAIN=$(lli --print-toolchain-path)$LLVM_TOOLCHAIN/clang++ -shared cpppart. cpp -lpolyglot-mock -o cpppartLet’s look at cpppart. cpp, it is a standard C++ program that exports a function. It takes an array and its length as the arguments and returns a number 1234567891011121314#include&lt;iostream&gt;using namespace std;// Function to find the sum of integer array// extern  C  is required to suppress manglingextern  C  int getSumOfArray(int array[], int size) {  printf( C++ =&gt; Find sum of numbers in an array\n );  int i, sum = 0;  for(i = 0; i &lt; size; i++) {    sum += array[i];  }  return sum;}Step 5: Rust: We are executing the cube_root function located in file rustpart. bc, compiled from rustpart. rs. We pass the result from C++ function here. 12345/* Rust: Find the cube root of sum */Value cubeRootFn = loadSource( llvm ,  rustpart. bc ). getMember( cube_root );// println! macro does not work from Rust when embedded, some issue with manglingSystem. out. println( Rust =&gt; Find cube root of the number );Double cubeRoot = cubeRootFn. execute(sum). asDouble();Let’s look at rustpart. rs, it is a standard Rust function that takes a number finds its cube root and returns it. But we do have to specify #[no_mangle] annotation and we cannot use any crates as well apparently. Simples functions with primitive args/output seem to work but more complex functions do not work when embedded. 123456#[no_mangle]fn cube_root(arg: f64) -&gt; f64 {  arg. cbrt()}fn main(){}We compile the Rust source to binary code using rustc compiler with the --emit=llvm-bc flag 1rustc --emit=llvm-bc rustpart. rsStep 6: Ruby: We are executing the factorial function located in file rubypart. rb. We are passing the result from the Rust function here. 123/* RUBY: Find factorial of the number */Value factorialFn = loadSource( ruby ,  rubypart. rb );long out = factorialFn. execute(cubeRoot). asLong();Let’s look at rubypart. rb, it is a standard Ruby lambda function that takes a number and returns its factorial. 1234factorial = -&gt; (num) {  puts  Ruby =&gt; Find factorial of the number   (1. . num). inject {|product, num| product * num }}And Finally, we print the outputs with Java. Run the program: To run this program we need to compile the C++, Rust and Java files first, and then run it using the JVM provided by GraalVM. Below are the steps, you can save this as run. sh and execute it. 123456export LLVM_TOOLCHAIN=$(lli --print-toolchain-path)$LLVM_TOOLCHAIN/clang++ -shared cpppart. cpp -lpolyglot-mock -o cpppart || exitrustc --emit=llvm-bc rustpart. rs || exitjavac Polyglot. java &amp;&amp; java PolyglotIt will produce the below output: ConclusionWasn’t this fun? So is such a polyglot capability useful? Well that depends, the polyglot capabilities of GraalVM are still not production-ready but it is still useful as it opens up the door for real language interoperability, imagine being able to use a library from any language from your program, this is already possible for many C, Ruby, R, JS and Java libraries with GraalVM but as support becomes better we would be able to break free from being limited to one language. GraalVM seems to be much faster for languages like Ruby than the standard CRuby or JRuby for example and that is promising as it would mean you wouldn’t have to worry much about overheads when embedding multiple languages in your program. GraalVM is one of the most revolutionary technologies I have encountered in recent times and I hope the polyglot language support becomes production-ready soon combined with its native image capabilities it would be a very powerful platform for truly polyglot applications. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Based on official logos of respective projects. "},{id:43,url:"https://deepu.tech/memory-management-in-v8/",title:"🚀 Visualizing memory management in V8 Engine (JavaScript, NodeJS, Deno, WebAssembly)",body:"2020/01/27 - In this multi-part series, I aim to demystify the concepts behind memory management and take a deeper look at memory management in some of the modern programming languages. I hope the series would give you some insights into what is happening under the hood of these languages in terms of memory management. In this chapter, we will look at the memory management of the V8 Engine for ECMAScript and WebAssembly, used by runtimes like NodeJS, Deno &amp; Electron and web browsers like Chrome, Chromium, Brave, Opera, and Microsoft Edge. Since JavaScript is an interpreted language, it needs an engine to interpret and execute code. The V8 engine interprets JavaScript and compiles it down to native machine code. V8 is written in C++ and can be embedded in any C++ application. If you haven’t read the first part of this series, please read it first as I explained the difference between the Stack and Heap memory there which would be useful to understand this chapter. V8 memory structureFirst, let us see what the memory structure of the V8 engine is. Since JavaScript is single-threaded V8 also uses a single process per JavaScript context and hence if you use service workers it will spawn a new V8 process per worker. A running program is always represented by some allocated memory in the V8 process and this is called Resident Set. This is further divided into different segments as below: This is slightly similar to the JVM memory structure we saw in the previous chapter. Let us see what the different segments are for: Heap Memory: This is where V8 stores objects or dynamic data. This is the biggest block of memory area and this is where Garbage Collection(GC) takes place. The entire heap memory is not garbage collected, only the Young and Old space is managed by garbage collection. Heap is further divided into below:  New Space: New space or “Young generation” is where new objects live and most of these objects are short-lived. This space is small and has two semi-space, similar to S0 &amp; S1 in JVM. This space is managed by the “Scavenger(Minor GC)”, we will look at it later. The size of the new space can be controlled using the --min_semi_space_size(Initial) and --max_semi_space_size(Max) V8 flags.  Old Space: Old space or “Old generation” is where objects that survived the “New space” for two minor GC cycles are moved to. This space is managed up by the Major GC(Mark-Sweep &amp; Mark-Compact)”, we will look at it later. The size of old space can be controlled using the --initial_old_space_size(Initial) and --max_old_space_size(Max) V8 flags. This space is divided into two:     Old pointer space: Contains survived objects that have pointers to other objects.    Old data space: Contains objects that just contain data(no pointer to other objects). Strings, boxed numbers, and arrays of unboxed doubles are moved here after surviving in “New space” for two minor GC cycles.     Large object space: This is where objects which are larger than the size limits of other spaces live. Each object gets its own [mmap](https://en. wikipedia. org/wiki/Mmap)'d region of memory. Large objects are never moved by the garbage collector.  Code-space: This is where the Just In Time(JIT) compiler stores compiled code Blocks. This is the only space with executable memory (although Codes may be allocated in “Large object space”, and those are executable, too).  Cell space, property cell space, and map space: These spaces contain Cells, PropertyCells, and Maps, respectively. Each of these spaces contains objects which are all the same size and has some constraints on what kind of objects they point to, which simplifies collection. Each of these spaces is composed of a set of pages. A Page is a contiguous chunk of memory allocated from the operating system with mmap (or [MapViewOfFile](https://docs. microsoft. com/en-us/windows/win32/api/memoryapi/nf-memoryapi-mapviewoffile) on Windows). Each page is 1MB in size, except for Large object space. Stack: This is the stack memory area and there is one stack per V8 process. This is where static data including method/function frames, primitive values, and pointers to objects are stored. The stack memory limit can be set using the --stack_size V8 flag. V8 memory usage (Stack vs Heap)Now that we are clear about how memory is organized let’s see how the most important parts of it are used when a program is executed. Let’s use the below JavaScript program, the code is not optimized for correctness hence ignore issues like unnecessary intermediatory variables and such, the focus is to visualize stack and heap memory usage. 123456789101112131415161718192021222324class Employee { constructor(name, salary, sales) {  this. name = name;  this. salary = salary;  this. sales = sales; }}const BONUS_PERCENTAGE = 10;function getBonusPercentage(salary) { const percentage = (salary * BONUS_PERCENTAGE) / 100; return percentage;}function findEmployeeBonus(salary, noOfSales) { const bonusPercentage = getBonusPercentage(salary); const bonus = bonusPercentage * noOfSales; return bonus;}let john = new Employee( John , 5000, 5);john. bonus = findEmployeeBonus(john. salary, john. sales);console. log(john. bonus);Click on the slides and move forward/backward using arrow keys to see how the above program is executed and how the stack and heap memory is used:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck. As you can see:  Global scope is kept in a “Global frame” on the Stack Every function call is added to the stack memory as a frame-block All local variables including arguments and the return value is saved within the function frame-block on the Stack All primitive types like int &amp; string are stored directly on the Stack. This applies to global scope as well and yes String is a primitive type of JavaScript All object types like Employee &amp; Function are created on the Heap and is referenced from the Stack using Stack pointers. Functions are just objects in JavaScript. This applies to global scope as well Functions called from the current function is pushed on top of the Stack When a function returns its frame is removed from the Stack Once the main process is complete, the objects on the Heap do not have any more pointers from Stack and becomes orphan Unless you make a copy explicitly, all object references within other objects are done using reference pointersThe Stack as you can see is automatically managed and is done so by the operating system rather than V8 itself. Hence we do not have to worry much about the Stack. The Heap, on the other hand, is not automatically managed by the OS and since it’s the biggest memory space and holds dynamic data, it could grow exponentially causing our program to run out of memory over time. It also becomes fragmented over time slowing down applications. This is where garbage collection comes in. Distinguishing pointers and data on the heap is important for garbage collection and V8 uses the “Tagged pointers” approach for this - in this approach, it reserves a bit at the end of each word to indicate whether it is pointer or data. This approach requires limited compiler support, but it’s simple to implement while being fairly efficient. V8 Memory management: Garbage collectionNow that we know how V8 allocates memory, let us see how it automatically manages the Heap memory which is very important for the performance of an application. When a program tries to allocate more memory on the Heap than that is freely available(depending on the V8 flags set) we encounter out of memory errors. An incorrectly managed heap could also cause a memory leak. V8 manages the heap memory by garbage collection. In simple terms, it frees the memory used by orphan objects, i. e, objects that are no longer referenced from the Stack directly or indirectly(via a reference in another object) to make space for new object creation.  Orinoco is the codename of the V8 GC project to make use of parallel, incremental and concurrent techniques for garbage collection, to free the main thread. The garbage collector in V8 is responsible for reclaiming the unused memory for reuse by the V8 process. V8 garbage collectors are generational(Objects in Heap are grouped by their age and cleared at different stages). There are two stages and three different algorithms used for garbage collection by V8: Minor GC (Scavenger): This type of GC keeps the young or new generation space compact and clean. Objects are allocated in new-space, which is fairly small (between 1 and 8 MB, depending on behavior heuristics). Allocation in “new space” is very cheap: there is an allocation pointer which we increment whenever we want to reserve space for a new object. When the allocation pointer reaches the end of the new space, a minor GC is triggered. This process is also called Scavenger and it implements Cheney’s algorithm. It occurs frequently and uses parallel helper threads and is very fast. Let us look at the minor GC process: The new space is divided into two equal-sized semi-spaces: to-space and from-space. Most allocations are made in from-space (except certain kinds of objects, such as executable Codes which are always allocated in old-space). When from-space fills up the minor GC is triggered. Click on the slides and move forward/backward using arrow keys to see the process:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck.  Let us assume that there are already objects on the “from-space” when we start(Blocks 01 to 06 marked as used memory) The process creates a new object(07) V8 tries to get the required memory from from-space, but there is no free space in there to accommodate our object and hence V8 triggers minor GC Minor GC recursively traverses the object graph in “from-space” starting from stack pointers(GC roots) to find objects that are used or alive(Used memory). These objects are moved to a page in the “to-space”. Any objects reference by these objects are also moved to this page in “to-space” and their pointers are updated. This is repeated until all the objects in “from-space” are scanned. By end of this, the “to-space” is automatically compacted reducing fragmentation Minor GC now empties the “from-space” as any remaining object here is garbage Minor GC swaps the “to-space” and “from-space”, all the objects are now in “from-space” and the “to-space” is empty The new object is allocated memory in the “from-space” Let us assume that some time has passed and there are more objects on the “from-space” now(Blocks 07 to 09 marked as used memory) The application creates a new object(10) V8 tries to get required memory from “from-space”, but there is no free space in there to accommodate our object and hence V8 triggers a second minor GC The above process is repeated and any alive objects that survived the second minor GC is moved to the “Old space”. First-time survivors are moved to the “to-space” and the remaining garbage is cleared from “from-space” Minor GC swaps the “to-space” and “from-space”, all the objects are now in “from-space” and the “to-space” is empty The new object is allocated memory in the “from-space”So we saw how minor GC reclaims space from the young generation and keeps it compact. It is a stop-the-world process but it’s so fast and efficient that it is negligible most of the time. Since this process doesn’t scan objects in the “old space” for any reference in the “new space” it uses a register of all pointers from old space to new space. This is recorded to the store buffer by a process called write barriers. Major GC: This type of GC keeps the old generation space compact and clean. This is triggered when V8 decides there is not enough old space, based on a dynamically computed limit, as it gets filled up from minor GC cycles. The Scavenger algorithm is perfect for small data size but is impractical for large heap, as the old space, as it has memory overhead and hence major GC is done using the Mark-Sweep-Compact algorithm. It uses a tri-color(white-grey-black) marking system. Hence major GC is a three-step process and the third step is executed depending on a fragmentation heuristic.   Marking: First step, common for both algorithms, where the garbage collector identifies which objects are in use and which ones are not in use. The objects in use or reachable from GC roots(Stack pointers) recursively are marked as alive. It’s technically a depth-first-search of the heap which can be considered as a directed graph Sweeping: The garbage collector traverses the heap and makes note of the memory address of any object that is not marked alive. This space is now marked as free in the free list and can be used to store other objects Compacting: After sweeping, if required, all the survived objects will be moved to be together. This will decrease fragmentation and increase the performance of allocation of memory to newer objectsThis type of GC is also referred to as stop-the-world GC as they introduce pause-times in the process while performing GC. To avoid this V8 uses techniques like  Incremental GC: GC is done in multiple incremental steps instead of one.  Concurrent marking: Marking is done concurrently using multiple helper threads without affecting the main JavaScript thread. Write barriers are used to keep track of new references between objects that JavaScript creates while the helpers are marking concurrently.  Concurrent sweeping/compacting: Sweeping and compacting are done in helper threads concurrently without affecting the main JavaScript thread.  Lazy sweeping. Lazy sweeping involves delaying the deletion of garbage in pages until memory is required. Let us look at the major GC process:  Let us assume that many minor GC cycles have passed and the old space is almost full and V8 decides to trigger a “Major GC” Major GC recursively traverses the object graph starting from stack pointers to mark objects that are used as alive(Used memory) and remaining objects as garbage(Orphans) in the old space. This is done using multiple concurrent helper threads and each helper follows a pointer. This does not affect the main JS thread.  When concurrent marking is done or if the memory limit is reached the GC does a mark finalization step using the main thread. This introduces a small pause time.  Major GC now marks all orphan object’s memory as free using concurrent sweep threads. Parallel compaction tasks are also triggered to move related blocks of memory to the same page to avoid fragmentation. Pointers are updated during these steps. ConclusionThis post should give you an overview of the V8 memory structure and memory management. This is not exhaustive, there are a lot more advanced concepts and you can learn about them from v8. dev. But for most JS/WebAssembly developers, this level of information would be sufficient and I hope it helps you write better code, considering these in mind, for more performant applications, and keeping these in mind would help you to avoid the next memory leak issue you might encounter otherwise. I hope you had fun learning about the V8 internals, stay tuned for the next post in the series. References v8. dev/blog/trash-talk jayconrod. com blog. codeship. com developer. mozilla. org blog. sessionstack. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:44,url:"https://deepu.tech/memory-management-in-jvm/",title:"🚀 Visualizing memory management in JVM(Java, Kotlin, Scala, Groovy, Clojure)",body:"2020/01/23 - In this multi-part series, I aim to demystify the concepts behind memory management and take a deeper look at memory management in some of the modern programming languages. I hope the series would give you some insights into what is happening under the hood of these languages in terms of memory management. In this chapter, we will look at the memory management of the Java Virtual Machine(JVM) used by languages like Java, Kotlin, Scala, Clojure, Groovy, and so on. If you haven’t read the first part of this series, please read it first as I explained the difference between the Stack and Heap memory there which would be useful to understand this chapter. JVM memory structureFirst, let us see what the memory structure of JVM is. This is based on JDK 11 onwards. Below is the memory available to a JVM process and is allocated by the Operating System(OS).  This is the native memory allocated by the OS and the amount depends on OS, processor, and JRE. Let us see what the different areas are for: Heap Memory: This is where JVM stores object or dynamic data. This is the biggest block of memory area and this is where Garbage Collection(GC) takes place. The size of heap memory can be controlled using the Xms(Initial) and Xmx(Max) flags. The entire heap memory is not committed to the Virtual Machine(VM) as some of it is reserved as virtual space and the heap can grow to use this. Heap is further divided into “Young” and “Old” generation space.  Young generation: Young generation or “New Space” is where new objects live and is further divided into “Eden Space” and “Survivor Space”. This space is managed by “Minor GC” also sometimes called “Young GC”     Eden Space: This is where new objects are created. When we create a new object, memory is allocated here.    Survivor Space: This is where objects that survived the minor GC are stored. This is divided into two halves, S0 and S1.     Old generation: Old generation or “Tenured Space” is where objects that reached the maximum tenure threshold during minor GC live. This space is managed up by “Major GC”. Thread Stacks: This is the stack memory area and there is one stack memory per thread in the process. This is where thread-specific static data including method/function frames and pointers to objects are stored. The stack memory limit can be set using the Xss flag. Meta Space: This is part of the native memory and doesn’t have an upper limit by default. This is what used to be Permanent Generation(PermGen) Space in earlier versions of JVM. This space is used by the class loaders to store class definitions. If this space keeps growing, the OS might move data stored here from RAM to virtual memory which might slow down the application. To avoid that it’s possible to set a limit on meta-space used with the XX:MetaspaceSize and -XX:MaxMetaspaceSize flag in which case the application might just throw out of memory errors. Code Cache: This is where the Just In Time(JIT) compiler stores compiled code blocks that are often accessed. Generally, JVM has to interpret byte code to native machine code whereas JIT-compiled code need not be interpreted as it is already in native format and is cached here. Shared Libraries: This is where native code for any shared libraries used is stored. This is loaded only once per process by the OS. JVM memory usage (Stack vs Heap)Now that we are clear about how memory is organized let’s see how the most important parts of it are used when a program is executed. Let’s use the below Java program, the code is not optimized for correctness hence ignore issues like unnecessary intermediatory variables, improper modifiers, and such, the focus is to visualize stack and heap memory usage. 123456789101112131415161718192021222324252627282930313233class Employee {  String name;  Integer salary;  Integer sales;  Integer bonus;  public Employee(String name, Integer salary, Integer sales) {    this. name = name;    this. salary = salary;    this. sales = sales;  }}public class Test {  static int BONUS_PERCENTAGE = 10;  static int getBonusPercentage(int salary) {    int percentage = salary * BONUS_PERCENTAGE / 100;    return percentage;  }  static int findEmployeeBonus(int salary, int noOfSales) {    int bonusPercentage = getBonusPercentage(salary);    int bonus = bonusPercentage * noOfSales;    return bonus;  }  public static void main(String[] args) {    Employee john = new Employee( John , 5000, 5);    john. bonus = findEmployeeBonus(john. salary, john. sales);    System. out. println(john. bonus);  }}Click on the slides and move forward/backward using arrow keys to see how the above program is executed and how the stack and heap memory is used:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck. As you can see:  Every function call is added to the thread’s stack memory as a frame-block All local variables including arguments and the return value is saved within the function frame-block on the Stack All primitive types like int are stored directly on the Stack All object types like Employee, Integer, String are created on the Heap and is referenced from the Stack using Stack pointers. This applies to static fields as well Functions called from the current function is pushed on top of the Stack When a function returns its frame is removed from the Stack Once the main process is complete the objects on the Heap do not have any more pointers from Stack and becomes orphan Unless you make a copy explicitly, all object references within other objects are done using pointersThe Stack as you can see is automatically managed and is done so by the operating system rather than JVM itself. Hence we do not have to worry much about the Stack. The Heap, on the other hand, is not automatically managed by the OS and since it’s the biggest memory space and holds dynamic data, it could grow exponentially causing our program to run out of memory over time. It also becomes fragmented over time slowing down applications. This is where the JVM helps. It manages the Heap automatically using the garbage collection process. JVM Memory management: Garbage collectionNow that we know how JVM allocates memory, let us see how it automatically manages the Heap memory which is very important for the performance of an application. When a program tries to allocate more memory on the Heap than that is freely available(depending on the Xmx config) we encounter out of memory errors. JVM manages the heap memory by garbage collection. In simple terms, it frees the memory used by orphan objects, i. e, objects that are no longer referenced from the Stack directly or indirectly(via a reference in another object) to make space for new object creation.  The garbage collector in JVM is responsible for:  Memory allocation from OS and back to OS.  Handing out allocated memory to the application as it requests it.  Determining which parts of the allocated memory is still in use by the application.  Reclaiming the unused memory for reuse by the application. JVM garbage collectors are generational(Objects in Heap are grouped by their age and cleared at different stages). There are many different algorithms available for garbage collection but Mark &amp; Sweep is the most commonly used one. Mark &amp; Sweep Garbage collection: JVM uses a separate daemon thread that runs in the background for garbage collection and the process runs when certain conditions are met. Mark &amp; Sweep GC generally involves two phases and sometimes there is an optional third phase depending on the algorithm used.   Marking: First step where garbage collector identifies which objects are in use and which ones are not in use. The objects in use or reachable from GC roots(Stack pointers) recursively are marked as alive.  Sweeping: The garbage collector traverses the heap and removes any object that is not marked alive. This space is now marked as free.  Compacting: After deleting unused objects, all the survived objects will be moved to be together. This will decrease fragmentation and increase the performance of allocation of memory to newer objectsThis type of GC is also referred to as stop-the-world GC as they introduce pause-times in the application while performing GC. JVM offers few different algorithms to choose from when it comes to GC and there might be few more options available depending on the JDK vendor you use(Like the Shenandoah GC, available on OpenJDK). The different implementations focus on different goals like:  Throughput: Time spent collecting garbage instead of application time affects throughput. The throughput ideally should be high(I. e when GC times are low).  Pause-time: The duration for which GC stops the application from executing. The pause-time ideally should be very low.  Footprint: Size of the heap used. This ideally should be kept low. Collectors available as of JDK 11: As of JDK 11, which is the current LTE version, the below garbage collectors are available and the default used is chosen by JVM based on hardware and OS used. We can always specify the GC to be used with the -XX switch as well.  Serial Collector: It uses a single thread for GC and is efficient for applications with small data sets and is most suitable for single-processor machines. This can be enabled using the -XX:+UseSerialGC switch.  Parallel Collector: This one is focused on high throughput and uses multiple threads to speed up the GC process. This is intended for applications with medium to large data sets running on multi-threaded/multi-processor hardware. This can be enabled using the -XX:+UseParallelGC switch.  Garbage-First(G1) Collector: The G1 collector is mostly concurrent (Means only expensive work is done concurrently). This is for multi-processor machines with a large amount of memory and is enabled as default on most modern machines and OS. It has a focus on low pause times and high throughput. This can be enabled using the -XX:+UseG1GC switch.  Z Garbage Collector: This is a new experimental GC introduced in JDK11. It is a scalable low-latency collector. It’s concurrent and does not stop the execution of application threads, hence no stop-the-world. It is intended for applications that require low latency and/or use a very large heap(multi-terabytes). This can be enabled using the -XX:+UseZGC switch. GC process: Regardless of the collector used, JVM has two types of GC process depending on when and where its performed, the minor GC and major GC. Minor GC: This type of GC keeps the young generation space compact and clean. This is triggered when the below conditions are met:  JVM is not able to get the required memory from the Eden space to allocate a new objectInitially, all the areas of heap space are empty. Eden memory is the first one to be filled, followed by survivor space and finally by tenured space. Let us look at the minor GC process: Click on the slides and move forward/backward using arrow keys to see the process:                                                Note: If the slides look cut off at edges, then click on the title of the slide or here to open it directly in SpeakerDeck.  Let us assume that there are already objects on the Eden space when we start(Blocks 01 to 06 marked as used memory) The application creates a new object(07) JVM tries to get the required memory from Eden space, but there is no free space in Eden to accommodate our object and hence JVM triggers minor GC The GC recursively traverses the object graph starting from stack pointers to mark objects that are used as alive(Used memory) and remaining objects as garbage(Orphans) JVM chooses one random block from S0 and S1 as the “To Space”, let’s assume it was S0. The GC now moves all the alive objects into the “To Space”, S0, which was empty when we started and increments their age by one.  The GC now empties the Eden space and the new object is allocated memory in the Eden space Let us assume that some time has passed and there are more objects on the Eden space now(Blocks 07 to 13 marked as used memory) The application creates a new object(14) JVM tries to get the required memory from Eden space, but there is no free space in Eden to accommodate our object and hence JVM triggers a second minor GC The mark phase is repeated and alive/orphan objects are marked including the ones in survivor space “To Space” JVM chooses the free S1 as the “To Space” now and S0 becomes “From Space”. The GC now moves all the alive objects from Eden space and the “From Space”, S0, into the “To Space”, S1, which was empty when we started, and increments their age by one. Since some objects don’t fit here, they are moved to the “Tenured Space” as the survivor space cannot grow and this process is called premature promotion. This can happen even if one of the survivor space is free The GC now empties the Eden space and the “From Space”, S0, and the new object is allocated memory in the Eden space This keeps on repeating for each minor GC and the survivors are shifted between S0 and S1 and their age is incremented. Once the age reaches the “max-age threshold”, 15 by default, the object is moved to the “Tenured space”So we saw how minor GC reclaims space from the young generation. It is a stop-the-world process but it’s so fast that it is negligible most of the time. Major GC: This type of GC keeps the old generation(Tenured) space compact and clean. This is triggered when the below conditions are met:  Developer calls System. gc(), or Runtime. getRunTime(). gc() from the program.  JVM decides there is not enough tenured space as it gets filled up from minor GC cycles.  During minor GC, if the JVM is not able to reclaim enough memory from the Eden or survivor spaces.  If we set a MaxMetaspaceSize option for the JVM and there is not enough space to load new classes. Let us look at the major GC process, it’s not as complex as minor GC:  Let us assume that many minor GC cycles have passed and the tenured space is almost full and JVM decides to trigger a “Major GC” The GC recursively traverses the object graph starting from stack pointers to mark objects that are used as alive(Used memory) and remaining objects as garbage(Orphans) in the tenured space. If the major GC was triggered during a minor GC the process includes the young(Eden &amp; Survivor) and tenured space The GC now removed all orphan objects and reclaims the memory During a major GC event, if there are no more objects in the Heap, the JVM reclaims memory from the meta-space as well by removing loaded classes from it this is also referred to as full GCConclusionThis post should give you an overview of the JVM memory structure and memory management. This is not exhaustive, there are a lot more advanced concepts and tuning options available for specific use cases and you can learn about them from https://docs. oracle. com. But for most JVM(Java, Kotlin, Scala, Clojure, JRuby, Jython) developers this level of information would be sufficient and I hope it helps you write better code, considering these in mind, for more performant applications and keeping these in mind would help you to avoid the next memory leak issue you might encounter otherwise. I hope you had fun learning about the JVM internals, stay tuned for the next post in the series. References docs. oracle. com pythontutor. com/java. html www. journaldev. com www. yourkit. com dzone. com www. infoq. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:45,url:"https://deepu.tech/life-of-a-full-stack-developer/",title:"Life of a Full-stack developer 😱 🤯 😱",body:"2020/01/19 - Hello friends, the second edition of my book “Full Stack Development with JHipster”, co-authored with Sendil Kumar N, is published this week and I thought of dedicating it to all the Full Stack Developers out there. Disclaimer: This article contains excerpts from my book “Full Stack Development with JHipster – Second Edition” published by Packt. According to the Stack Overflow developer survey 2019, full stack developer is the most popular developer title. The software industry defines a full stack developer as someone who can work on different areas of an application stack. The term stack refers to the different components and tools that make up an application. In terms of web application development, the stack can be broadly classified into two areas—frontend and backend stack, also known as the client-side and server-side stack. The term frontend generally refers to the part of the code that is responsible for rendering the user interface, and the term backend refers to the part that is responsible for the business logic, database interactions, user authentication, server configuration, and so on. There is also the DevOps part of the application, which includes continuous integration, production deployment, and so on. A full-stack Java web application developer is expected to work on both frontend and backend technologies, ranging from writing HTML/JavaScript for the user interface to writing Java class files for business logic and SQL queries for database operations. They are also expected to work on DevOps, ranging from production deployments to setting up continuous integration and continuous delivery (CI/CD) as required. With an ever-evolving software architecture landscape, the scope of technologies that a full stack web developer is expected to work with has increased dramatically. It is no longer enough that we can write HTML and JavaScript to build a user interface — we are expected to know client-side frameworks, such as Angular, React, and Vue. js. It is also not enough that we are proficient in enterprise Java and SQL — we are expected to know server-side frameworks, such as Spring, Hibernate, Play, and Quarkus. Modern full-stack web development: The life of a full stack developer would be worthy of a whole book by itself, so let’s leave that topic for another day. Expectations from #java full stack developer these days. . spring/#quarkus/micronautsql/nosql#react/angular/vueWebpack/bazel/rollupJs/ts, html, css/sass#docker, #kubernetesaws/azure/gcp#devops, terraform/cloudformationShell scripting, maven/gradleMilking a unicorn 🦄 pic. twitter. com/bCFYHsBbKs &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) August 5, 2019Instead, let’s look at a user story from a full-stack Java web application and see what is involved. What a day looks like: Let’s use an example of developing a user management module for a typical Java web application. Let’s assume that you would be writing unit test cases for all of the code, and so we won’t look at it in detail here:  You would start by designing the architecture for the feature. You would decide on the plugins and frameworks to use, patterns to follow, and so on.  You will be modeling the domain model for the feature depending on the database technology used.  Then, you would create server-side code and database queries to persist and fetch data from the database.  Once the data is ready, you would implement the server-side code for any business logic.  Then, you would implement an API that can be used to provide data for the presentation over an HTTP connection.  You would write integration tests for the API.  Since the backend is ready, you would start writing frontend code in JavaScript or similar technology.  You would write client-side services to fetch data from the backend API.  You would write client-side components to display the data on a web page.  You would build the page and style it as per the design provided.  You would write some automated end-to-end tests for the web page.  You are not done yet. Once you have tested whether everything works locally, you would create pull requests or check the code into the version control system used.  You would wait for the continuous integration process to verify everything and fix anything that is broken.  Once everything is green and the code is accepted, you would typically start the deployment of this feature to a staging or acceptance environment, either on-premises or to a cloud provider using technologies like Docker and Kubernetes. If you choose the latter, you would be expected to be familiar with the cloud technologies used as well. You would also be upgrading the database schema as necessary and writing migration scripts when required.  Once the feature is accepted, you might be responsible for deploying it into the production environment in a similar way, troubleshooting issues where necessary. In some teams, you might swap the steps with other team members so that you would be deploying a feature developed by your coworker while they deploy yours.  You might also be responsible, along with your coworkers, for making sure that the production environment is up and running, including the database, virtual machines, and so on. As you can see, it is no easy task. The range of responsibilities spans from making stylesheet updates on the client-side to running database migration scripts on a virtual machine in the production cloud service. If you are not familiar enough with the setup, then this would be a herculean task, and you would soon be lost in the vast ocean of frameworks, technologies, and design patterns out there. Challenges: Full-stack development is not for the faint-hearted. It takes a lot of time and effort to keep yourself up to date with the various technologies and patterns in multiple disciplines of software development. The following are some of the common problems you might face as a full stack Java developer:  Client-side development is not just about writing plain HTML and JavaScript anymore. It is becoming as complex as server-side development, with build tools, transpilers, frameworks, and patterns.  There is a new framework almost every week in the JavaScript world, and if you are coming from outside a Java background, it could be very overwhelming for you.  Container technologies such as Docker &amp; Kubernetes revolutionized the software industry, but they also introduced a lot of new stuff to learn and keep track of, such as orchestration tools and container management tools.  Cloud services are growing day by day. To stay on track, you would have to familiarize yourself with their APIs and related orchestration tools.  Java server-side technologies have also undergone a major shift in recent times with the introduction of JVM languages, such as Scala, Groovy, and Kotlin, forcing you to keep yourself up to date with them. On the other side, server-side frameworks are becoming more feature-rich, and therefore more complex. The most important thing of all is to make sure that all of these work well together when required. This task will need a lot of configuration, some glue code, and endless cups of coffee. Conclusion: It’s very easy to get lost here, and this is where technologies such as JHipster and Spring Boot step in to help. They help by providing the wiring between moving parts so that you only need to concentrate on writing business code. JHipster also helps by providing the abstractions to deploy and manage the application to various cloud providers. A full-stack developer is one of the toughest roles in our industry these days and I think it is one that is under-appreciated a lot. If you are a full stack developer be proud as it takes some courage to be one. You may not be an expert in a single technology or framework but you are a magician of sorts. You can juggle between so many technologies and frameworks without breaking a sweat. You are the real 10x developer, what the heck, you are a 100x developer, not because you use specific keys on your keyboard more than others, its because your job used to be something that required at least four people to work together. You may not get enough credits, though you deserve it just for the effort that is required to keep up, you are awesome. Though the satisfaction of being able to work on all aspects of application development is rewarding, It is still a shame that we are not getting the salary that is the sum paid to four people who were needed to do this work 😉 So if you are a proud full stack developer let me know in the comments and tell us how your day looks like. If you liked this article you might like my book as well. You can get it from Packt and Amazon.  If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Arian Darvishi on Unsplash "},{id:46,url:"https://deepu.tech/memory-management-in-programming/",title:"🚀 Demystifying memory management in modern programming languages",body:"2020/01/08 - In this multi-part series, I aim to demystify the concepts behind memory management and take a deeper look at memory management in some of the modern programming languages. I hope the series would give you some insights into what is happening under the hood of these languages in terms of memory management. Learning about memory management will also help us to write more performant code as the way we write code also has an impact on memory management regardless of the automatic memory management technique used by the language. Part 1: Introduction to Memory managementMemory management is the process of controlling and coordinating the way a software application access computer memory. It is a serious topic in software engineering and its a topic that confuses some people and is a black box for some. What is it?: When a software runs on a target Operating system on a computer it needs access to the computers RAM(Random-access memory) to:  load its own bytecode that needs to be executed store the data values and data structures used by the program that is executed load any run-time systems that are required for the program to executeWhen a software program uses memory there are two regions of memory they use, apart from the space used to load the bytecode, Stack and Heap memory. Stack: The stack is used for static memory allocation and as the name suggests it is a last in first out(LIFO) stack (Think of it as a stack of boxes).  Due to this nature, the process of storing and retrieving data from the stack is very fast as there is no lookup required, you just store and retrieve data from the topmost block on it.  But this means any data that is stored on the stack has to be finite and static(The size of the data is known at compile-time).  This is where the execution data of the functions are stored as stack frames(So, this is the actual execution stack). Each frame is a block of space where the data required for that function is stored. For example, every time a function declares a new variable, it is “pushed” onto the topmost block in the stack. Then every time a function exits, the topmost block is cleared, thus all of the variables pushed onto the stack by that function, are cleared. These can be determined at compile time due to the static nature of the data stored here.  Multi-threaded applications can have a stack per thread.  Memory management of the stack is simple and straightforward and is done by the OS.  Typical data that are stored on stack are local variables(value types or primitives, primitive constants), pointers and function frames.  This is where you would encounter stack overflow errors as the size of the stack is limited compared to the Heap.  There is a limit on the size of value that can be stored on the Stack for most languages. Stack used in JavaScript, objects are stored in Heap and referenced when needed. Here is a video of the same. Heap: Heap is used for dynamic memory allocation and unlike stack, the program needs to look up the data in heap using pointers (Think of it as a big multi-level library).  It is slower than stack as the process of looking up data is more involved but it can store more data than the stack.  This means data with dynamic size can be stored here.  Heap is shared among threads of an application.  Due to its dynamic nature heap is trickier to manage and this is where most of the memory management issues arise from and this is where the automatic memory management solutions from the language kick in.  Typical data that are stored on the heap are global variables, reference types like objects, strings, maps, and other complex data structures.  This is where you would encounter out of memory errors if your application tries to use more memory than the allocated heap(Though there are many other factors at play here like GC, compacting).  Generally, there is no limit on the size of the value that can be stored on the heap. Of course, there is the upper limit of how much memory is allocated to the application. Why is it important?: Unlike Hard disk drives, RAM is not infinite. If a program keeps on consuming memory without freeing it, ultimately it will run out of memory and crash itself or even worse crash the operating system. Hence software programs can’t just keep using RAM as they like as it will cause other programs and processes to run out of memory. So instead of letting the software developer figure this out, most programming languages provide ways to do automatic memory management. And when we talk about memory management we are mostly talking about managing the Heap memory. Different approaches?: Since modern programming languages don’t want to burden(more like trust 👅) the end developer to manage the memory of his/her application most of them have devised a way to do automatic memory management. Some older languages still require manual memory handling but many do provide neat ways to do that. Some languages use multiple approaches to memory management and some even let the developer choose what is best for him/her(C++ is a good example). The approaches can be categorized as below Manual memory management: The language doesn’t manage memory for you by default, it’s up to you to allocate and free memory for the objects you create. For example, C and C++. They provide the malloc, realloc, calloc, and free methods to manage memory and it’s up to the developer to allocate and free heap memory in the program and make use of pointers efficiently to manage memory. Let’s just say that it’s not for everyone 😉. Garbage collection(GC): Automatic management of heap memory by freeing unused memory allocations. GC is one of the most common memory management in modern languages and the process often runs at certain intervals and thus might cause a minor overhead called pause times. JVM(Java/Scala/Groovy/Kotlin), JavaScript, C#, Golang, OCaml, and Ruby are some of the languages that use Garbage collection for memory management by default.   Mark &amp; Sweep GC: Also known as Tracing GC. Its generally a two-phase algorithm that first marks objects that are still being referenced as “alive” and in the next phase frees the memory of objects that are not alive. JVM, C#, Ruby, JavaScript, and Golang employ this approach for example. In JVM there are different GC algorithms to choose from while JavaScript engines like V8 use a Mark &amp; Sweep GC along with Reference counting GC to complement it. This kind of GC is also available for C &amp; C++ as an external library.  Reference counting GC: In this approach, every object gets a reference count which is incremented or decremented as references to it change and garbage collection is done when the count becomes zero. It’s not very preferred as it cannot handle cyclic references. PHP, Perl, and Python, for example, uses this type of GC with workarounds to overcome cyclic references. This type of GC can be enabled for C++ as well. Resource Acquisition is Initialization (RAII): In this type of memory management, an object’s memory allocation is tied to its lifetime, which is from construction until destruction. It was introduced in C++ and is also used by Ada and Rust. Automatic Reference Counting(ARC): It’s similar to Reference counting GC but instead of running a runtime process at a specific interval the retain and release instructions are inserted to the compiled code at compile-time and when an object reference becomes zero its cleared automatically as part of execution without any program pause. It also cannot handle cyclic references and relies on the developer to handle that by using certain keywords. Its a feature of the Clang compiler and provides ARC for Objective C &amp; Swift. Ownership: It combines RAII with an ownership model, any value must have a variable as its owner(and only one owner at a time) when the owner goes out of scope the value will be dropped freeing the memory regardless of it being in stack or heap memory. It is kind of like Compile-time reference counting. It is used by Rust, in my research I couldn’t find any other language using this exact mechanism.  We have just scratched the surface of memory management. Each programming language uses its own version of these and employs different algorithms tuned for different goals. In the next parts of the series, we will take a closer look at the exact memory management solution in some of the popular languages. Stay tuned for upcoming parts of this series:  Part 2: Memory management in JVM(Java, Kotlin, Scala, Groovy) Part 3: Memory management in V8(JavaScript/WebAssembly) Part 4: Memory management in Go Part 5: Memory management in Rust Part 6: Memory management in PythonReferences homepages. inf. ed. ac. uk javarevisited. blogspot. com net-informations. com gribblelab. org medium. com/computed-comparisons en. wikipedia. org/wiki/Garbage-collection en. wikipedia. org/wiki/Automatic-Reference-Counting blog. sessionstack. comIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Image credits:Stack visualization: Created based on pythontutor. Ownership illustration: Link Clark, The Rust team under Creative Commons Attribution Share-Alike License v3. 0. "},{id:47,url:"https://deepu.tech/languages-to-learn-in-2020/",title:"7 Languages to learn in 2020 with free resources to get started",body:"2020/01/01 - We are at the dawn of another decade and the software industry is becoming bigger than ever. If you have to bet on a career than it would be software development. If you are already a software developer then there is so much to learn out there in the next decade. Let us start with programming Languages that you absolutely must learn if you haven’t already. These are the Languages that are on the rise and the ones that are not going away any time soon. My recommendations are also based on practicality and the advantage the language would you give you on the software development marketplace. So let’s start in no particular order. Kotlin: Kotlin is one of the fastest-growing languages in the last few years and is an excellent choice for a JVM language. It has great interoperability with Java, has great features and is not very complex to learn. Why:  JVM is still one of the biggest and most used platforms, especially trusted and used in enterprises world over.  JVM has a pretty great Job market and there is always a demand for JVM developers in the foreseeable future.  Kotlin’s interoperability with Java helps with adopting it incrementally.  Has simple and intuitive syntax, a great feature set and great concurrency and asynchronous programming support with Coroutines.  It is the language of choice these days for Android programming and hence gives you a wider scope if you wish to dive into mobile application development. Who should learn it:  If you are a Java programmer and you are looking to learn a language that you can use without having to switch completely to a new stack.  If you are interested in the JVM but you think Java is not cool anymore.  Web application developers or full-stack developers who want to learn a new language.  If you are interested in Android application development then Kotlin is the place to start. Where to start:  The official tutorial is a great place to start and its free with an interactive playground. The official getting started guides can be found here.  This is a nice and free tutorial on medium for Android development with Kotlin.  This Udemy course is great if you want to focus on Android development with Kotlin. It’s not a free course though. Golang: Golang is a simple language by design and has become hugely popular thanks to the marketing from google and early adoption by popular tools like Docker and Kubernetes. Here is my detailed review of the language.                               My reflections on Golang:             12-Jul-2019                           #go #programming #languages #thepragmaticprogrammer                                    Why:  One of the simplest language to learn and a great community.  It has pretty good performance and can be used as a general-purpose language if you are not bothered by the boilerplate.  A great choice for use cases with heavy concurrency or parallelism requirements.  It has a good demand in the job market and its adoption is only increasing with many big names using Golang for their main platforms. Who should learn it:  Developers working with languages like C, C++ would be at home with Go, while you might find the features limited, once you get used to it its a great language to have in your arsenal.  System programmers looking to learn a new language will find Go pretty descent.  DevOps engineers who are more on the Ops side looking to learn a language would find Go quite handy.  If you are writing microservices than Golang is a good choice for that and you should learn it.  If you are a front-end developer wishing to venture into web assembly, then Go is one of the easiest languages to start with as it supports web assembly. Where to start:  The official tour of Go is a great place to start and its free with an interactive playground.  This freeCodeCamp video tutorial if you prefer videos. TypeScript: You like it or not JavaScript is here to stay as long as the internet is around. So if you are not already a fan why not embrace it in the form of TypeScript instead of hating it.                               My love-hate relationship with JavaScript:             21-Nov-2019                           #javascript #programming #languages #thepragmaticprogrammer                                    Why:  JavaScript is the most used language and has the biggest ecosystem and TypeScript being a strict superset of JavaScript means you can make use of it any use case that JavaScript can handle.  TypeScript is one of the rapidly growing languages and will help with some of the shortcomings in JavaScript.  Demand for front-end engineers is never going away and with MVVM frameworks like Angular, React and Vue it’s only rising.  The extent of the JavaScript/TypeScript ecosystem spreads from front-end to desktop to serverside to robotics. So you can say JavaScript is everywhere. Who should learn it:  JavaScript developers should learn TypeScript, it will make maintaining big JavaScript projects easier and will help you write better code.  If you are coming from a strictly typed language background and you are thinking of exploring the world of frontend engineering then TypeScript is a great place to start.  If you are Java developer and want to do front-end, then TypeScript is easier to start with due to its similarity to Java.  You want to get your hands dirty with Angular, React or Vue but you hate JavaScript then TypeScript should be your logical choice.  If you are planning to learn Angular framework then knowing TypeScript is a must.  If you are considering a career in IT as a front-end developer, then JavaScript is the ultimate choice and once you learned JavaScript then TypeScript is a logical next step. Where to start:  This is a detailed tutorial on TypeScript and is free.  This is a free quickstart guide if you don’t want to spend too much time.  This video tutorial from one of my conference deep dive session if you want to see advanced TypeScript features in action. Python: Python has been around for a while and in recent times it has seen a sudden increase in popularity. The popularity of data science and machine learning should be attributed to that as Python has become the go-to Language in those fields. Why:  Python is a general-purpose language and like JavaScript, it has a wide scope of application.  It’s a dynamic language like JavaScript and hence great for scripting.  It’s becoming the go-to language for machine learning and data science.  It has a great job market and a huge community. Who should learn it:  If you are looking to get into data science or machine learning, then Python is your language.  DevOps engineers will find Python easy to start with and great for general-purpose scripting.  If you are looking to learn a general-purpose language then Python is a great choice. Where to start:  The official Python tutorial here.  This free and interactive tutorial.  This w3school tutorial with interactive playground. Rust: Rust is the most loved language according to the Stack Overflow developer survey last few years, not only this, the language is gaining momentum and is one of the fastest-growing languages. Here is my detailed review of Rust.                               My first impressions of Rust:             07-Nov-2019                           #rust #programming #languages #thepragmaticprogrammer                                    Why:  A great alternative to C/C++ with similar performance and better memory safety.  Extremely memory safe, highly performant and flexible language.  Suitable for low-level programming and hence a great choice for systems programming, concurrency and so on.  It has web assembly support. Who should learn it:  C/C++ programmers should consider learning Rust. Its a better alternative to C/C++ IMO and is more future proof.  If you are a systems programmer, then you should learn Rust as it’s becoming a fast-growing choice in that space.  If you are a front-end developer wishing to venture into web assembly, then Rust is a great choice as it supports web assembly. It might not be as easy as Golang to start with but definitely a great choice.  If you are a developer writing low-level programs that requires extreme memory safety, performance and memory efficiency then Rust is a great choice with its memory safety guarantee, Ownership model and pointer support. You can tune your programs to be very memory efficient.  If you are into concurrency and parallelism then Rust has great support for that with a pluggable multi-threading model which you can tune to your likeness. With libraries like Tokio, you can achieve great concurrency performance while maintaining memory safety. Where to start:  The official Rust book is the best place to get started. It’s free and interactive.  The official Rust by examples if you want to learn hands-on. Java: If you are a millennial you might think that Java is too old school and not cool anymore, but I assure you that Java will outlive a lot of languages that are considered new and cool. Java is one of the most mature languages out there and the with current release model of Java new features are being added twice a year closing the gap with other modern languages. Why:  JVM is still one of the biggest and most used platforms, especially trusted and used in enterprises world over.  JVM has a pretty great Job market and there is always a demand for JVM developers in the foreseeable future.  It has simple syntax, a great feature set, and a huge ecosystem and community.  It is the language of choice for enterprises and has some of the most stable and trusted web application frameworks and libraries in the ecosystem.  It has one of the best ecosystems around it for building web applications and especially enterprise web applications. Who should learn it:  If you are a front end engineer and you want to become a full-stack engineer then JVM is a great choice as it has the best Job market for full-stack engineers.  Web application developers or full-stack developers who want to learn a JVM language.  If you are considering a career in IT as a backend developer, but you are unsure then JVM and Java is a safe bet and it has a great community and ecosystem to help you. Where to start:  This w3school interactive tutorial to get a nice introduction.  This interactive tutorial on JavaTpoint.  This video tutorial. C# : C# is one of the popular languages in the programming world and is the go-to choice in the . NET world. It is a multiparadigm language suitable for different styles of programming. Like Java, JavaScript and Python it has retained its position in the top 10 languages in most of the language surveys. Why:  It’s kind of a defacto choice for Windows programming and Windows is still the most used OS in the world.  Well integrated with Windows and . NET platform with great support for enterprise applications.  general-purpose language can be used for web applications, desktop applications, CLI tools and so on.  It has a great Job market because Windows has a major market share in terms of OS usage and enterprise usage. Who should learn it:  If you are thinking of programming applications for Windows or . NET then this should be your language of choice.  If you are a Java developer and want to learn . NET then C# is the logical choice as it has a great syntax and concept resemblance to Java. Where to start:  This w3school interactive tutorial to get a nice introduction.  This exhaustive tutorial on JavaTpoint. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Based on official logos. "},{id:48,url:"https://deepu.tech/three-tips-for-clean-code/",title:"3 tips for clean & efficient code",body:"2019/12/16 - One of the most important aspect of software engineering is code quality and one of the most important criteria should be readability and maintainability of your code, especially when working in teams where others have to read and understand your code. The best code in my opinion is that doesn’t need any explanation or even documentation and that even a complete beginner can read and understand.  It is easy to write complex and cryptic piece of code. It is much harder to write simple and readable code. It is easy to get carried away and write long and complex functions and code that shows off how smart you are and how you can write something in the smartest way, especially when you area beginner and you are trying impress your peers. But the smartest way might not be the most readable and maintainable way. It doesn’t take a lot of skill to do that IMO, but writing simple and clean code that is easy to read and maintain takes skill. Its not a skill that you can acquire overnight, its something you develop over the years. Below are some tips to make your code more efficient, readable, easy to navigate and more maintainable. This is not an exhaustive list, its just some of them that could help. 1. Fail fast and early: When writing any logic try to do the failure assertions as early as possible and fail fast. It makes programs feel faster as you wouldn’t execute a lot of stuff first just to fail at the end for something that you could have checked to begin with. This approach also makes your code easier to understand as it makes intent much clearer. Consider the below function, let us use JavaScript on NodeJS for simplicity. Note that i’m using the syncrnous methods from the NodeJS FileSystem API for simplicity. 123456789101112131415function processFile(filePath) { if (fs. existsSync(filePath)) {  // check if file exists  var fileContent = fs. readFileSync(filePath,  utf8 ); // read the file content  if (fileContent !==   ) {   console. log( Content found );   // do stuff  } else {   throw Error( No content found on file! );  } } else {  throw Error( File not found! ); }}We are fetching the given file and doing some processing on it, if the file is not found or if the content is empty we throw an error. There is nothing wrong with this method but if you have a lot of steps in processing the reader has to go through all that first to see the error handling hence the intent is not that clear, also the cyclomatic complexity of the function is higher due to multiple if/else. Consider the below version of the same function 123456789101112131415function processFile(filePath) { if (!fs. existsSync(filePath)) {  // check if file does not exist  throw Error( File not found! ); } var fileContent = fs. readFileSync(filePath,  utf8 ); // read the file content if (fileContent ===   ) {  throw Error( No content found on file! ); } console. log( Content found ); // do stuff}This does the same logic but is less complex and more readable. It has less cyclomatic complexity and the intent is clear 2. Return early: Similar to failing early it is also much nicer to return early, i. e use the language’s ability to flow code based on returns to reduce the complexity of code and improve readability and intent. Consider the below logic added to our function 1234567891011121314151617181920function processFile(filePath) {  . . .   console. log( Content found );  // check if the file content is base64 encoded: Lib used https://github. com/miguelmota/is-base64  if (isBase64(fileContent. trim())) {    console. log( Decoding content );    // check if the base64 content is image and return a placeholder    if (isBase64(fileContent. trim(), { allowMime: true })) {      return  Image content ;    } else {      // decode to string      var buffer = Buffer. from(fileContent,  base64 );      var decoded = buffer. toString( ascii );      return decoded;    }  } else {    return fileContent;  }}The logic checks if the content is base64 encoded, and if not decodes it. Again the function works fine but consider the below version as well 12345678910111213141516171819function processFile(filePath) {  . . .   console. log( Content found );  // check if the base64 content is image and return a placeholder  if (isBase64(fileContent, { allowMime: true })) {    return  Image content ;  }  // check if the file content is not base64 encoded: Lib used https://github. com/miguelmota/is-base64  if (!isBase64(fileContent. trim())) {    return fileContent;  }  console. log( Decoding content );  // decode to string  var buffer = Buffer. from(fileContent,  base64 );  var decoded = buffer. toString( ascii );  return decoded;}Isn’t this much simpler and easier to reason with. So try to return early and keep the code flow simple. 3. Write small focused functions: Finally do not write huge functions, even if you are doing imperative style of programming try to break it down into small functions as much as possible. The above example is not a huge function, but still you could break it logically into two as below 12345678910111213141516171819202122function decodeBase64(fileContent) {  // check if the base64 content is image and return a placeholder  if (isBase64(fileContent, { allowMime: true })) {    return  Image content ;  }  // check if the file content is not base64 encoded: Lib used https://github. com/miguelmota/is-base64  if (!isBase64(fileContent. trim())) {    return fileContent;  }  console. log( Decoding content );  // decode to string  var buffer = Buffer. from(fileContent,  base64 );  var decoded = buffer. toString( ascii );  return decoded;}function processFile(filePath) {  . . .   console. log( Content found );  return decodeBase64(fileContent);}Having smaller focused functions helps with following  Unit testing becomes easier.  Functions become reuseable and composable.  Easier to focus and read, without having to build a mental model of the entire logic Easier to change something in a small function than in a huge one, as you will have less cases to worry about.  Easier to debug and easier to spot bugs.  reduces side effects as the function does something very focused. Bonus: Write pure functions: This is not a functional programming thing, though it was made popular by FP. In general mutations are always tricky and could cause unwanted bugs. When we try to avoid side effects(includes external date mutations) by keeping small functions pure we also reduce the surface area for such bugs. So keep your pure functions separate from impure functions and try to write as much as pure functions. Do side-effects and external data/state mutations from within a unction only if its unavoidable. You can follow this regardless of programming paradigm you are following. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Christopher Robin Ebbinghaus on Unsplash "},{id:49,url:"https://deepu.tech/golang-for-javascript-developers-part-2/",title:"Golang for JavaScript developers - Part 2",body:"2019/12/12 - If you are a JavaScript developer thinking about learning another programming language, then Golang is a great choice. It is simple, has a lot of momentum, is very performant and has some similarities to JavaScript. This post is not a comparison of the languages or is stating that they are very similar. Its a guide for JavaScript developers to grasp Golang quickly. There are many aspects of Go that are entirely different from JavaScript. We will touch upon that as well. In the previous part of this series, we learned about things that are more similar between JS and Go. We touched upon:  Functions Scope Flow control Memory managementIn this part of the series, we will touch upon things that are more different between JS and Go. If you haven’t read the previous part please read it first. Things that are more differentAs you can see there are more things in this part than previous, but please also note that some differences are quite subtle so it would be easy to digest for a JavaScript developer. Types &amp; Variables: This is one of the main differences. JavaScript is dynamic and loosely typed and Go is static and strictly typed. JavaScript 123456789101112131415var foo = { message:  hello ,};var bar = foo;// mutatebar. message =  world ;console. log(foo. message === bar. message); // prints 'true'// reassignbar = { message:  mars ,};console. log(foo. message === bar. message); // prints 'false'Go 1234567891011121314151617181920212223242526272829var foo = struct {  message string}{ hello }var bar = foo // will create a copy of foo and assign to bar// mutates only bar// note bar. message is short for (*bar). messagebar. message =  world fmt. Println(foo. message == bar. message) // prints  false // reassign barbar = struct {  message string}{ mars }fmt. Println(foo. message == bar. message) // prints  false var barPointer = &amp;foo // assigns pointer to foo// mutates foobarPointer. message =  world fmt. Println(foo. message == barPointer. message) // prints  true // reassigns foo*barPointer = struct {  message string}{ mars }fmt. Println(foo. message == bar. message) // prints  true Similarities:  There is no much similarity other than the name of keywords var and const. var keyword in Go is closer to let keyword in JS in terms of behavior.  Multiple var can be declared together like var a, foo, bar int; similar to JS. But in Go, you can go further and initialize them as well like var a, foo, bar = true, 10,  hello . In JS you can do a destructuring assignment for similar effect like var [a, foo, bar] = [true, 10,  hello ]Differences:  Go needs type information at compile time either by specified type or from type inference.  Go has value types(primitives, arrays, and structs), reference types(slice, map &amp; channels) and pointers. JS has value types(primitives) and reference types(objects, arrays, functions).  The type of a variable cannot be changed after the declaration in Go.  Variables assignments cannot use short-circuit expressions in Go.  var has a shorthand syntax with := inside Go functions.  Go strictly doesn’t let you have unused variables, any unused variable must be named as _, which is a reserved character.  JS does not have private/public access modifiers(There is a proposal to add it), In Go, however, you can modify that using the naming convention. Starting a field, variable name with uppercase will make it public and lowercase will make it private.  const in Go is not the same as in JavaScript. Only primitives like character, string, boolean, or numeric values can be assigned to constants in Go.  Arrays in Go are different from JS as they are fixed length. JS arrays are dynamic and hence are more similar to Go slices which are slices of an array with dynamic length. JavaScript 1234567const foo = [ Rick ,  Morty ];// Adds to the end of the array. foo. push( Beth );// Removes from the end of the array. element = foo. pop();Go 12345678910foo := []string{ Rick ,  Morty } // creates a slice// Adds to the end of the array. foo = append(foo,  Beth )// Removes from the end of the array. n := len(foo) - 1 // index of last elementelement := foo[n] // optionally also grab the last elemenfoo = foo[:n]   // remove the last element JavaScript has Object, Map/Set and WeakMap/WeakSet that can be used as dictionaries and sets. Go has only a simple Map which is more similar to JavaScript Object and hence serves the purpose. Also, note that maps in Go are not ordered. JavaScript 123456789const dict = { key1: 10, key2:  hello ,};const stringMap = { key1:  hello , key2:  world ,};Go 123456789var dict = map[string]interface{}{   key1 : 10,   key2 :  hello ,}var stringMap = map[string]string{   key1 :  hello ,   key2 :  world ,}Mutability: Another major difference between JS and Go is how variable mutations are handled. In JavaScript, every non-primitive variable is passed by reference and there is no way to change that behavior whereas in Go everything except slice, map &amp; channels are passed by value and we can choose to change that by explicitly passing a pointer to a variable instead. Because of this in Go, we have more control over mutability than in JS. Another notable difference is that in Javascript we can prevent reassignment of variables using the const keyword which is not possible in Go. We saw some mutability in action in the above section, let’s see a bit more JavaScript 123456789let foo = { msg:  hello ,};function mutate(arg) { arg. msg =  world ;}mutate(foo);console. log(foo. msg); // prints 'world'Go 1234567891011121314151617type Foo struct {  msg string}var foo = Foo{ hello }var tryMutate = func(arg Foo) {  arg. msg =  world }tryMutate(foo)fmt. Println(foo. msg) // prints 'hello'var mutate = func(arg *Foo) {  arg. msg =  world }mutate(&amp;foo)fmt. Println(foo. msg) // prints 'world'Error handling: The only similarity in terms of error handling between Go and JS is that errors are also just value types. In both languages, you can pass errors as values. Apart from the above error handling are quite different in both. In JavaScript, we can either;  use a try/catch mechanism to catch errors from synchronous functions and asynchronous functions that use async/await handle errors by passing them to callback functions or using promises for asynchronous functions. In Go there is no try/catch mechanism, the only way to handle the error is by returning it as a value from a function or by halting execution with a panic function or using the recover function in a defer block to rescue the execution. This makes error handling quite verbose in Go and you will often see the famous if err != nil statement in Go. JavaScript 123456789101112131415161718192021222324252627282930313233function errorCausingFunction() { throw Error( Oops );}try { errorCausingFunction();} catch (err) { console. error(`Error: ${err}`);} finally { console. log(`Done`);}// prints// Error: Error: Oops// Done// or the async wayfunction asyncFn() { try {  errorCausingFunction();  return Promise. resolve(); } catch (err) {  return Promise. reject(err); }}asyncFn() . then((res) =&gt; console. log(`:)`)) . catch((err) =&gt; console. error(`Error: ${err}`)) . finally((res) =&gt; console. log(`Done`));// prints// Error: Error: Oops// DoneGo 123456789101112131415161718192021222324252627282930var errorCausingFunction = func() error {  return fmt. Errorf( Oops )}err := errorCausingFunction()defer fmt. Println( Done ) // Closest to finally, but executes only at end of the enclosing functionif err != nil {  fmt. Printf( Error: %s\n , err. Error())} else {  fmt. Println( :) )}// prints// Error: Oops// Done// orerr := errorCausingFunction()defer func() { // Closest thing to finally behaviour, but executes only at end of the enclosing function  if err := recover(); err != nil {    fmt. Println( Recovered from err , err) // closest thing to catch behaviour  }  fmt. Println( Done )}()if err != nil {  panic(err)} else {  fmt. Println( :) )}Composition instead of inheritance: In JavaScript, we can use inheritance to extend or share behavior while Go choose composition instead. There is also prototype level inheritance in JavaScript and the possibility of doing composition due to the flexible nature of the language. JavaScript 123456789101112131415161718192021222324class Animal { species; constructor(species) {  this. species = species; } species() {  return this. species; }}class Person extends Animal { name; constructor(name) {  super( human );  this. name = name; } name() {  return this. name; }}var tom = new Person( Tom );console. log(`${tom. name} is a ${tom. species}`); // prints 'Tom is a human'Go 12345678910111213141516171819202122232425262728293031323334type IAnimal interface {\tSpecies() string}type IPerson interface {\tIAnimal // composition of IAnimal interface\tName() string}type Animal struct {\tspecies string}type Person struct {\tAnimal // composition of Animal struct\tname  string}func (p *Person) Name() string {\treturn p. name}func (p *Animal) Species() string {\treturn p. species}func NewPerson(name string) IPerson {\treturn &amp;Person{Animal{ human }, name}}func main() {\tvar tom IPerson = NewPerson( Tom )\tfmt. Printf( %s is a %s\n , tom. Name(), tom. Species()) // prints 'Tom is a human'}Concurrency: Concurrency is one of the most important features of Golang and this is where it really shines. JavaScript technically is single-threaded and hence there is no real native concurrency there. The addition of service workers brings some support for parallelism but is still no match for the power and simplicity of goroutines. Concurrency is not the same as asynchronous or reactive programming for which JavaScript has great support. 1234567891011121314151617181920// Sequentialasync function fetchSequential() { const a = await fetch( http://google. com/ ); console. log(a. status); await a. text(); const b = await fetch( http://twitter. com/ ); console. log(b. status); await b. text();}// Concurrent but not multi threadedasync function fetchConcurrent() { const values = await Promise. all([fetch( http://google. com/ ), fetch( http://twitter. com/ )]); values. forEach(async (resp) =&gt; {  console. log(resp. status);  await resp. text(); });}Go, on the other hand, is fully geared towards concurrency and parallelism. The concepts are built into the language using goroutines and channels. It is also possible to do asynchronous programming in Go but it looks more verbose than the JS equivalent. This means you can write API as sync and use it in an async way using goroutines and Go community generally advocates against writing asynchronous APIs. 12345678910111213141516171819202122232425262728293031323334// Sequentialfunc fetchSequential() {\trespA, _ := http. Get( http://google. com/ )\tdefer respA. Body. Close()\tfmt. Println(respA. Status)\trespB, _ := http. Get( http://twitter. com/ )\tdefer respB. Body. Close()\tfmt. Println(respB. Status)}// Concurrent and multithreadedfunc fetchConcurrent() {\tresChanA := make(chan *http. Response, 0)\tgo func(c chan *http. Response) {\t\tres, _ := http. Get( http://google. com/ )\t\tc &lt;- res\t}(resChanA)\trespA := &lt;-resChanA\tdefer respA. Body. Close()\tfmt. Println(respA. Status)\tresChanB := make(chan *http. Response, 0)\tgo func(c chan *http. Response) {\t\tres, _ := http. Get( http://twitter. com/ )\t\tc &lt;- res\t}(resChanB)\trespB := &lt;-resChanB\tdefer respB. Body. Close()\tfmt. Println(respB. Status)}Compilation: JavaScript is interpreted and not compiled. Some JS engines use JIT compilation but to developers, it doesn’t matter as we do not have to compile JavaScript in order to run it. Transpiling using TypeScript or Babel doesn’t count 😉 Go is compiled and hence offers compile-time type safety and to an extent memory safety. Paradigm: JavaScript is Object-oriented primarily but you can easily write imperative or functional-style code due to the flexibility of the language. The language is quite free form and really doesn’t enforce anything. It is not opinionated and doesn’t provide any tooling out of the box. The developer would need to set up his/her own tooling. Go is imperative primarily, you can do a little bit of OOP and functional but is not as easy to do as in JavaScript. The language is quite strict and opinionated and enforces things like code style and formating. It also provides built-in capabilities for testing, formatting, building and so on. ConclusionSomeone asked me in the comments of the previous part in the series that why should a JS developer choose Go among all the available options. In my opinion, JS is not a perfect language and hence learning few other languages will greatly benefit a JS developer to use JS more pragmatically and would help to cement her/his knowledge of fundamental programming concepts better. There are of course many options out there like Rust, Go, Haskel, Kotlin and so on, but I think Go is a great place to start as its one of the simplest among all the available options and has wide adoption. My second choice would be Kotlin or Rust. References: http://www. pazams. com/Go-for-Javascript-Developers/ https://github. com/miguelmota/golang-for-nodejs-developersIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image photo created using images from norfolkjs (designed by Lookmai Rattana) and juststickers "},{id:50,url:"https://deepu.tech/docker-env-cleanup/",title:"How to clean up your Docker environment",body:"2019/12/06 - Cleanup your Docker setupIf you are using Docker on your PC or Mac, over time it is gonna accumulate a lot of Junk, the majority of which being dangling images and orphan volumes. It could take up a lot of space in your machine. You should clean this up once in a while, thankfully there are some docker commands to help us here and along with some bash magic, we can nicely do this in 3 easy steps. Clean up old containers: Docker has a docker rm command to remove containers, we can use this along with some docker ps filters to remove all containers that are not being used currently. This is perfectly fine as it doesn’t affect anything that is running and if you want to use any of the removed images again, docker will download it for you. So the below command should do the trick. 1docker rm -v $(docker ps -a -q -f status=exited);docker ps -a -q -f status=exited provides a list of container Ids that are in exited status and docker rm -v removes those along with their associated volumes. Run docker rm --help and docker ps --help to see what the flags mean. Note: If you want anything from these volumes, you should back it up before doing this. Clean up dangling volumes: A dangling volume is one that exists and is no longer connected to any containers. There is a similar rm command for volumes as well. We can use this along with docker volume ls command with a filter to remove volumes that are dangling. Below is the command for that. 1docker volume rm $(docker volume ls -q -f dangling=true);docker volume ls -q -f dangling=true returns the volume names that are not connected to any containers and docker volume rm removes them. Run docker volume rm --help and docker volume ls --help to see what the flags mean. Note: If you want anything from these volumes, you should back it up before doing this. Clean up dangling images: Finally, we need to clean up dangling images. Docker images are made up of multiple layers and dangling images are layers that have no relationship to any tagged images. They no longer serve any purpose and consume disk space and hence can be safely removed. We can use the docker image prune -a command from Docker to remove unused images but for some reason, this command was not working for me and hence I had to resort to the image rm command as below. 1docker image rm $(docker images -q -f dangling=true);docker images -q -f dangling=true returns the image names that are not related to any tagged images and docker image rm removes them. Run docker image rm --help and docker images --help to see what the flags mean. Clean up everythingUpdate: Seems like docker provides a native command to clean up everything. 1docker system prune --volumesYou can run this to achieve the same result as below command and a bit more. Thanks to cyberjack for pointing it out. I’ll leave the below for educational purposes. Now we can add these commands as a handy bash function so that we can do this cleanup with a single command like I always do. It frees up a lot of disk space and helps when I’m having some weird cache issues with docker-compose 😉. Add the below function to your ~/. bashrc or ~/. zshrc and reload your terminal or source the file by running . ~/. zshrc or . ~/. bashrc. 12345function dpurgeall(){ docker rm -v $(docker ps -a -q -f status=exited); docker volume rm $(docker volume ls -qf dangling=true); docker image rm $(docker images -qf dangling=true);}Now you can simply run dpurgeall and it will clean up your Docker environment. If there is nothing to clean up the command will just exist so nothing to worry there. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by chuttersnap on Unsplash "},{id:51,url:"https://deepu.tech/golang-for-javascript-developers-part-1/",title:"Golang for JavaScript developers - Part 1",body:"2019/12/04 - If you are a JavaScript developer thinking about learning another programming language, then Golang is a great choice. It is simple, has a lot of momentum, very performant and has some similarities to JavaScript. Edit: Someone asked me in the comments that why should a JS developer choose Go among all the available options. In my opinion, JS is not a perfect language and hence learning few other languages will greatly benefit a JS developer to use JS more pragmatically and would help to cement her/his knowledge of fundamental programming concepts better. There are of course many options out there like Rust, Go, Haskel, Kotlin and so on, but I think Go is a great place to start as its one of the simplest among all the available options and has wide adoption. My second choice would be Kotlin or Rust. This post is not a comparison of the languages or is stating that they are very similar. Its a guide for JavaScript developers to grasp Golang quickly. There are many aspects of Go that are entirely different from JavaScript we will touch upon that as well. Things that are more similarThere are many things in Go which are quite similar to concepts in JavaScript. Most are not the same but similar. let us get them out of our way first. In the first part of this series, we will see how they are similar and note any key differences as well. Functions: The most similar feature in JS and Go are the functions. Similarities:  Functions are first-class citizens.  Functions can be assigned to variables.  Functions can be passed as arguments to other functions and can be returned from functions.  Functions can be nested.  Functions can be curried(partial functions).  Functions can memorize its surrounding context thus creating closures.  Functions can be named or anonymous. Anonymous functions can be immediately invoked(IIFE)JavaScript 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// A normal function with access to `this`function standardFunction(arg1, arg2) { return `${arg1}:${arg2}`;}// A function assigned to a variableconst assignedFunction1 = standardFunction;// An arrow function assigned to a variableconst assignedArrowFunction = (arg1, arg2) =&gt; { return `${arg1}:${arg2}`;};// A higher-order-function that accepts functions as argument and returns a functionfunction functionAsArgumentAndReturn(addFn, arg1, arg2) { const out = addFn(arg1, arg2); // This returns a closure return function (numArg) {  return out + numArg; };}const out = functionAsArgumentAndReturn( (a, b) =&gt; {  return a + b; }, 5, 10)(10);// returns 25// Nested functionsfunction nested() { console. log( outer fn ); function nested2() {  console. log( inner fn );  const arrow = () =&gt; {   console. log( inner arrow );  };  arrow(); } nested2();}nested(); // prints:// outer fn// inner fn// inner arrow// this is a higher-order-function that returns a functionfunction add(x) { // A function is returned here as closure // variable x is obtained from the outer scope of this method and memorized in the closure return (y) =&gt; x + y;}// we are currying the add method to create more variationsvar add10 = add(10);var add20 = add(20);var add30 = add(30);console. log(add10(5)); // 15console. log(add20(5)); // 25console. log(add30(5)); // 35// An anonymous function invoked immediately(IIFE)(function () { console. log( anonymous fn );})();// prints: anonymous fnGo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// A normal function, this cannot be nestedfunc standardFunction(arg1 string, arg2 string) string {\treturn fmt. Sprintf( %s:%s , arg1, arg2)}func main() {\t// A function assigned to a variable\tvar assignedFunction1 = standardFunction\t// An anonymous function assigned to a variable and nested\tvar assignedFunction2 = func(arg1 string, arg2 string) string {\t\treturn fmt. Sprintf( %s:%s , arg1, arg2)\t}\t// A higher-order-function that accepts functions as argument and returns a function\tvar functionAsArgumentAndReturn = func(addFn func(int, int) int, arg1 int, arg2 int) func(int) int {\t\tvar out = addFn(arg1, arg2)\t\t// This returns a closure\t\treturn func(numArg int) int {\t\t\treturn out + numArg\t\t}\t}\tvar out = functionAsArgumentAndReturn(\t\tfunc(a, b int) int {\t\t\treturn a + b\t\t},\t\t5,\t\t10,\t)(10)\tfmt. Println(out) // prints 25\t// Nested anonymous functions\tvar nested = func() {\t\tfmt. Println( outer fn )\t\tvar nested2 = func() {\t\t\tfmt. Println( inner fn )\t\t\tvar nested3 = func() {\t\t\t\tfmt. Println( inner arrow )\t\t\t}\t\t\tnested3()\t\t}\t\tnested2()\t}\tnested() // prints:\t// outer fn\t// inner fn\t// inner arrow\t// this is a higher-order-function that returns a function\tvar add = func(x int) func(y int) int {\t\t// A function is returned here as closure\t\t// variable x is obtained from the outer scope of this method and memorized in the closure\t\treturn func(y int) int {\t\t\treturn x + y\t\t}\t}\t// we are currying the add method to create more variations\tvar add10 = add(10)\tvar add20 = add(20)\tvar add30 = add(30)\tfmt. Println(add10(5)) // 15\tfmt. Println(add20(5)) // 25\tfmt. Println(add30(5)) // 35\t// An anonymous function invoked immediately(IIFE)\t(func() {\t\tfmt. Println( anonymous fn )\t})()\t// prints: anonymous fn\tassignedFunction1( a ,  b )\tassignedFunction2( a ,  b )}Differences:  JavaScript Functions have two forms; regular functions, and arrow functions whereas in Go there is normal functions and interface functions. Normal Go functions do not have a this and hence are more similar to arrow functions whereas interface functions have something similar to a this and hence closer to normal functions in JavaScript. Go doesn’t have the concept of a global this. JavaScript 1234567891011121314151617181920function normalFnOutsideClass() { console. log(`I still can access global this: ${this}`);}const arrowFnOutsideClass = () =&gt; { console. log(`I don't have any this`);};class SomeClass { name =  Foo ; normalFnInsideClass = function () {  console. log(`I can access the callee as this: ${this. name}`); }; arrowFnInsideClass = () =&gt; {  console. log(`I can access the class reference as this: ${this. name}`); };}new SomeClass(). normalFnInsideClass();new SomeClass(). arrowFnInsideClass();Go 12345678910111213141516171819type SomeStruct struct {\tname string}func (this *SomeStruct) normalFnInsideStruct() {\t// you can name the variable this or anything else\tfmt. Printf( I can access the struct reference as this\n: %s , this. name)}func main() {\tvar normalFnOutsideStruct = func() {\t\tfmt. Println( I can access variables in my scope )\t}\tnormalFnOutsideStruct()\tvar structVal = SomeStruct{ Foo }\tstructVal. normalFnInsideStruct()} JavaScript functions are the same as any other value type and hence can even hold additional attributes which is not possible in Go.  Go functions can have implicit named returns.  Only anonymous functions can be nested in Go.  Go functions can return multiple values, whereas in JavaScript you can return only one value. However, in JS you can work around that by using destructuring so you can do similar looking functions in bothJavaScript 123456function holdMyBeer() { return [ John , 2];}let [a, b] = holdMyBeer();console. log(`Hey ${a}, hold my ${b} beer\n`);Go 12345678func holdMyBeer() (string, int64) {\treturn  John , 2}func main() {\ta, b := holdMyBeer()\tfmt. Printf( Hey %s, hold my %d beer\n , a, b)}Scope: The scope is the context in which a variable is valid, this decides where a variable can be used and both JS and Go has many similarities here Similarities:  Both have function Scope and Functions can memorize their surrounding scope.  Both have block scope.  Both have a global scope. Differences:  Go doesn’t have the concept of this which is a tricky concept in JavaScript. IMO this makes things much simpler in Go.  Variables in the same scope cannot be re-declared in Go. Go var is closer to let keyword in JS. Flow control: Flow control in Golang is quite similar but simpler than JavaScript in many aspects. Similarities:  for loops are very similar in both.  while loops are very similar, though Go uses the same for keyword.  forEach is also similar in functionality but the syntax is quite different.  You can break/continue from a loop. You can use labels to do so as well.  if/else syntax is quite similar, Go version is a bit more powerfulJavaScript 123456789101112131415161718192021222324252627282930313233343536373839// For loopfor (let i = 0; i &lt; 10; i++) { console. log(i);}// While looplet i = 0;while (i &lt; 10) { console. log(i); i++;}// Do whilelet j = 0;do { j += 1; console. log(j);} while (j &lt; 5);// ForEach loop[ John ,  Sam ,  Ram ,  Sabi ,  Deepu ]. forEach((v, i) =&gt; { console. log(`${v} at index ${i}`);});// for of loopfor (let i of [ John ,  Sam ,  Ram ,  Sabi ,  Deepu ]) { console. log(i);}// For in loopconst obj = { a:  aVal , b:  bVal ,};for (let i in obj) { console. log(obj[i]);}Go 123456789101112131415161718192021222324252627282930313233343536373839func main() {\t// For loop\tfor i := 0; i &lt; 10; i++ {\t\tfmt. Println(i)\t}\t// While loop\ti := 0\tfor i &lt; 10 {\t\tfmt. Println(i)\t\ti++\t}\t// Do while\tj := 0\tfor {\t\tj += 1\t\tfmt. Println(j)\t\tif j == 5 {\t\t\tbreak\t\t}\t}\t// ForEach and for of loop\tfor i, v := range []string{ John ,  Sam ,  Ram ,  Sabi ,  Deepu } {\t\tfmt. Printf( %v at index %d\n , v, i)\t}\t// For in loop\tvar obj = map[string]string{\t\t a :  aVal ,\t\t b :  bVal ,\t}\tfor i, v := range obj {\t\tfmt. Printf( %v at index %s\n , v, i)\t}}Differences:  There is no ternary operator in Go.  switch statement syntax is similar but Go defaults to break and JS defaults to fall through. In Go, you can use the fallthrough keyword for that functionality while in JS, we have the break keyword.  JS has many more ways of iterations, like while, forEach, for in &amp; for of loops and so on which are not available in Go though most of them can be achieved using the for syntax.  if/else can have an init assignment in Go. In the below code the assignment for val has scope only within the if and else blocks and not outside of it. This is not possible in JS. Go 12345if val := getVal(); val &lt; 10 {  return val} else {  return val + 1}Memory management: Memory management is also quite similar except for details in both JS and Go. Similarities:  Both are garbage collected at runtime.  Both have heap and stack memory which means the same in both. Differences:  Go has pointers that are exposed to users while their memory management is abstracted away whereas in JavaScript pointers are abstracted away completely and you only work with values and references.  Go uses a concurrent tricolor mark-and-sweep algorithm with a focus on latency whereas JS engines normally implement different algorithms with Mark-Sweep being a very popular choice. V8 engine, for example, uses both Mark-Sweep and a Scavenge algorithm. Misc:  Commenting is same in both, with // and /* */ Both JS and Go supports importing other modules, though the behavior is not the same SetTimeout is similar in both. setTimeout(somefunction, 3*1000) vs time. AfterFunc(3*time. Second, somefunction).  Both have a spread operator console. log(. . . array) vs fmt. Println(array. . . ). Go spread works only on interface arrays/slices though.  Both have rest operator for method arguments . . . nums vs nums . . . int. ConclusionIn this part, we saw concepts that are similar in both languages. In the next part of the series, we will see things that are more different between JS and Go. There are more things in the next part, that are different, than this, but please also note that some differences are quite subtle so it would be easy to digest for a JavaScript developer. In the next chapter we will see:  Types &amp; Variables Error handling Mutability Composition instead of inheritance Concurrency Compilation ParadigmReferences: http://www. pazams. com/Go-for-Javascript-Developers/ https://github. com/miguelmota/golang-for-nodejs-developersIf you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image photo created using images from norfolkjs (designed by Lookmai Rattana) and juststickers "},{id:52,url:"https://deepu.tech/reflections-on-javascript/",title:"My love-hate relationship with JavaScript",body:"2019/11/21 - There are three types of programmers, the ones who love JavaScript, the ones who hate JavaScript and the ones who do both. JavaScript is the second language(First was C/C++) I learned when I was trying to run my Wordpress blog. It was even before I started my career. When I started my engineering career I started as a Java Web app developer, which meant I had the chance to work on JavaScript as well for the front-end part. I was pretty good at JS/HTML/CSS and soon I was doing a lot of front-end focused Java Web apps. I also learned JQuery and fell in love with it. During the initial years of my career, JavaScript was undoubtedly the language I loved the most as I found it insanely flexible and easy, especially when I wanted to hack something together fast, even though I was doing an equal amount of coding in Java as well. My former immature self even used to believe that JavaScript was the best programming language in the world and I used to vehemently debate anyone who thought JavaScript wasn’t good, I mean I did have some good reasons to think so. Fast forward to now and I think I know better and in my attempts at being more pragmatic, I started looking at languages and frameworks more objectively and without bias. Now I wouldn’t say JavaScript is the best language out there, but its a very important one, I know its flaws and there are things I dislike in the JS ecosystem which now I’m mature enough to admit. Don’t get me wrong, I still love JavaScript(TypeScript even more) and I have seen the rise and fall of frameworks from JQuery to current MVVM frameworks and worked with most of them. JavaScript is one of the most loved and most hated language at the same time. You may notice that many of the things I like about JavaScript are the same I dislike as well and that’s why the title. So after more than 10 years of working with JavaScript and its huge ecosystem here is what I think about the language. Please note that a lot of them are personal preference based opinions and hence might sound a bit biased. What I like about JavaScriptFirst, let us talk about things that I love in JavaScript Beginner friendly but also powerful: JavaScript is one of the easiest languages for beginners. Regardless of its quirks, it’s easy to get started. You don’t even need to install or set up anything. If you have a web browser on your computer that is all you need to write JavaScript. There is also an infinite amount of help available on the internet. The basic syntax is quite easy and the basic concepts are easy to follow as well. This doesn’t mean it is a simple language, we will talk about that later. JavaScript is also a really powerful language as you can get almost anything done with JavaScript like building a web page, a server app, a mobile app, a robot and so on (doesn’t mean you should 😜). There is no other language that I have seen which is as versatile as JavaScript. But remember learning JavaScript is easy but becoming a good JavaScript developer is quite hard. Dynamic &amp; Extremely flexible: JavaScript is the most dynamic language that I have used, there are things that you can do it JavaScript which is not even thinkable in many other languages. You can get away with a lot in JavaScript as its a very forgiving language. Changing the type of variables at runtime? no problem, add variables and methods to a class you have no control over? no problem, write code that generates code? no problem. The list just goes on. This kind of dynamic behavior is really useful for some use-cases, especially scripting or a templating engine for example. But it’s not without its costs. Flexibility is the biggest strength and biggest weakness of JavaScript, it is extremely handy when it comes to scripting and stuff but means makes maintenance harder in larger codebases, we will see about that in the dislike section. I spend a good amount of time in my career creating prototypes and PoCs and the dynamic nature and flexibility of JavaScript made that productive and easy, but I would never recommend those for real applications that need to be maintained. For example, you can do the below in JavaScript to build dynamic functions 1234567891011121314const functions = {};for (let i = 0; i &lt; 10; i++) { functions[`myAwesomeFunc${i}`] = new Function( fnName , `console. log('Hello world from ' + fnName + ' fn created by index ${i}');`);}Object. values(functions). forEach((fn) =&gt; { fn(fn. name);});// prints// Hello world from anonymous fn created by index 0// . . . // Hello world from anonymous fn created by index 9Multiparadigm: JavaScript started as an imperative scripting language and later added features to make OOP possible and due to a lot of features it has you can use it as a functional programming language as well. I like this in a language as you can use the best of all paradigms to get your work done efficiently. Functions as first-class citizens: Functions in JavaScript are first-class citizens and they don’t differ from any other type of objects in JavaScript. You can pass them around, create them at runtime, change them, store them and so on. You can even add attributes to a function. 1234567function foo(msg) { console. log(`Hello world! ${msg}`);}foo. bar =  Yo ;foo(foo. bar); // prints 'Hello world! Yo'Useful syntax sugars(personal preference): JavaScript provides a lot of useful syntax sugars like async/await, spread/rest operators, destructuring, ternary operator and so on and I really like them as they make code less verbose for trained eyes. Of course, if you are very new to JS they might seem a bit confusing. Metaprogramming: JavaScript has great support for Metaprogramming. It provides the Proxy and Reflect objects that allow you to intercept and define custom behavior to existing language operators. Definitely an advanced feature which has its own use-cases. Less verbose and clean syntax(personal preference): I might be a bit biased here as JavaScript and Java are the languages I have worked with most and so when it comes to syntax I might be unconsciously finding them nicer. It definitely is possible to write unreadable code in JavaScript but at the same time you can write beautiful expressive code as well and I find the JS syntax more readable than many other languages. Can run anywhere: Technically JavaScript can run anywhere. It is undoubtedly the biggest programming platform in the world, especially due to the internet, as JavaScript is the language of the web. You can run JS in a browser, mobile devices, server-side, Desktop apps, OS, IoT, robots, virtual reality, smartwatches, from other languages like Java and so on. This is an interesting article from Anil Dash about this topic. Biggest community: JavaScript has the biggest community out there, its the most popular programming language after all. The NPM repository has more packages than most other languages combined and you will find help easily for anything related to JS on the web and there is a huge ecosystem around JavaScript making it really easy to work with. Whatever need you have, you can be sure there will be a JavaScript library or tool for that.  As long as web browsers and internet is around JavaScript will be around: Whenever people say Java and JavaScript are like dinosaurs(old, outdated and bulky) I try to correct them. IMO, JS and Java are like cockroaches they can survive anything and I’m pretty sure JavaScript will be around for the foreseeable future unless there is a huge revolution in the internet industry making way to something else. So your skills in JS is gonna be pretty relevant and hence is an important skill to have. NodeJS: One of the reasons the JavaScript community grew is also because of NodeJS, It paved the way for JS to be considered outside of the web browser and boy did that explode. I like NodeJS as it lets anyone build and publish reusable packages to the community without having to spend too much effort. Of course, there are issues like fragmentation and bloat to address but NodeJS still is an important tool in a programmer’s arsenal. Typescript: You might argue TypeScript is its own language, but technically its a syntax superset of JavaScript and hence I would rather place it here. TypeScript addresses a lot of common issues in JavaScript like support for static typing, scalability and so on. So this is definitely something I would put in the like column. I wish every JavaScript runtime had native support for TypeScript(like Deno for example) or that JS evolves into TypeScript(that would be super cool). What I don’t like about JavaScriptNow let’s talk about things that I don’t like in JavaScript language and ecosystem. Fragmentation (Browser implementations, version compatibility): For me the biggest issue for JavaScript is fragmentation. The JS model is that the end-user can choose the implementation, which means the programmer has very little control over what implementation her/his code will run against. There are two major parts at play here when it comes to implementation; Vendor: There are so many different JS engines with slightly different implementations making the life of programmers hell. For example, there is the V8 engine used by Chrome, NodeJs, Opera and so on and SpiderMonkey from Mozilla, JavaScriptCore from apple and many more. The problem is the ECMAScript standard for JS doesn’t have any reference implementation and vendors slightly change implementation details to fit their needs making the same code behave differently in different engines. remember Internet explorer? A major share of the front-end code written in the last two decades is just for browser compatibility, as the huge portion of the JQuery codebase which was to make it work in Internet explorer. While I’m glad that the IE browser is finally EOL, there are still subtle bugs arising from these differing implementations every now and then for someone building JS apps. Version: Another reason for fragmentation is the ECMAScript version, again vendors go ahead and implement versions as and when they like making it impossible for developers to rely on any particular version as they wouldn’t know if end-user has a browser that supports this version. This leads way to an unwanted middleman like Babel transpiling your code to the common denominator, mostly to ES5 adding complexity and overhead. One of the biggest issues even present today because of these factors is the JS module system, everyone uses a module system(requireJS, commonJS or ES modules) but still, there is no consensus on what should be the standard implementation and its quite frustrating. This is probably why JS is the only language with such dedicated websites like caniuse. com I wish there was a single-engine managed by the community and used by all the browsers and runtimes thus making fragmentation less of an issue. Beauracracy: JavaScript being a huge community comes with its own Beauracracy and process layer similar to Java, there are different governing bodies like ECMA International, ISO, JS Foundation, W3C and so on which has stakes in the future of JavaScript, then there are different browser vendors like Google, Mozilla, and Apple with their own agenda, all this makes language evolution slow, messy and painful. Language quirks: Sometimes I wonder if JavaScript was designed by someone during an acid(LSD) + Ecstasy trip as there are quirks in the language that is mind-numbing. I don’t think you will find so many quirks in any other languages and hence it gives criticizers of JavaScript a field day. There is a whole repo dedicated to documenting this https://github. com/denysdovhan/wtfjs Here is a sample, don’t even try to understand what happens here. 12console. log((![] + [])[+[]] + (![] + [])[+!+[]] + ([![]] + [][[]])[+!+[] + [+[]]] + (![] + [])[!+[] + !+[]]);// prints 'fail'Npm hell: NodeJS was like a blessing to JavaScript community and it brought NPM along which really has exploded and now it is kind of annoyance that JavaScript developers have learned to live with. Have you ever tried to find the number of files in a node_modules folder? NPM is a good package manager and has some pretty great features like npm link for example but its nested dependency structure along with fluid versioning makes it a recipe for disaster and countless hours of painful debugging sessions. Also, there is the issue of a monolithic registry holding so many packages used by so many applications. Remember leftpad? Error prone (a side effect of flexibility): With JavaScript, it is too easy to shoot in the foot. Well, JavaScript will give you 100 different types of loaded guns, it will hold your hand while you point your gun and will pull the trigger if you hesitate and once you have shot at your foot, it will cut off your leg and make you eat it. I didn’t want to go all cannibalistic here but that’s how it is with JavaScript. A lot of it has to do with the fact that JavaScript was never designed for the scale that it has today. It was a simple dynamic scripting language. Since JavaScript is too dynamic and flexible it lets you do all sorts of stuff that many other languages will not allow and combine that with the huge list of quirks, bugs are just waiting to happen. Today the situation is much better with newer versions and with many tools like ESList, VSCode, TypeScript and so on that help you a lot to avoid common mistakes, but even with all that it really takes experience and hard work to write large JavaScript programs without subtle bugs. In my career, most of the debugging sessions that I have done would be in JavaScript. Yet another framework syndrome: There is also the phenomena that is unique to JavaScript, its called Yet another framework syndrome, new frameworks and libraries are invented on a daily basis, almost, and the churn is so great that if you take a break of one year from JS world and comeback you won’t be able to recognize anything and will find yourself learning some new framework. This means teams maintaining JavaScript applications are constantly spending time migrating to newer frameworks from obsolete ones and so on. I had to spend a lot of time migrating from JQuery to AngularJS, AngularJS to Angular, Angular to React and so on in my career. The churn rate in Java, for example, is extremely low compared to this. The JS community also seems to suffer from not invented here syndrome much more than other language communities, you will find at least a dozen options for everything here. Complexity: As I said earlier, JavaScript is very beginner-friendly and easy to learn but it is not a simple language at its current form. It has evolved a lot and along with all the simplicity on its cover has quite a lot of complex features underneath and it keeps on growing, and due to its legacy and dynamic nature it has too many ways to do the same thing, which I dislike in any language, and has a complex ecosystem that one must learn to use JavaScript at scale. You would have to learn stuff like Webpack, NodeJS, NPM, Babel, ESLint and so on to be productive. It is also very easy to write complex unreadable code in JavaScript using callbacks and stuff, generally referred to as callback hell! Add to this the dynamic nature, legacy quirks and the complexity keeps on increasing. Scalability: JavaScript by itself is not scalable at all, you will be productive when the codebase is small but as it grows the issues starts to appear, due to the lack of a type system, large codebases become a nightmare to maintain unless you are using something like TypeScript on top. Even with that large JavaScript codebases are much more difficult to traverse and maintain compared to other languages, I have experience of this from JHipster for example. Soon you will find yourself adding build tools, linters, transpilers and so on to ease maintenance. NitpicksWell, when it comes to JavaScript you either love it, hate it or both, there are no real nitpicks at least for me. ConclusionIf you search on the internet for opinions on JavaScript, you will find tons and tons of content, some praising it, some bashing it, and some objective. A lot can be said about JavaScript and its community. For most its a love-hate relationship, some are brave enough to admit that. If you absolutely hate JavaScript then either you haven’t worked with it a lot or you are holding some prejudice against it. Try it, its a fun language(at least it will keep you awake a lot 😜), it has its purpose and like it or not its the language of the modern web, and it does a pretty good job there. If you think you can be more productive on the web using any other language, then maybe you should try building a large website using that language, then learn JS and try the same. IMO JS is not going anywhere and if anything it is only getting more and more adoption, so it would be foolish not to know the most popular language. Every programmer should learn JavaScript, you never know when it would be handy. If you absolutely love JavaScript and use JavaScript to everything, then maybe you should also learn few other languages like Java, Go or Rust and you would see why JavaScript is not ideal for many use-cases(It can, of course, do it, any Turing complete language can, that doesn’t mean you should do it) The key is not knowing how to use JavaScript, it is knowing when to use JavaScript and when not. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Made with imgflip "},{id:53,url:"https://deepu.tech/functional-programming-in-rust/",title:"Easy functional programming techniques in Rust for everyone",body:"2019/11/14 - There is a lot of hype around functional programming(FP) and a lot of cool kids are doing it but it is not a silver bullet. Like other programming paradigms/styles, functional programming also has its pros and cons and one may prefer one paradigm over the other. If you are a Rust developer and wants to venture into functional programming, do not worry, you don’t have to learn functional programming oriented languages like Haskell or Clojure(or even Scala or JavaScript though they are not pure functional programming languages) since Rust has you covered and this post is for you. If you are looking for functional programming in Java, Golang or TypeScript check other posts in the series. I’m not gonna dive into all functional programming concepts in detail, instead, I’m gonna focus on things that you can do in Rust that are in line with functional programming concepts. I’m also not gonna discuss the pros and cons of functional programming in general. Please note that some introductions in this post are repeated from my other posts in the series for your ease of reading. What is functional programming?: As per Wikipedia,  Functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. Hence in functional programming, there are two very important rules  No Data mutations: It means a data object should not be changed after it is created.  No implicit state: Hidden/Implicit state should be avoided. In functional programming state is not eliminated, instead, its made visible and explicitThis means:  No side effects: A function or operation should not change any state outside of its functional scope. I. e, A function should only return a value to the invoker and should not affect any external state. This means programs are easier to understand.  Pure functions only: Functional code is idempotent. A function should return values only based on the arguments passed and should not affect(side-effect) or depend on the global state. Such functions always produce the same result for the same arguments. Apart from these there are functional programming concepts below that can be applied in Rust, we will touch upon these further down.  Higher-order-functions Closures Currying Recursion Lazy evaluations Referential transparencyUsing functional programming doesn’t mean it’s all or nothing, you can always use functional programming concepts to complement Object-oriented or imperative concepts in Rust. The benefits of functional programming can be utilized whenever possible regardless of the paradigm or language you use. And that is exactly what we are going to see. Functional programming in Rust: Rust is primarily geared towards the procedural/imperative style of programming but it also lets you do the functional and object-oriented style of programming as well. And that is my favorite kind of mix. So let us see how we can apply some of the functional programming concepts above in Rust using the language features. First-class and higher-order functions: First-class functions(function as a first-class citizen) mean you can assign functions to variables, pass a function as an argument to another function or return a function from another. Functions in Rust are a bit more complex than other languages, it’s not as straightforward as in Go or JavaScript. There are different kinds of functions and two different ways of writing them. The first one is a function that cannot memoize its outer context and the second one is closures which can memoize its outer context. Hence concepts like currying and higher-order functions are possible in Rust but may not be as easy to wrap your head around as in other languages. Also, functions that accept a closure can accept a pointer to a function depending on the context. In many places, Rust functions and closures can be interchangeable. It would have been nicer if functions were simple and we could do all the below without having to rely on closures. But Rust chose these compromises for better memory safety and performance. A function can be considered as a higher-order function only if it takes one or more functions as parameters or if it returns another function as a result. In Rust, this is quite easy to do with closures, it might look a bit verbose but if you are familiar with Rust then you should be fine. 123456789101112131415161718192021222324fn main() {  let list = vec![    String::from( Orange ),    String::from( Apple ),    String::from( Banana ),    String::from( Grape ),  ];  // we are passing the array and a closure as arguments to map_for_each method.   let out = map_for_each(list, |it: &amp;String| -&gt; usize {    return it. len();  });  println!( {:?} , out); // [6, 5, 6, 5]}// The higher-order-function takes an array and a closure as argumentsfn map_for_each(list: Vec&lt;String&gt;, fun: fn(&amp;String) -&gt; usize) -&gt; Vec&lt;usize&gt; {  let mut new_array: Vec&lt;usize&gt; = Vec::new();  for it in list. iter() {    // We are executing the closure passed    new_array. push(fun(it));  }  return new_array;}There are also more complex versions that you can write with generics like below for an example 12345678910111213141516171819fn main() {  let list = vec![2, 5, 8, 10];  // we are passing the array and a closure as arguments to map_for_each method.   let out = map_for_each(list, |it: &amp;usize| -&gt; usize {    return it * it;  });  println!( {:?} , out); // [4, 25, 64, 100]}// The higher-order-function takes an array and a closure as arguments, but uses generic typesfn map_for_each&lt;A, B&gt;(list: Vec&lt;A&gt;, fun: fn(&amp;A) -&gt; B) -&gt; Vec&lt;B&gt; {  let mut new_array: Vec&lt;B&gt; = Vec::new();  for it in list. iter() {    // We are executing the closure passed    new_array. push(fun(it));  }  return new_array;}But then we could also simply do it this way using built-in functional methods like map, fold(reduce) and so on which is much less verbose. Rust provides a lot of useful functional style methods for working on collections like map, fold, for_each, filter, and so on. 12345678fn main() {  let list = [ Orange ,  Apple ,  Banana ,  Grape ];  // we are passing a closure as arguments to the built-in map method.   let out: Vec&lt;usize&gt; = list. iter(). map(|x| x. len()). collect();  println!( {:?} , out); // [6, 5, 6, 5]}Closures in Rust can memorize and mutate their outer context but due to the concept of ownership in Rust, you cannot have multiple closures mutating the same variables in the outer context. Currying is also possible in Rust but again due to ownership and lifetime concepts, it might feel a bit more verbose. 1234567891011121314151617fn main() {  // this is a higher-order-function that returns a closure  fn add(x: usize) -&gt; impl Fn(usize) -&gt; usize {    // A closure is returned here    // variable x is obtained from the outer scope of this method and memorized in the closure by moving ownership    return move |y| -&gt; usize { x + y };  };  // we are currying the add method to create more variations  let add10 = add(10);  let add20 = add(20);  let add30 = add(30);  println!( {} , add10(5)); // 15  println!( {} , add20(5)); // 25  println!( {} , add30(5)); // 35}Pure functions: As we saw already a pure function should return values only based on the arguments passed and should not affect or depend on the global state. It is possible to do this in Rust easily. Take the below, this is a pure function. It will always return the same output for the given input and its behavior is highly predictable. We can safely cache the method if needed. 123fn sum(a: usize, b: usize) -&gt; usize {  return a + b;}But since Rust variables are immutable by default, unless specified a function cannot mutate any variables passed to it and cannot capture any variable in its context. So if we try to affect external state like below the compiler will complain “can’t capture the dynamic environment in an fn item” 1234567891011use std::collections::HashMap;fn main() {  let mut holder = HashMap::new();  fn sum(a: usize, b: usize) -&gt; usize {    let c = a + b;    holder. insert(String::from(format!( ${a}+${b} , a = a, b = b)), c);    return c;  }}In Rust, to capture external state, we would have to use closures, so we can rewrite the above as 12345678910111213use std::collections::HashMap;fn main() {  let mut holder = HashMap::new();  let sum = |a: usize, b: usize| -&gt; usize {    let c = a + b;    holder. insert(String::from(format!( ${a}+${b} , a = a, b = b)), c);    return c;  };  println!( {} , sum(10, 20));}But the compilation will still fail with the message “cannot borrow sum as mutable, as it is not declared as mutable”. So to do external state mutation, we would have to explicitly specify the function as mutable like let mut sum = . . . So Rust will help you keep your functions pure and simple by default. Of course, that doesn’t mean you can avoid side effects that don’t involve variable mutations, for those you have to take care of it yourself. Recursion: Functional programming favors recursion over looping. Let us see an example for calculating the factorial of a number. I did some benchmarks and added the ns/op results inline as a comment In traditional iterative approach: 12345678910111213fn main() {   // Average 8. 5858 ns/op  fn factorial(mut num: usize) -&gt; usize {    let mut result = 1;    while num &gt; 0 {      result *= num;      num = num - 1;    }    return result;  }  println!( {} , factorial(20)); // 2432902008176640000}The same can be done using recursion as below which is favored in functional programming – But recursion is not the solution always, for some cases a simple loop is more readable in my personal opinion. 1234567891011fn main() {   // Average 8. 6150 ns/op  fn factorial(num: usize) -&gt; usize {    return match num {      0 =&gt; 1,      _ =&gt; num * factorial(num - 1),    };  }  println!( {} , factorial(20)); // 2432902008176640000}The downside of the recursive approach, in general, is that it might result in stack overflow errors since every function call needs to be saved as a frame to the stack. To avoid this tail recursion is preferred, especially when the recursion is done too many times. In tail recursion, the recursive call is the last thing executed by the function and hence the functions stack frame need not be saved by the compiler. Most compilers can optimize the tail recursion code the same way iterative code is optimized hence avoiding the performance penalty. In Rust this mostly is not an issue due to Rust’s zero-cost abstractions as both the imperative and recursive code will be compiled down to same assembly in most cases. Rust also does tail-call optimizations but it’s not always guaranteed. The same use-case with the tail call will look like this. 123456789101112131415fn main() {   // Average 8. 6869 ns/op  fn factorial(num: usize) -&gt; usize {    factorial_inner(1, num)  }  fn factorial_inner(acc: usize, val: usize) -&gt; usize {    return match val {      1 =&gt; acc,      _ =&gt; factorial_tail_inner(acc * val, val - 1),    };  }  println!( {} , factorial(20)); // 2432902008176640000}For factorials, there is an even better way using iterators which performs the best 12345678fn main() {   // Average 6. 6387 ns/op  fn factorial(num: usize) -&gt; usize {    (1. . num). fold(1, |n1, n2| n1 * n2)  }  println!( {} , factorial(20)); // 2432902008176640000}Consider using recursion when writing Rust code for readability and immutability and due to Rust’s zero-cost abstractions we don’t have to worry about performance. Lazy evaluation: Lazy evaluation or non-strict evaluation is the process of delaying the evaluation of an expression until it is needed. In general, Rust does strict/eager evaluations. We can utilize higher-order functions, closures, and memoization techniques to do lazy evaluations. Take this example where Rust eagerly evaluates everything. 123456789101112131415161718192021fn main() {  fn add(x: usize) -&gt; usize {    println!( executing add ); // this is printed since the functions are evaluated first    return x + x;  }  fn multiply(x: usize) -&gt; usize {    println!( executing multiply ); // this is printed since the functions are evaluated first    return x * x;  }  fn add_or_multiply(add: bool, on_add: usize, on_multiply: usize) -&gt; usize {    if add {      on_add    } else {      on_multiply    }  }  println!( {} , add_or_multiply(true, add(4), multiply(4))); // 8  println!( {} , add_or_multiply(false, add(4), multiply(4))); // 16}This will produce the below output and we can see that both functions are executed always 123456executing addexecuting multiply8executing addexecuting multiply16We can use higher-order-functions to rewrite this into a lazily evaluated version 123456789101112131415161718192021222324fn main() {  fn add(x: usize) -&gt; usize {    println!( executing add ); // this is printed since the functions are evaluated first    return x + x;  }  fn multiply(x: usize) -&gt; usize {    println!( executing multiply ); // this is printed since the functions are evaluated first    return x * x;  }  type FnType = fn(t: usize) -&gt; usize;  // This is now a higher-order-function hence evaluation of the functions are delayed in if-else  fn add_or_multiply(add: bool, on_add: FnType, on_multiply: FnType, t: usize) -&gt; usize {    if add {      on_add(t)    } else {      on_multiply(t)    }  }  println!( {} , add_or_multiply(true, add, multiply, 4)); // 8  println!( {} , add_or_multiply(false, add, multiply, 4)); // 16}This outputs the below and we can see that only required functions were executed 1234executing add8executing multiply16You can also use memoization/caching techniques to avoid unwanted evaluations in pure and referentially transparent functions like below 12345678910111213141516171819202122232425262728293031323334353637383940414243use std::collections::HashMap;fn main() {  let mut cached_added = HashMap::new();  let mut add = |x: usize| -&gt; usize {    return match cached_added. get(&amp;x) {      Some(&amp;val) =&gt; val,      _ =&gt; {        println!( {} ,  executing add );        let out = x + x;        cached_added. insert(x, out);        out      }    };  };  let mut cached_multiplied = HashMap::new();  let mut multiply = |x: usize| -&gt; usize {    return match cached_multiplied. get(&amp;x) {      Some(&amp;val) =&gt; val,      _ =&gt; {        println!( executing multiply );        let out = x * x;        cached_multiplied. insert(x, out);        out      }    };  };  fn add_or_multiply(add: bool, on_add: usize, on_multiply: usize) -&gt; usize {    if add {      on_add    } else {      on_multiply    }  }  println!( {} , add_or_multiply(true, add(4), multiply(4))); // 8  println!( {} , add_or_multiply(false, add(4), multiply(4))); // 16}This outputs the below and we can see that functions were executed only once for the same values. 1234executing addexecuting multiply816These may not look that elegant especially to seasoned Rust programmers. Fortunately, most of the functional APIs, like the iterators, provided by Rust do lazy evaluations and there are libraries like rust-lazy and Thunk which can be used to make functions lazy. Also, Rust provides some advanced types with which lazy evaluations can be implemented. Doing Lazy evaluations in Rust might not be worth the code complexity some of the times, but if the functions in question are heavy in terms of processing then it is worth it to lazy evaluate them. Type system: Rust is a strong statically typed language and also has great type inference. There are also advanced concepts like type aliasing and so on. Referential transparency: From Wikipedia:  Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent. Rust has great ways to ensure referential transparency, variables in Rust are immutable by default and even reference passing is immutable by default. So you would have to explicitly mark variables or references as mutable to do so. So in Rust, it is quite easy to avoid mutations. For example, the below will produce an error 12345fn main() {  let list = [ Apple ,  Orange ,  Banana ,  Grape ];  list = [ John ,  Raju ,  Sabi ,  Vicky ];}And so does all of the below 12345678910111213141516fn main() {  let list = vec![    String::from( Orange ),    String::from( Apple ),    String::from( Banana ),    String::from( Grape ),  ];  list. push(String::from( Strawberry )); // This will fail as the reference is immutable  fn mutating_fn(val: String) {    val. push('!'); // this will fail unless the argument is marked mutable reference or value passed is marked mutable reference  }  mutating_fn(String::from( Strawberry )); // this will fail if the reference is not passed as mutable}To compile these, we would have to riddle it with mut keywords 1234567891011121314151617fn main() {  let mut list = vec![    String::from( Orange ),    String::from( Apple ),    String::from( Banana ),    String::from( Grape ),  ];  list. push(String::from( Strawberry )); // This will work as the reference is mutable  fn mutating_fn(val: &amp;mut String) {    val. push('!'); // this will work as the argument is marked as a mutable reference  }  mutating_fn(&amp;mut String::from( Strawberry )); // this will work as the reference is passed as mutable}There are even more advanced concepts in Rust when it comes to data mutation and all that makes it easier to write immutable code. Data structures: When using functional programming techniques it is encouraged to use data types such as Stacks, Maps, and Queues as they also have functional implementations. Hence Hashmaps are better than arrays or hash sets as data stores in functional programming. Rust provides such data types and is hence conforms to the functional specifications regarding data structures. Conclusion: This is just an introduction for those who are trying to apply some functional programming techniques in Rust. There are a lot more that can be done in Rust. As I said earlier functional programming is not a silver bullet but it offers a lot of useful techniques for more understandable, maintainable, and testable code. It can co-exist perfectly well with imperative and object-oriented programming styles. In fact, we all should be using the best of everything to solve the problem at hand instead of getting too obsessed with a single methodology. I hope you find this useful. If you have any questions or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:54,url:"https://deepu.tech/first-impression-of-rust/",title:"My first impressions of Rust",body:"2019/11/07 - So I started learning Rust a while ago and since my post about what I thought of Go was popular, I decided to write about what my first impressions of Rust were as well. But unlike Go, I actually didn’t build any real-world application in Rust, so my opinions here are purely personal and some might not be accurate as I might have misunderstood something. So do give me the consideration of a Rust newbie. If you find something I said here is inaccurate please do let me know. Also, my impressions might actually change once I start using the language more. If it does, I’ll make sure to update the post. As I have said in some of my other posts, I consider myself to be more pragmatic than my younger self now. Some of the opinions are also from that pragmatic perspective(or at least I think so). I have weighed practicality, readability, and simplicity over fancy features, syntax sugars, and complexity. Also, some things which I didn’t like but found not such a big deal are put under nitpicks rather than the dislike section as I thought it was fairer that way. One thing that sets Rust apart from languages like Go is that Rust is not garbage collected, and I understand that many of the language features/choices where designed with that in mind. Rust is primarily geared towards procedural/imperative style of programming but it also lets you do a little bit of functional and object-oriented style of programming as well. And that is my favorite kind of mix. So, without any further ado, let’s get into it. What I like about RustThings that I really liked, in no particular order. No Garbage collection: One of the first things you would notice in Rust, especially if you are coming from garbage collected languages like Java or Golang is the lack of garbage collection. Yes, there is no GC in Rust, then how does it ensure my program runs efficiently in the given memory and how does it prevent out of memory errors? Rust has something called ownership, so basically any value in Rust must have a variable as its owner(and only one owner at a time) when the owner goes out of scope the value will be dropped freeing the memory regardless of it being in stack or heap memory. For example, in the below example the value of foo is dropped as soon as the method execution completes and the value of bar is dropped right after the block execution. 1234567891011fn main() {  let foo =  value ; // owner is foo and is valid within this method  {    let bar =  bar value ; // owner is bar and is valid within this block scope    println!( value of bar is {} , bar); // bar is valid here  }  println!( value of foo is {} , foo); // foo is valid here  println!( value of bar is {} , bar); // bar is not valid here as its out of scope}So by scoping variables carefully, we can make sure the memory usage is optimized and that is also why Rust lets you use block scopes almost everywhere. Also, the Rust compiler helps you in dealing with duplicate pointer references and so on. The below is invalid in Rust since foo is now using heap memory rather than stack and assigning a reference to a variable is considered a move. If deep copying(expensive) is required it has to be performed using the clone function which performs a copy instead of move. 1234567891011fn main() {  let foo = String::from( hello ); // owner is foo and is valid within this method  {    let bar = foo; // owner is bar and is valid within this block scope, foo in invalidated now    println!( value of bar is {} , bar); // bar is valid here  }  println!( value of foo is {} , foo); // foo is invalid here as it has moved}The ownership concept can be a bit weird to get used to, especially since passing a variable to a method will also move it(if its not a literal or reference), but given that it saves us from GC I think its worth it and the compiler takes care of helping us when we make mistakes. Immutable by default: Variables are immutable by default. If you want to mutate a variable you have to specifically mark it using the mut keyword. 12let foo =  hello  // immutablelet mut bar =  hello  // mutableVariables are by default passed by value, in order to pass a reference we would have to use the &amp; symbol. Quite similar to Golang. 12345678910111213141516fn main() {  let world = String::from( world );  hello_ref(&amp;world); // pass by reference. Keeps ownership  // prints: Hello world  hello_val(world); // pass by value and hence transfer ownership  // prints: Hello world}fn hello_val(msg: String) {  println!( Hello {} , msg);}fn hello_ref(msg: &amp;String) {  println!( Hello {} , msg);}When you pass a reference it is still immutable so we would have to explicitly mark that mutable as well as below. This makes accidental mutations very difficult. The compiler also ensures that we can only have one mutable reference in a scope. 1234567891011121314151617fn main() {  let mut world = String::from( world );  hello_ref(&amp;mut world); // pass by mutable reference. Keeps ownership  // prints: Hello world!  hello_val(world); // pass by value and hence transfer ownership  // prints: Hello world!}fn hello_val(msg: String) {  println!( Hello {} , msg);}fn hello_ref(msg: &amp;mut String) {  msg. push_str( ! ); // mutate string  println!( Hello {} , msg);}Pattern matching: Rust has first-class support for pattern matching and this can be used for control flow, error handling, variable assignment and so on. pattern matching can also be used in if, while statements, for loops and function parameters. 123456789101112131415fn main() {  let foo = String::from( 200 );  let num: u32 = match foo. parse() {    Ok(num) =&gt; num,    Err(_) =&gt; {      panic!( Cannot parse! );    }  };  match num {    200 =&gt; println!( two hundred ),    _ =&gt; (),  }}Generics: One thing I love in Java and TypeScript is the generics. It makes static typing more practical and DRY. Strongly typed languages(Looking at you Golang) without generics are annoying to work with. Fortunately, Rust has great support for generics. It can be used in types, functions, structs, and enums. The icing on the cake is that Rust converts the Generic code using specific code during compile time thus there is no performance penalty in using them. 1234567891011121314151617struct Point&lt;T&gt; {  x: T,  y: T,}fn hello&lt;T&gt;(val: T) -&gt; T {  return val;}fn main() {  let foo = hello(5);  let foo = hello( 5 );  let integer = Point { x: 5, y: 10 };  let float = Point { x: 1. 0, y: 4. 0 };}Static types and advanced type declarations: Rust is a strictly typed language with a static type system. It also has great type inference which means we don’t have to define types manually for everything. Rust also allows for complex type definitions. 12345type Kilometers = i32;type Thunk = Box&lt;dyn Fn() + Send + 'static&gt;;type Result&lt;T&gt; = std::result::Result&lt;T, std::io::Error&gt;;Nice and simple error handling: Error handling in Rust is quite nice, there are recoverable and unrecoverable errors. For recoverable errors, you can handle them using pattern matching on the Result enum or using the simple expect syntax. There is even a shorthand operator to propagate errors from a function. Take that Go. 1234567891011121314use std::fs::File;fn main() {  let f = File::open( hello. txt ). expect( Failed to open hello. txt );  // or  let f = match File::open( hello. txt ) {    Ok(file) =&gt; file,    Err(error) =&gt; {      panic!( Problem opening the file: {:?} , error)    },  };}Tuples: Rust has built-in support for tuples, and this is highly helpful when you have to return multiple values from a function or when you want to unwrap a value and so on. Block expressions: In Rust, you can have block expressions with their own scope almost anywhere. It also lets you assign a variable value from block expressions, if statement, loops and so on. 12345678fn main() {  let foo = {    println!( Assigning foo );    5  };  let bar = if foo &gt; 5 { 6 } else { 10 };}Beautiful compiler output: Rust simply has the best error output during compilation that I have seen. It can’t get better than this I think. It is so helpful.  Built-in tooling: Like many modern programming languages, Rust also provides a lot of build-in standard tooling and honestly, I think this is one of the best that I have come across. Rust has Cargo which is the built-in package manager and build system. It is an excellent tool. It takes care of all common project needs like compiling, building, testing and so on. It can even create new projects with a skeleton and manage packages globally and locally for the project. That means you don’t have to worry about setting up any tooling to get started in Rust. I love this, it saves so much time and effort. Every programming language should have this. Rust also provides built-in utilities and asserts to write tests which then can be executed using Cargo. In Rust, related functionality is grouped into modules, modules are grouped together into something called crates and crates are grouped into packages. We can refer to items defined in one module from another module. Packages are managed by cargo. You can specify external packages in the Cargo. toml file. Reusable public packages can be published to the crates. io registry. There are even offline built-in docs that you can get by running rustup docs and rustup docs --book which is amazing. Thanks to Mohamed ELIDRISSI for pointing it out to me. Concurrency: Rust has first-class support for memory safe concurrent programming. Rust uses threads for concurrency and has 1:1 threading implementation. i. e, 1 green thread per operating system thread. Rust compiler guarantees memory safety while using the threads. It provides features like waiting for all threads to finish, sharing data with move closures or channels(similar to Go). It also lets you use shared state and sync threads. 12345678910111213141516use std::thread;use std::time::Duration;fn main() {  thread::spawn(|| {    for i in 1. . 10 {      println!( hi number {} from the spawned thread! , i);      thread::sleep(Duration::from_millis(1));    }  });  for i in 1. . 5 {    println!( hi number {} from the main thread! , i);    thread::sleep(Duration::from_millis(1));  }}Macros and meta-programming: While I don’t like all aspects of macros in Rust, there are more things to like here than dislike. The annotation macros, for example, are quite handy. Not a fan of the procedure macros though. For advanced users, you can write your own macro rules and do metaprogramming. Traits: Traits are synonymous to interfaces in Java, it is used to define shared behaviors that can be implemented on structs. Traits can even specify default methods. The only thing I dislike here is the indirect implementation. 123456789101112131415161718192021222324252627282930pub trait Summary {  fn summarize_author(&amp;self) -&gt; String;  fn summarize(&amp;self) -&gt; String {    format!( (Read more from {}. . . ) , self. summarize_author())  }}pub struct NewsArticle {  pub author: String,  pub content: String,}impl Summary for NewsArticle {  fn summarize_author(&amp;self) -&gt; String {    format!( @{} , self. author)  }}fn main() {  let article = NewsArticle {    author: String::from( Iceburgh ),    content: String::from(       The Pittsburgh Penguins once again are the best hockey team in the NHL.  ,    ),  };  println!( New article available! {} , article. summarize());}Ability to use unsafe features if required:  Useful in advanced use-cases where you know what you are doing. A necessary evil IMO. I like it since its doable only within an unsafe { } block making it very explicit. I would have moved this to the dislike section if that was not the case.  When you use these, the Rust compiler cannot guarantee memory and runtime safety and you are on your own to get this right. So definitely for advanced and experienced users. What I don’t like about RustThings that I didn’t like very much, in no particular order. Complexity: I don’t like it when a language offers multiple ways to do the same things. This is one think Golang does pretty well, there are no two ways to do the same thing and hence it is easier for people to work on larger codebases and to review code. Also, you don’t have to always think of the best possible way to do something. Unfortunately, Rust does this and I’m not a fan of it. IMO it makes the language more complex.  Too many ways for iterations -&gt; loops, while, for, iterators.  Too many ways for creating procedures -&gt; Functions, closures, macrosEdit: Based on discussions here and on Reddit, I say my perception of complexity only have increased. It seems like once you get past all the niceties there are a lot of things that would take some time to wrap your head around. I’m pretty sure if you are experienced in Rust, it would be a cakewalk for you but the language indeed is quite complex, especially the ways functions and closures behave in different contexts, lifetimes in structs and stuff. Shadowing of variables in the same context: So Rust lets you do this 123456{  let foo =  hello ;  println!( {} , foo);  let foo =  world ;  println!( {} , foo);} Kind of beats being immutable by default(I understand the reasoning of being able to perform transformations on an immutable variable, especially when passing references to a function and getting it back) IMO lets people practice bad practices unintentionally, I would have rather marked the variable mutable as I consider the above mutation as well.  You can as easily accidentally shadow a variable as you would accidentally mutate one in Languages like JavaScript Gives people a gun to shoot in the footEdit: I saw a lot of comments here and on Reddit explaining why this is good. While I agree that it is useful in many scenarios, so is the ability of mutation. I think it would have been perfectly fine not to have this and people would have still loved Rust and all of them would have defended the decision not have this. So my opinion on this hasn’t changed. Functions are not first-class citizens: While it is possible to pass a function to another they are not exactly first-class citizens like in JavaScript or Golang. You cannot create closures from functions and you cannot assign functions to variables. Closures are separate from functions in Rust, they are quite similar to Java lambdas from what I see. While closures would be sufficient to perform some of the functional style programming patterns it would have been much nicer if it was possible using just functions thus making language a bit more simple. Edit: Oh boy! this opinion triggered a lot of discussions here and on Reddit. So seems like Functions and closures are similar and different based on context, It also seems like Functions are almost like first-class citizens, but if you are used to languages like Go or JavaScript where functions are much more straight forward then you are in for a crazy ride. Functions in Rust seems much much more complex. A lot of people who commented seemed to miss the fact that my primary complaint was that having two constructs(closures and functions) that look and act quite similar in most of the scenarios makes things more complex. At least in Java and JS where there are multiple constructs(arrow functions, lambdas) those where due to the fact that they were added much later to the language and those are still something I don’t like in those languages. The best explanation was from Yufan Lou and another from zesterer. I’m not gonna remove this from stuff I don’t like since I still don’t like the complexity here. Implicit implementation of traits: I’m not a fan of implicit stuff as its easier to abuse this and harder to read. You could define a struct in one file and you could implement a trait for that struct in another file which makes it less obvious. I prefer when the implementation is done by intent like in Java which makes it more obvious and easier to follow. While the way in Rust is not ideal, it is definitely better than Golang which is even more indirect. NitpicksFinally some stuff I still don’t like but I don’t consider them a big deal.  I don’t see the point of having the const keyword when let is immutable by default. It seems more like syntax sugar for the old school constants concept. Diane pointed out the difference const provides and that makes sense.  The block expression and implicit return style are a bit error-prone and confusing to get used to, I would have preferred explicit return. Also, it’s not that readable IMO.  If you have read my other post about Go, you might know that I’m not a fan of structs. Structs in Rust is very similar to structs in Golang. So like in Go, it would be easy to achieve unreadable struct structures. But fortunately, the structs in Rust seem much nicer than Go as you have functions, can use spread operator, shorthand syntax and so on here. Also, you can make structs which are Tuples. The structs in Rust are more like Java POJOs. I would have moved this to the liked section if having optional fields in structs where easier. Currently, you would have to wrap stuff in an Optional enum to do this. Also lifetimes :( Given strings are the most used data types, it would have been nice to have a simpler way of working with strings(like in Golang) rather than working with the Vector types for mutable strings or slice types for immutable string literals. This makes code more verbose than it needs to be. This is more likely a compromise due to the fact that Rust is not garbage collected and has a concept of ownership to manage memory. https://doc. rust-lang. org/rust-by-example/std/str. html - Edit: I have moved this point to nitpicks rather than dislikes after a discussion on the comments with robertorojasrConclusionI like programming languages that focus more on simplicity rather than fancy syntax sugars and complex features. In my post “My reflections on Golang”, I explain why I consider Golang to be too simple for my taste. Rust, on the other hand, is leaning towards the other side of the spectrum. While it is not as complex as Scala it is not as simple as Go as well. So its somewhere in between, not exactly the sweet spot but almost near that quite close to where JavaScript is maybe. So overall I can say that there are more things in Rust for me to like than to dislike which is what I would expect from a nice programming language. Also, bear in mind that I’m not saying Rust should do anything differently or that there are better ways to do things that I complained about. I’m just saying that I don’t like those things but I can live with it given the overall advantages. Also, I fully understand why some of those concepts are the way they are, those are mostly tradeoffs to focus on memory safety and performance. But don’t be fooled by what you see over the hood, Rust is definitely not something you should start with as your first programming language IMO, as it has a quite a lot of complex concepts and constructs underneath but if you are already familiar with programming then it shouldn’t be an issue after banging your head on the doors a few times :P So far I can say that I like Rust more than Golang even without implementing a real project with it and might choose Rust over Go for system programming use cases and for high-performance requirements. Note: Some of my opinions have changed after using Rust more. Checkout my new post about the same                               My second impression of Rust and why I think it's a great general-purpose language!:             07-May-2021                           #rust #programming #languages #webassembly                                    If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Image from Link Clark, The Rust team under Creative Commons Attribution Share-Alike License v3. 0. "},{id:55,url:"https://deepu.tech/microservices-the-good-bad-and-the-ugly/",title:"Microservices - the good, bad, and the ugly",body:"2019/10/29 - So you are attending this awesome conference where speaker after speaker talks about how awesome microservices are. You hear a speaker from a Tech giant presenting how they scaled up and solved their issues using microservices running on containers or something, and you wonder shouldn’t we do the same as well? The reality is that there is no silver bullet in software engineering. Microservices doesn’t magically solve every scaling issues, some times they introduce more problems then they solve. Well, this is not a microservice bashing post, as someone who likes microservice architectures(sometimes for their sheer complexity and sometimes for the ingenuity), I think it’s important to know the cost of doing microservice architectures in real-world use cases. I talked briefly about this in my book as well. Microservice architecturesIn a microservice architecture, you generally split your domain model into individual loosely coupled services that can be deployed and scaled individually. Each of the services is in itself a small application with its own architecture and even runtime sometimes. The services might offer REST endpoints that can be aggregated by an application acting as a gateway or the services could be communicating with each other using a messaging system. Some services might have Web GUI some are headless services offering just an API. In microservice architectures, each service is responsible for handling its data and ideally do not share database schema with other services. Such architectures make it easy to provide different clients for Mobile and Desktop experience which can be scaled independently based on demand.  There are different architecture patterns for building microservices. The patterns used also decides the way services communicate and the way they are aggregated. This article details some of these patterns quite nicely. So let’s break down the benefits and issues of microservice architecture and see when it makes sense to adopt one and when to avoid one. General benefits of Microservice architecturesIn general, Microservices provide(As it depends on how you actually implement it) or at least promise the below benefits, which can be grouped into three major categories Loose coupling: Microservice components are more loosely coupled than traditional architectures. Such systems employ event-driven or message-driven architectures to achieve communication between loosely coupled components. This results in better isolation of components and makes it easy to unit test them and faster to startup. Such systems also provide other benefits like, for example, a memory leak in one of the services, are isolated and hence will not bring down the entire application. Hence overall single point of failures are reduced Loosely coupled individual components will start up much faster than a big monolith making it possible to parallelize and improve overall start-up for large systems. It also makes it easy to refactor existing features as you can gradually refactor things rather than having to refactor an entire system in one go. Because of such loose coupling, each service can choose to use a database/datastore that is more appropriate whereas in a monolith you might compromise with a single database type. For example, a service dealing with a lot of unstructured data can choose a NoSQL database while a service that is handling transactions or structured data can opt for a SQL database. Faster development &amp; release cycle: In a well-implemented microservice architecture, development turnaround is faster and hence you get a better time to market for new features and easier refactoring of existing features. A complex problem domain can be easily tackled by splitting it into separately manageable services making it easier to understand and maintain in the long run. Technology adoption is easier, components can be independently upgraded in incremental migration making it possible to have a different stack for each component. It is also possible to have different microservices in a system use different implementation languages and just communicate using a common messaging format like gRPC or a message queue or pub/sub,thus making it possible to have teams with different language skills hence less dependency on a single language or stack. Teams will be less dependent on each other as communication between systems is governed by a public API or contract letting you change internals without having to worry about breaking someone else’s code. Best suited for agile teams. Such teams can have better focus as they only need to worry about a single service. Fine-grained scaling: One of the most important benefits of a microservice architecture is the ability to scale individual components based on load. If implemented properly this will result in ideal load distribution and reduced overall infrastructure cost. Services with more demand can be scaled up while the ones with less demand can be scaled down utilizing infrastructure more efficiently. Deploying services independently also makes the application more reliable and makes patching easier as you do not have to upgrade the entire application to fix an issue in a single service. More complex and efficient scaling models can be established. Critical services can be scaled more effectively. Infrastructure is used more efficiently. Continuous delivery of such complex applications also would be easier than its equivalent monolith as components are smaller and any issue in deployment can be investigated easily and rectified on a per-component basis General issues with microservice architecturesWell with any architectures, there are disadvantages of Microservice architectures as well. Complexity: Complexity is one of the biggest side effects of this architecture. While microservices can reduce the domain complexity by breaking the problem into smaller services,there could be complexities of a distributed system in terms of;  Overall stack as different components might have different technology stacks forcing the team to invest more time in keeping up with them.  Scaling is more efficient but it would require advanced features such as service discovery, DNS routing, and so on.  Communication between components might require a messaging system(Queue, PubSub, Event store).  Business transactions on a distributed system might involve updating multiple databases making rollbacks more complex and error-prone.  The entire application is more complex to deploy as there are complexities with containers, orchestration, and virtualization involved.  Requires a complex infrastructure. Most often will require containers (Docker), Orchestration(Kubernetes) and multiple JVM or app containers to run on. Integration testing: End-to-end tests and integration tests become harder to perform there are more moving parts in the stack and more complex communication between components. The testing infrastructure required also becomes more difficult to set up and maintain. Team size and experience: The technical stack for microservices is more complex and most of the time harder to learn and hence it would demand a more experienced team with more senior-level skillset than that would be required for a similar monolithic application. It will also require a bigger team to maintain the application as there are more components and more technologies involved. Implementing requirements that span multiple services would require more upfront time to agree on contracts and APIs. Team members share varying skill sets based on the component they work on but might not be having a birds-eye view of the entire application making business requirements harder to visualize and cross-cutting issues harder to fix. Overhead: Complex microservices will have the additional overhead of running monitoring setup, messaging services, orchestration, service registry and so on. Initial development time will be higher due to the complexity making time to market slower. The overall cost of the initial infrastructure might be much higher than that of a similar monolith. In microservice architectures, there is always code duplication between services which also can be considered overhead. When you should not be considering Microservice architecturesYou should not be using microservice architecture unless you absolutely have to, remember not every application has the same scale requirements as Netflix, Google, Amazon or Spotify. Many of the benefits that microservices provide to these kinds of applications are due to their sheer scale which might not be applicable to you. So here are some reasons not to choose microservices and maybe stick to monoliths.  When your application’s scope is small and you know that it’s not going to grow and turn into something like Facebook. For well defined simple usecases a monolith is always the best fit. Examples are     A CRUD application for an internal use case in a company.    A small application with a very niche user base. Like a shopping site for some specialty items.     When the time to market is critical for a new application. The initial time to market would be higher for microservices.  When the size of your team is small or the average experience of the team is less. Its best to start with a monolith when you are a small or inexperienced team.  When your infrastructure budget is limited. Though on long-run microservice might help to save money, in the beginning, it is going to cost you more. Most importantly do not choose microservices because it is the hype or because it is used by a popular company or because it was suggested by a popular person. For most use cases monoliths are still a great solution and even if you start with a monolith you can always split away into microservices if required. When you could consider Microservice architecturesIn general, Microservices tend to be beneficial if you have one of the below scenarios.  When your use case domain is complex, you have a large team with experience and splitting it up would make it easier to implement.  When you are expecting to become the next Facebook, Netflix or Twitter in terms of user load. So ideally when you are expecting an exponential user base.  If your application is going to be an API provider for other applications with a large userbase. Like a payment gateway or inventory service that will be used by a social media application When you have a popular e-commerce application with a large userbase with an uneven load on different services in the application. Its time to split them into microservices. So in conclusion, don’t choose an architecture pattern because it works for someone else, choose a pattern that is appropriate for your use case, scale and requirements. Not everyone needs to handle millions of concurrent users or stream terabytes of data. If you liked this article you might like my book as well. You can get it from Packt and Amazon.  If you like this article, please leave a like or a comment. If you do decide to build microservices checkout JHipster and my below articles don’t forget to give it a star on Github.  Create full Microservice stack using JHipster Domain Language under 30 minutes Deploying JHipster Microservices on Azure Kubernetes Service (AKS) JHipster microservices with Istio service mesh on KubernetesYou can follow me on Twitter and LinkedIn. Cover image photo by Sergei Akulich on Unsplash "},{id:56,url:"https://deepu.tech/functional-programming-in-typescript/",title:"Easy functional programming techniques in TypeScript for everyone",body:"2019/08/14 - There is a lot of hype around functional programming(FP) and a lot of cool kids are doing it but it is not a silver bullet. Like other programming paradigms/styles, functional programming also has its pros and cons and one may prefer one paradigm over the other. If you are a TypeScript/JavaScript developer and wants to venture into functional programming, do not worry, you don’t have to learn functional programming oriented languages like Haskell or Clojure since JavaScript and hence TypeScript has you covered and this post is for you. If you are looking for functional programming in Java or Golang check other posts in the series. I’m not gonna dive into all functional programming concepts in detail, instead, I’m gonna focus on things that you can do in TypeScript which are in line with functional programming concepts. I’m also not gonna discuss the pros and cons of functional programming in general. Please keep in mind, though this post is about TypeScript, you can easily do the same in JavaScript as well since TypeScript is just a typed superset of JavaScript. What is functional programming?: As per Wikipedia,  Functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. Hence in functional programming, there are two very important rules  No Data mutations: It means a data object should not be changed after it is created.  No implicit state: Hidden/Implicit state should be avoided. In functional programming state is not eliminated, instead, its made visible and explicitThis means:  No side effects: A function or operation should not change any state outside of its functional scope. I. e, A function should only return a value to the invoker and should not affect any external state. This means programs are easier to understand.  Pure functions only: Functional code is idempotent. A function should return values only based on the arguments passed and should not affect(side-effect) or depend on global state. Such functions always produce the same result for the same arguments. Apart from these there are functional programming concepts below that can be applied in TypeScript, we will touch upon these further down.  Higher-order-functions Closures Currying Recursion Lazy evaluations Referential transparencyUsing functional programming doesn’t mean its all or nothing, you can always use functional programming concepts to complement Object-oriented concepts in TypeScript. The benefits of functional programming can be utilized whenever possible regardless of the paradigm or language you use. And that is exactly what we are going to see. Functional programming in TypeScript: TypeScript is not a purely functional language but offers a lot of concepts which are in line with functional languages, so let us see how we can apply some of the functional programming concepts above in TypeScript. First-class and higher-order functions: First-class functions(function as a first-class citizen) means you can assign functions to variables, pass a function as an argument to another function or return a function from another. TypeScript supports this and hence makes concepts like closures, currying, and higher-order-functions easy to write. A function can be considered as a higher-order-function only if it takes one or more functions as parameters or if it returns another function as a result. In TypeScript, this is quite easy to do 123456789101112131415161718type mapFn = (it: string) =&gt; number;// The higher-order-function takes an array and a function as argumentsfunction mapForEach(arr: string[], fn: mapFn): number[] {  const newArray: number[] = [];  arr. forEach(it =&gt; {    // We are executing the method passed    newArray. push(fn(it));  });  return newArray;}const list = [ Orange ,  Apple ,  Banana ,  Grape ];// we are passing the array and a function as arguments to mapForEach method. const out = mapForEach(list, (it: string): number =&gt; it. length);console. log(out); // [6, 5, 6, 5]But then in JavaScript/TypeScript we could also simply do it this way using built-in functional methods like map, reduce and so on. 123456const list = [ Orange ,  Apple ,  Banana ,  Grape ];// we are passing a function as arguments to the built-in map method. const out = list. map(it =&gt; it. length);console. log(out); // [6, 5, 6, 5]Closures and currying are also possible in TypeScript 123456789101112131415// this is a higher-order-function that returns a functionfunction add(x: number): (y: number) =&gt; number {  // A function is returned here as closure  // variable x is obtained from the outer scope of this method and memorized in the closure  return (y: number): number =&gt; x + y;}// we are currying the add method to create more variationsvar add10 = add(10);var add20 = add(20);var add30 = add(30);console. log(add10(5)); // 15console. log(add20(5)); // 25console. log(add30(5)); // 35There are also many built-in declarative higher-order-functions in TypeScript/JavaScript like map, reduce, forEach, filter and so on. There are also many libraries that provide functional interfaces to be used in TypeScript/JavaScript. Pure functions: As we saw already a pure function should return values only based on the arguments passed and should not affect or depend on global state. It is possible to do this in TypeScript easily. This is quite simple, take the below this is a pure function. It will always return the same output for the given input and its behavior is highly predictable. We can safely cache the method if needed. 123function sum(a: number, b: number): number {  return a + b;}If we add an extra line in this function, the behavior becomes unpredictable as it now has a side effect that affects an external state. 1234567const holder = {};function sum(a: number, b: number): number {  let c = a + b;  holder[`${a}+${b}`] = c;  return c;}So try to keep your functions pure and simple. Using tools like ESLint and typescript-eslint it is possible to enforce these. Recursion: Functional programming favors recursion over looping. Let us see an example for calculating the factorial of a number. In traditional iterative approach: 123456789function factorial(num: number): number {  let result = 1;  for (; num &gt; 0; num--) {    result *= num;  }  return result;}console. log(factorial(20)); // 2432902008176640000The same can be done using recursion as below which is favored in functional programming. 1234const factorial = (num: number): number =&gt;  num == 0 ? 1 : num * factorial(num - 1);console. log(factorial(20)); // 2432902008176640000The downside of the recursive approach is that it will be slower compared to an iterative approach most of the times(The advantage we are aiming for is code simplicity and readability) and might result in stack overflow errors since every function call needs to be saved as a frame to the stack. To avoid this tail recursion is preferred, especially when the recursion is done too many times. In tail recursion, the recursive call is the last thing executed by the function and hence the functions stack frame need not be saved by the compiler. Most compilers can optimize the tail recursion code the same way iterative code is optimized hence avoiding the performance penalty. Tail call optimization is part of the ECMAScript specs but unfortunately, most JavaScript engines do not support this yet. Now using tail recursion the same function can be written as below, but depending on the engine this might not be optimized, though there are workarounds, still it performed better in benchmarks. 123456const factorialTailRec = (num: number): number =&gt; factorial(1, num);const factorial = (accumulator: number, val: number): number =&gt;  val == 1 ? accumulator : factorial(accumulator * val, val - 1);console. log(factorialTailRec(20)); // 2432902008176640000Consider using recursion when writing TypeScript code for readability and immutability, but if performance is critical or if the number of iterations will be huge use standard loops. Lazy evaluation: Lazy evaluation or non-strict evaluation is the process of delaying evaluation of an expression until it is needed. In general, TypeScript does strict/eager evaluation but for operands like &amp;&amp;, || and ?: it does a lazy evaluation. We can utilize short-circuiting, higher-order-functions, closures, and memoization techniques to do lazy evaluations. Take this example where TypeScript eagerly evaluates everything. 1234567891011121314151617181920function add(x: number): number {  console. log( executing add ); // this is printed since the functions are evaluated first  return x + x;}function multiply(x: number): number {  console. log( executing multiply ); // this is printed since the functions are evaluated first  return x * x;}function addOrMultiply(  add: boolean,  onAdd: number,  onMultiply: number): number {  return add ? onAdd : onMultiply;}console. log(addOrMultiply(true, add(4), multiply(4))); // 8console. log(addOrMultiply(false, add(4), multiply(4))); // 16This will produce the below output and we can see that both functions are executed always 123456executing addexecuting multiply8executing addexecuting multiply16We can use higher-order-functions to rewrite this into a lazily evaluated version 12345678910111213141516171819202122function add(x: number): number {  console. log( executing add );  return x + x;}function multiply(x: number): number {  console. log( executing multiply );  return x * x;}type fnType = (t: number) =&gt; number;// This is now a higher-order-function hence evaluation of the functions are delayed in if-elsefunction addOrMultiply(  add: boolean,  onAdd: fnType,  onMultiply: fnType,  t: number): number {  return add ? onAdd(t) : onMultiply(t);}console. log(addOrMultiply(true, add, multiply, 4));console. log(addOrMultiply(false, add, multiply, 4));This outputs the below and we can see that only required functions were executed 1234executing add8executing multiply16Or by memoization like this 1234567891011121314151617181920212223242526272829303132const cachedAdded = {};function add(x: number): number {  if (cachedAdded[x]) {    return cachedAdded[x];  }  console. log( executing add );  const out = x + x;  cachedAdded[x] = out;  return out;}const cachedMultiplied = {};function multiply(x: number): number {  if (cachedMultiplied[x]) {    return cachedMultiplied[x];  }  console. log( executing multiply );  const out = x * x;  cachedMultiplied[x] = out;  return out;}function addOrMultiply(  add: boolean,  onAdd: number,  onMultiply: number): number {  return add ? onAdd : onMultiply;}console. log(addOrMultiply(true, add(4), multiply(4))); // 8console. log(addOrMultiply(false, add(4), multiply(4))); // 16This outputs the below and we can see that functions were executed only once for the same values 1234executing addexecuting multiply816Please note that memoization techniques will work only when your functions are pure and referentially transparent. There are also other ways of doing Lazy evaluations like this. Doing Lazy evaluations in TypeScript might not be worth the code complexity some of the times, but if the functions in question are heavy in terms of processing then its is absolutely worth it to lazy evaluate them. Type system: TypeScript has a strong type system and also has great type inference. While the underlying JavaScript itself is weakly typed, TypeScript along with a compatible IDE can bridge that gap. Referential transparency: From Wikipedia:  Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent. Unfortunately, there are not many ways to strictly limit data mutation in JavaScript, however by using pure functions and by explicitly avoiding data mutations and reassignment using other concepts we saw earlier this can be achieved. JavaScript by default passes primitive variables by value and objects by reference so we need to take care not to mutate data inside functions. Libraries like Immutable JS could also be considered. Use const as much as possible to avoid reassignments. For example, the below will produce an error 123const list = [ Apple ,  Orange ,  Banana ,  Grape ];list = [ Earth ,  Saturn ];But this will not help when variables are holding references to other objects, for example, the below mutation will work irrespective of the const keyword. 1234const list = [ Apple ,  Orange ,  Banana ,  Grape ];list. push( Earth ); // will mutate the listlist. push( Saturn ); // will mutate the listconst keyword allows the internal state of referenced variables to be mutated and hence from a functional programming perspective const keyword is useful only for primitive constants and to catch reassignments. However, with TypeScript, we can use special mapped types to make objects read-only and hence avoiding accidental data mutations which are caught during compile time. Thanks to @stereobooster and @juliang for pointing it out. Read my post about mapped and conditional types here to learn more. 123const list: Readonly&lt;string[]&gt; = [ Apple ,  Orange ,  Banana ,  Grape ];list. push( Earth ); // will cause compilation erroror 123const list: ReadonlyArray&lt;string&gt; = [ Apple ,  Orange ,  Banana ,  Grape ];list. push( Earth ); // will cause compilation errorOther techniques to follow are using Object. freeze or built-in methods like map, reduce, filter and so on as they do not mutate the data. We can also use this ESlint plugin to restrict mutations. Data structures: When using functional programming techniques it is encouraged to use data types such as Stacks, Maps and Queues which have functional implementations as well. Hence maps are better than arrays or hash sets in functional programming as data stores. Conclusion: This is just an introduction for those who are trying to apply some functional programming techniques in TypeScript. There are a lot more that can be done in TypeScript and with the ever-evolving ECMAScript underneath, this should be even easier. As I said earlier functional programming is not a silver bullet but it offers a lot of useful techniques for more understandable, maintainable and testable code. It can co-exist perfectly well with imperative and object-oriented programming styles. In fact, we all should be using the best of everything. I hope you find this useful. If you have any question or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:57,url:"https://deepu.tech/functional-programming-in-go/",title:"7 Easy functional programming techniques in Go",body:"2019/08/14 - There is a lot of hype around functional programming(FP) and a lot of cool kids are doing it but it is not a silver bullet. Like other programming paradigms/styles, functional programming also has its pros and cons and one may prefer one paradigm over the other. If you are a Go developer and wants to venture into functional programming, do not worry, you don’t have to learn functional programming oriented languages like Haskell or Clojure(or even Scala or JavaScript though they are not pure functional programming languages) since Go has you covered and this post is for you. If you are looking for functional programming in Java then check this out                               7 Functional programming techniques in Java - A primer:             30-Jul-2019                           #java #functional #beginners #programming                                    I’m not gonna dive into all functional programming concepts in detail, instead, I’m gonna focus on things that you can do in Go which are in line with functional programming concepts. I’m also not gonna discuss the pros and cons of functional programming in general. What is functional programming?: As per Wikipedia,  Functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. Hence in functional programming, there are two very important rules  No Data mutations: It means a data object should not be changed after it is created.  No implicit state: Hidden/Implicit state should be avoided. In functional programming state is not eliminated, instead, its made visible and explicitThis means:  No side effects: A function or operation should not change any state outside of its functional scope. I. e, A function should only return a value to the invoker and should not affect any external state. This means programs are easier to understand.  Pure functions only: Functional code is idempotent. A function should return values only based on the arguments passed and should not affect(side-effect) or depend on global state. Such functions always produce the same result for the same arguments. Apart from these there are functional programming concepts below that can be applied in Go, we will touch upon these further down.  Higher-order-functions Closures Currying Recursion Lazy evaluations Referential transparencyUsing functional programming doesn’t mean its all or nothing, you can always use functional programming concepts to complement Object-oriented or imperative concepts in Go. The benefits of functional programming can be utilized whenever possible regardless of the paradigm or language you use. And that is exactly what we are going to see. Functional programming in Go: Golang is a multi-paradigm language so let us see how we can apply some of the functional programming concepts above in Go. First-class and higher-order functions: First-class functions(function as a first-class citizen) means you can assign functions to variables, pass a function as an argument to another function or return a function from another. Go supports this and hence makes concepts like closures, currying, and higher-order-functions easy to write. A function can be considered as a higher-order-function only if it takes one or more functions as parameters or if it returns another function as a result. In Go, this is quite easy to do 12345678910111213141516171819func main() {\tvar list = []string{ Orange ,  Apple ,  Banana ,  Grape }\t// we are passing the array and a function as arguments to mapForEach method. \tvar out = mapForEach(list, func(it string) int {\t\treturn len(it)\t})\tfmt. Println(out) // [6, 5, 6, 5]}// The higher-order-function takes an array and a function as argumentsfunc mapForEach(arr []string, fn func(it string) int) []int {\tvar newArray = []int{}\tfor _, it := range arr {\t\t// We are executing the method passed\t\tnewArray = append(newArray, fn(it))\t}\treturn newArray}Closures and currying are also possible in Go 1234567891011121314151617181920// this is a higher-order-function that returns a functionfunc add(x int) func(y int) int {\t// A function is returned here as closure\t// variable x is obtained from the outer scope of this method and memorized in the closure\treturn func(y int) int {\t\treturn x + y\t}}func main() {\t// we are currying the add method to create more variations\tvar add10 = add(10)\tvar add20 = add(20)\tvar add30 = add(30)\tfmt. Println(add10(5)) // 15\tfmt. Println(add20(5)) // 25\tfmt. Println(add30(5)) // 35}There are also many built-in higher-order-functions in Go standard libraries. There are also some functional style libraries like this and this offering map-reduce like functional methods in Go. Pure functions: As we saw already a pure function should return values only based on the arguments passed and should not affect or depend on global state. It is possible to do this in Go easily. This is quite simple, take the below this is a pure function. It will always return the same output for the given input and its behavior is highly predictable. We can safely cache the method if needed. 123func sum(a, b int) int {\treturn a + b}If we add an extra line in this function, the behavior becomes unpredictable as it now has a side effect that affects an external state. 1234567var holder = map[string]int{}func sum(a, b int) int {\tc := a + b\tholder[fmt. Sprintf( %d+%d , a, b)] = c\treturn c}So try to keep your functions pure and simple. Recursion: Functional programming favors recursion over looping. Let us see an example for calculating the factorial of a number. In traditional iterative approach: 1234567891011func factorial(num int) int {\tresult := 1\tfor ; num &gt; 0; num-- {\t\tresult *= num\t}\treturn result}func main() {\tfmt. Println(factorial(20)) // 2432902008176640000}The same can be done using recursion as below which is favored in functional programming. 123456789func factorial(num int) int {\tif num == 0 {\t\treturn 1\t}\treturn num * factorial(num-1)}func main() {\tfmt. Println(factorial(20)) // 2432902008176640000}The downside of the recursive approach is that it will be slower compared to an iterative approach most of the times(The advantage we are aiming for is code simplicity and readability) and might result in stack overflow errors since every function call needs to be saved as a frame to the stack. To avoid this tail recursion is preferred, especially when the recursion is done too many times. In tail recursion, the recursive call is the last thing executed by the function and hence the functions stack frame need not be saved by the compiler. Most compilers can optimize the tail recursion code the same way iterative code is optimized hence avoiding the performance penalty. Go compiler, unfortunately, does not do this optimization. Now using tail recursion the same function can be written as below, but Go doesn’t optimize this, though there are workarounds, still it performed better in benchmarks. 1234567891011121314func factorialTailRec(num int) int {\treturn factorial(1, num)}func factorial(accumulator, val int) int {\tif val == 1 {\t\treturn accumulator\t}\treturn factorial(accumulator*val, val-1)}func main() {\tfmt. Println(factorialTailRec(20)) // 2432902008176640000}I ran some benchmarks with all 3 approaches and here is the result, as you can see looping is still the most performing followed by the tail recursion. 12345678goos: linuxgoarch: amd64BenchmarkFactorialLoop-12    \t100000000\t    11. 7 ns/op\t    0 B/op\t    0 allocs/opBenchmarkFactorialRec-12    \t30000000\t    52. 9 ns/op\t    0 B/op\t    0 allocs/opBenchmarkFactorialTailRec-12  \t50000000\t    44. 2 ns/op\t    0 B/op\t    0 allocs/opPASSok \t_/home/deepu/workspace/deepu105. github. io/temp\t5. 072sSuccess: Benchmarks passed. Consider using recursion when writing Go code for readability and immutability, but if performance is critical or if the number of iterations will be huge use standard loops. Lazy evaluation: Lazy evaluation or non-strict evaluation is the process of delaying evaluation of an expression until it is needed. In general, Go does strict/eager evaluation but for operands like &amp;&amp; and || it does a lazy evaluation. We can utilize higher-order-functions, closures, goroutines, and channels to emulate lazy evaluations. Take this example where Go eagerly evaluates everything. 123456789101112131415161718192021func main() {\tfmt. Println(addOrMultiply(true, add(4), multiply(4))) // 8\tfmt. Println(addOrMultiply(false, add(4), multiply(4))) // 16}func add(x int) int {\tfmt. Println( executing add ) // this is printed since the functions are evaluated first\treturn x + x}func multiply(x int) int {\tfmt. Println( executing multiply ) // this is printed since the functions are evaluated first\treturn x * x}func addOrMultiply(add bool, onAdd, onMultiply int) int {\tif add {\t\treturn onAdd\t}\treturn onMultiply}This will produce the below output and we can see that both functions are executed always 123456executing addexecuting multiply8executing addexecuting multiply16We can use higher-order-functions to rewrite this into a lazily evaluated version 12345678910111213141516171819202122func add(x int) int {\tfmt. Println( executing add )\treturn x + x}func multiply(x int) int {\tfmt. Println( executing multiply )\treturn x * x}func main() {\tfmt. Println(addOrMultiply(true, add, multiply, 4))\tfmt. Println(addOrMultiply(false, add, multiply, 4))}// This is now a higher-order-function hence evaluation of the functions are delayed in if-elsefunc addOrMultiply(add bool, onAdd, onMultiply func(t int) int, t int) int {\tif add {\t\treturn onAdd(t)\t}\treturn onMultiply(t)}This outputs the below and we can see that only required functions were executed 1234executing add8executing multiply16There are also other ways of doing it using Sync &amp; Futures like this and using channels and goroutines like this. Doing Lazy evaluations in Go might not be worth the code complexity most of the times, but if the functions in question are heavy in terms of processing then its is absolutely worth it to lazy evaluate them. Type system: Go has a strong type system and also has pretty decent type inference. The only thing missing compared to other functional programming languages are something like case classes and pattern matching. Referential transparency: From Wikipedia:  Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent. Unfortunately, there are not many ways to strictly limit data mutation in Go, however by using pure functions and by explicitly avoiding data mutations and reassignment using other concepts we saw earlier this can be achieved. Go by default passes variables by value, except for slices and maps. So, avoid passing them by reference(using pointers) as much as possible. For example, the below will mutate external state as we are passing a parameter by reference and hence doesn’t ensure referential transparency 12345678910111213141516171819func main() {\ttype Person struct {\t\tfirstName string\t\tlastName string\t\tfullName string\t\tage    int\t}\tvar getFullName = func(in *Person) string {\t\tin. fullName = in. firstName + in. lastName // data mutation\t\treturn in. fullName\t}\tjohn := Person{\t\t john ,  doe ,   , 30,\t}\tfmt. Println(getFullName(&amp;john)) // johndoe\tfmt. Println(john) // {john doe johndoe 30}}If we pass parameters by the value we can ensure referential transparency even if there is an accidental mutation of passed data within the function 12345678910111213141516171819func main() {\ttype Person struct {\t\tfirstName string\t\tlastName string\t\tfullName string\t\tage    int\t}\tvar getFullName = func(in Person) string {\t\tin. fullName = in. firstName + in. lastName\t\treturn in. fullName\t}\tjohn := Person{\t\t john ,  doe ,   , 30,\t}\tfmt. Println(getFullName(john))\tfmt. Println(john)}We cannot rely on this when passed parameters are maps or slices. Data structures: When using functional programming techniques it is encouraged to use functional data types such as Stacks, Maps and Queues. Hence maps are better than arrays or hash sets in functional programming as data stores. Conclusion: This is just an introduction for those who are trying to apply some functional programming techniques in Go. There are a lot more that can be done in Go and with the addition of generics in the next major version, this should be even easier. As I said earlier functional programming is not a silver bullet but it offers a lot of useful techniques for more understandable, maintainable and testable code. It can co-exist perfectly well with imperative and object-oriented programming styles. In fact, we all should be using the best of everything. I hope you find this useful. If you have any question or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:58,url:"https://deepu.tech/static-site-generators-rundown-how-i-set-up-my-own-blog-with-jekyll/",title:"Static Site Generators rundown - How I set up my own blog with Jekyll",body:"2019/08/01 - Some time ago I decided to move my blogs from Medium to Dev. to, I have detailed the reasons in the below post.                               Why I’m moving away from Medium:             13-Jun-2019                           #writing #medium #development #tech                                    A lot of people suggested in the comments to set up my own blog and cross-post to Dev. to instead of relying only on one platform and I completely agree with them. I was procrastinating setting up my own blog for quite some time now. Finally, I decided to do it and set up my own blog at https://deepu. tech/blogs/ in the process I also updated my personal website to use the same platform. So when I decided to do this finally I had to choose a blogging platform and there were few requirements I was keen about which influenced my decision. Requirements:  The platform should support writing posts using Markdown with code syntax highlights I love the Dev community and hence wanted to cross-post everything to Dev. to as well without having to make any changes to the post. Means I would author once and publish to both my blog and Dev. This means some constraints/requirements     It should support customizable front matter so that I can align it with Dev   It should support the custom liquid tags used by Dev or I should be able to easily add those    I should be able to have custom pages for my personal website Should be open source and have a good stable community Should be theme-able, have plugins for SEO, search and so on Should be statically generated and reasonably fast Should be able to host using GitHub pages - This was an optional requirementThe options rundown: With these in mind, I started evaluating some of the popular options below. Jekyll: Pros:  I have experience with Jekyll since I built the new JHipster website using it Supports Markdown, Liquid tags and Front Matter Supports custom pages, themes, plugins and is statically generated Is OSS and has a vibrant community Can be hosted on GitHubCons:  I would have to build or find replacements for the custom Liquid tags used by Dev I don’t have much experience with Ruby and I’m not very familiar with the Ruby ecosystem Not the fastest among the options. Becomes slower as site size increasesHugo: Pros:  Is very fast I have extensive experience with Go and Go templates which would be helpful Supports Markdown and Front Matter Supports custom pages, themes and is statically generated Is OSS and has a vibrant community Can be hosted on GitHubCons:  Doesn’t support Liquid tags Doesn’t have plugins. The built-in options are enough for my requirements at the moment thoughVuePress: Pros:  Built with VueJS and I have good experience with JavaScript and quite familiar with Vue Supports Markdown and Front Matter Supports custom pages, themes, SEO, search and is statically generated Is OSS and has a vibrant community Can be hosted on GitHubCons:  Doesn’t support Liquid tags Doesn’t have plugins. The built-in options are enough for my requirements at the moment though Not geared towards blogging, but it’s possible to do it easily with some hackingGatsby: Pros:  Built with React and I have good experience with React Supports Markdown and Front Matter Supports custom pages, themes, plugins and is statically generated Is OSS and has a vibrant community Can be hosted on GitHubCons:  Doesn’t support Liquid tagsWordPress: Pros:  Have used it in the past and is a battle-tested solution Supports Markdown using plugins Supports custom pages, themes, plugins and can be statically generated using plugins Is OSS and has a vibrant community Can be hosted on GitHub with some workaroundsCons:  Doesn’t support Front Matter and Liquid tags Since most of my core requirements can only be achieved using plugins and workarounds it feels too clumsyThough I personally liked Hugo because of its speed, based on the above the most logical choice for me was Jekyll. Building a personal website and blog with Jekyll: Getting started: Setting up Jekyll is super easy, I followed the official guide and had a site up and running in minutes. The steps in order were as below  Install a full Ruby development environment Install Jekyll and bundler gems for my user - gem install jekyll bundler --user-install Create a new site - jekyll new DeepuKSasidharan --skip-bundle, skipped the bundle install as I want to install to a vendor folder Cd into the folder DeepuKSasidharan and install gems to a vendor folder - bundle install --path vendor/bundle --full-index Start server - bundle exec jekyll serve and go to http://localhost:4000Using a Theme: Up next was setting up a custom theme, since I really like the minimal design of Medium, I decided to use Mediumish Jekyll Theme so I did the below steps to switch to this. Steps 3-5 above can be skipped and instead step 2 from the below can be done directly as well.  Delete the folder DeepuKSasidharan we created above Clone the theme to this folder - git clone https://github. com/wowthemesnet/mediumish-theme-jekyll. git DeepuKSasidharan Cd into the folder DeepuKSasidharan and install gems to a vendor folder - bundle install --path vendor/bundle --full-index Customize the _config. yaml file with my own user details, Google Analytics, Disqus ID and so on     I had to update the exclude section to add vendor/ to it and to . gitignore as well   Updated the jekyll-paginate plugin to jekyll-paginate-v2 in the plugins section   Commented out the baseurl section    Start server - bundle exec jekyll serve and go to http://localhost:4000Customizations: So now I had a good looking website with an about page and blog up and running. I customized the look and feel a bit and changed the default page from blogs to about. You can check the source code at deepu105/deepu105. github. io Now the next challenge was to make sure I can author once and post to both my blog and Dev. to, this means I have to make sure the front matter supported by Dev. to also works on my blog and any custom Liquid tags from Dev I use in the blog needs to work on my site as well. The first part was easy, I just had to customize my sites includes and layouts to use cover_image instead of image and use the tag: [] syntax for tags. I also added support for Dev. to like series and read time with a custom ruby plugin. Adding custom liquid tags: In order to use Dev. to tags, first I tried if I can reuse them from Dev since its OSS, but it seems like they are heavily coupled with Rails and internal models to be extracted into Gems. I created a ticket hoping this would happen eventually. So decided to write my own Liquid tags in Ruby. I reused available OSS Liquid tags and customized them to work like the Dev. to ones in syntax and feature. I ended up creating the codesandbox, twitter, gist, link, speakerdeck and youtube tags. You can find them here. Probably will add more as I use them. This is not scalable and I would love to see the Dev. to tags published as Ruby gems. For example, here is a simple stub for the youtube tag. 123456789101112131415161718192021module Jekyll   # A simple stub for the Dev. to youtube tag  class YoutubeTag &lt; Liquid::Tag   def initialize(name, id, tokens)    super    @id = id   end   def render(context)    %(&lt;p&gt;      &lt;div class= embed-video-container &gt;        &lt;iframe width= 710  height= 399  src= https://www. youtube. com/embed/#{@id}  allowfullscreen&gt;&lt;/iframe&gt;      &lt;/div&gt;    &lt;/p&gt;)   end  end end Liquid::Template. register_tag('youtube', Jekyll::YoutubeTag)Publishing to GitHub: Now that I have a site up and running with markdown posts that work in both my blog and Dev. to without having to make any adjustments, I decided to publish this to my Github accounts Github pages. But there was an issue here. Github doesn’t allow running any custom Ruby code on GitHub pages, so I can’t just push to GitHub and get the site built and published so I decided to write a simple script to do the site generation on my machine from the source branch and push it to the master branch on GitHub. 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bashrm -rf _siteif [ -z  $(git status --porcelain)  ]; then  echo  &gt;&gt;&gt; Working directory clean   TMP_LOC=/tmp/deepu. github. io  /bin/rm -rf _site || exit  /bin/rm -rf $TMP_LOC || exit  echo  &gt;&gt; Publish to Dev. to and update slugs   npm run publish-to-dev || exit  git add --all || exit  git commit --allow-empty -am  Updated posts with Dev. to slug  || exit  echo  &gt;&gt; Building site   bundle update listen || exit  bundle exec jekyll build || exit  echo  &gt;&gt; Move site to temp folder   mkdir --parents $TMP_LOC || exit  mv _site/* $TMP_LOC || exit  echo  &gt;&gt; Checkout and clean master   git checkout master || exit  find -mindepth 1 -depth -print0 | grep -vEzZ '(_drafts(/|$)|node_modules(/|$)|temp(/|$)|vendor(/|$)|. git(/|$)|/. gitignore$)' | xargs -0 rm -rvf || exit  echo  &gt;&gt; Move site form temp &amp; publish to GitHub   mv $TMP_LOC/* . || exit  now=$(date)  git add --all || exit  git commit -am  Updated site on $now  || exit  git push origin master --force || exit  echo  &gt;&gt; $now: Published changes to GitHub   git checkout site_srcelse  echo  &gt;&gt; Working directory is not clean. Commit changes!   exitfiMy current workflow: I also wrote a small script to automatically publish or update posts to Dev. to as well using their API. Here is the script. So now that I have things in place, I author posts as markdown with a full front matter like below and publish on my blog and the script automatically cross-post the same to Dev. to as well. 12345678---title:  Static Site Generators rundown - How I set up my own blog with Jekyll published: falsedescription: Static Site Generators comparisontags: [showdev, ruby, Jekyll, blogging]cover_image:canonical_url: https://deepu. tech/setting-up-a-blog-with-jekyll/---I’m not using the RSS import option in Dev as it uses the rendered blog and hence might need adjustments. I also set the canonical_url to my blog site. When I update a post the same script above takes care of updating it on my site and Dev. to as well so both are always kept in sync. Future plans: There are some things that can be improved.  Use the Dev. to API to publish this direct from my publish script when I author a new post or make updates to an existing one. Update: This is done Improve the link tag and add some more tags for GitHub. Update: This is partially done Use local assets image for my own blog and generate the image URL for Dev. to when publishing.  Currently, all links point to Dev. to, make the link tag smart enough to point to my blog when published to my site(I don’t want my readers to switch between sites). This might be a bit hard since Dev. to links have a random suffix. Update: This is doneSo what do you think? If you have any suggestions on improvements or questions leave a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Photo by Patrick Fore on Unsplash "},{id:59,url:"https://deepu.tech/functional-programming-in-java-for-beginners/",title:"7 Functional programming techniques in Java - A primer",body:"2019/07/30 - There is a lot of hype around functional programming(FP) and a lot of cool kids are doing it but it is not a silver bullet. Like other programming paradigms/styles, functional programming also has its pros and cons and one may prefer one paradigm over the other. If you are a Java developer and wants to venture into functional programming, do not worry, you don’t have to learn functional programming oriented languages like Haskell or Clojure(or even Scala or JavaScript though they are not pure functional programming languages) since Java has you covered and this post is for you. I’m not gonna dive into all functional programming concepts in detail, instead, I’m gonna focus on things that you can do in Java which are in line with functional programming concepts. I’m also not gonna discuss the pros and cons of functional programming in general. What is functional programming?: As per Wikipedia,  Functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. Hence in functional programming, there are two very important rules  No Data mutations: It means a data object should not be changed after it is created.  No implicit state: Hidden/Implicit state should be avoided. In functional programming state is not eliminated, instead, its made visible and explicitThis means:  No side effects: A function or operation should not change any state outside of its functional scope. I. e, A function should only return a value to the invoker and should not affect any external state. This means programs are easier to understand.  Pure functions only: Functional code is idempotent. A function should return values only based on the arguments passed and should not affect(side-effect) or depend on global state. Such functions always produce the same result for the same arguments. Apart from these there are functional programming concepts below that can be applied in Java, we will touch upon these further down.  Higher-order-functions Closures Currying Recursion Lazy evaluations Referential transparencyUsing functional programming doesn’t mean its all or nothing, you can always use functional programming concepts to complement Object-oriented concepts, especially in Java. The benefits of functional programming can be utilized whenever possible regardless of the paradigm or language you use. And that is exactly what we are going to see. Functional programming in Java: So let us see how we can apply some of the functional programming concepts above in Java. We will be using Java 11 as it is the LTS version currently. First-class and higher-order functions: First-class functions(function as a first-class citizen) means you can assign functions to variables, pass a function as an argument to another function or return a function from another. Unfortunately, Java doesn’t support this and hence makes concepts like closures, currying and higher-order-functions less convenient to write. The closest to first-class functions in Java is Lambda expressions. There are also some built-in functional interfaces like Function, Consumer, Predicate, Supplier and so on under the java. util. function package which can be used for functional programming. A function can be considered as a higher-order-function only if it takes one or more functions as parameters or if it returns another function as a result. The closest to higher-order-functions we can get in Java is using Lambda expressions and built-in Functional interfaces. This is not the nicest looking way of doing higher-order-functions, but this is how it is in Java and its not that bad IMO. 12345678910111213141516171819202122232425262728public class HocSample {  public static void main(String[] args) {    var list = Arrays. asList( Orange ,  Apple ,  Banana ,  Grape );    // we are passing an array and an anonymous inner class instance of FnFactory as arguments to mapForEach method.     var out = mapForEach(list, new FnFactory&lt;String, Object&gt;() {      @Override      public Object execute(final String it) {        return it. length();      }    });    System. out. println(out); // [6, 5, 6, 5]  }  // The method takes an array and an instance of FnFactory as arguments  static &lt;T, S&gt; ArrayList&lt;S&gt; mapForEach(List&lt;T&gt; arr, FnFactory&lt;T, S&gt; fn) {    var newArray = new ArrayList&lt;S&gt;();    // We are executing the method from the FnFactory instance    arr. forEach(t -&gt; newArray. add(fn. execute(t)));    return newArray;  }  @FunctionalInterface // this doesn't do anything it is just informative.   public interface FnFactory&lt;T, S&gt; {    // The interface defines the contract for the anonymous class    S execute(T it);  }}Fortunately, can actually simplify the above example further using the built-in Function interface and using the lambda expression syntax. 1234567891011121314151617public class HocSample {  public static void main(String[] args) {    var list = Arrays. asList( Orange ,  Apple ,  Banana ,  Grape );    // we are passing the array and a lambda expression as arguments to mapForEach method.     var out = mapForEach(list, it -&gt; it. length());    // This can be further simplified to  mapForEach(list, String::length); , I'm writing the expanded version for readability    System. out. println(out); // [6, 5, 6, 5]  }  // The method takes an array and an instance of Function as arguments (we have replaced the custom interface with the built-in one)  static &lt;T, S&gt; ArrayList&lt;S&gt; mapForEach(List&lt;T&gt; arr, Function&lt;T, S&gt; fn) {    var newArray = new ArrayList&lt;S&gt;();    // We are executing the method from the Function instance    arr. forEach(t -&gt; newArray. add(fn. apply(t)));    return newArray;  }}Using these concepts along with lambda expressions we can write closures and currying like below 1234567891011121314151617181920212223242526272829public class ClosureSample {  // this is a higher-order-function that returns an instance of Function interface  Function&lt;Integer, Integer&gt; add(final int x) {    // this is a closure, i. e, a variable holding an anonymous inner class instance of the Function interface    // which uses variables from the outer scope    var partial = new Function&lt;Integer, Integer&gt;() {      @Override      public Integer apply(Integer y) {        // variable x is obtained from the outer scope of this method which is declared as final        return x + y;      }    };    // The closure function instance is returned here    return partial;  }  public static void main(String[] args) {    ClosureSample sample = new ClosureSample();    // we are currying the add method to create more variations    var add10 = sample. add(10);    var add20 = sample. add(20);    var add30 = sample. add(30);    System. out. println(add10. apply(5)); // 15    System. out. println(add20. apply(5)); // 25    System. out. println(add30. apply(5)); // 35  }}We can simplify this further with lambda expressions like below 123456789101112131415161718192021public class ClosureSample {  // this is a higher-order-function that returns an instance of Function interface  Function&lt;Integer, Integer&gt; add(final int x) {    // The lambda expression is returned here as closure    // variable x is obtained from the outer scope of this method which is declared as final    return y -&gt; x + y;  }  public static void main(String[] args) {    ClosureSample sample = new ClosureSample();    // we are currying the add method to create more variations    var add10 = sample. add(10);    var add20 = sample. add(20);    var add30 = sample. add(30);    System. out. println(add10. apply(5));    System. out. println(add20. apply(5));    System. out. println(add30. apply(5));  }}There are also many built-in higher-order-functions in Java for example here is the sort method from java. util. Collections 12345678var list = Arrays. asList( Apple ,  Orange ,  Banana ,  Grape );// This can be simplified as  Collections. sort(list, Comparator. naturalOrder()); , I'm writing the expanded version for readabilityCollections. sort(list, (String a, String b) -&gt; {  return a. compareTo(b);});System. out. println(list); // [Apple, Banana, Grape, Orange]The Java stream API also provides many interesting higher-order-functions like forEach, map and so on. Pure functions: As we saw already a pure function should return values only based on the arguments passed and should not affect or depend on global state. It is possible to do this in Java except for some cases when there are checked exceptions involved. This is quite simple, take the below this is a pure function. It will always return the same output for the given input and its behavior is highly predictable. We can safely cache the method if needed. 123public static int sum(int a, int b) {  return a + b;}If we add an extra line in this function, the behavior becomes unpredictable as it now has a side effect that affects an external state. 1234567static Map map = new HashMap&lt;String, Integer&gt;();public static int sum(int a, int b) {  var c = a + b;  map. put(a +  +  + b, c);  return c;}So try to keep your functions pure and simple. Recursion: Functional programming favors recursion over looping. In Java, this can be achieved either by using the stream API or by writing recursive functions. Let us see an example for calculating the factorial of a number. I also ran a benchmark on these using JMH and mentioned the nanoseconds/operation below In traditional iterative approach: 1234567891011121314public class FactorialSample {  // benchmark 9. 645 ns/op  static long factorial(long num) {    long result = 1;    for (; num &gt; 0; num--) {      result *= num;    }    return result;  }  public static void main(String[] args) {    System. out. println(factorial(20)); // 2432902008176640000  }}The same can be done using recursion as below which is favored in functional programming. 12345678910public class FactorialSample {  // benchmark 19. 567 ns/op  static long factorialRec(long num) {    return num == 1 ? 1 : num * factorialRec(num - 1);  }  public static void main(String[] args) {    System. out. println(factorialRec(20)); // 2432902008176640000  }}The downside of the recursive approach is that it will be slower compared to an iterative approach most of the times(The advantage we are aiming for is code simplicity and readability) and might result in stack overflow errors since every function call needs to be saved as a frame to the stack. To avoid this tail recursion is preferred, especially when the recursion is done too many times. In tail recursion, the recursive call is the last thing executed by the function and hence the functions stack frame need not be saved by the compiler. Most compilers can optimize the tail recursion code the same way iterative code is optimized hence avoiding the performance penalty. Java compiler, unfortunately, does not do this optimization :( Now using tail recursion the same function can be written as below, but Java doesn’t optimize this, though there are workarounds, still it performed better in benchmarks. 1234567891011121314public class FactorialSample {  // benchmark 16. 701 ns/op  static long factorialTailRec(long num) {    return factorial(1, num);  }  static long factorial(long accumulator, long val) {    return val == 1 ? accumulator : factorial(accumulator * val, val - 1);  }  public static void main(String[] args) {    System. out. println(factorialTailRec(20)); // 2432902008176640000  }}We can also use the Java stream library for recursion but its slower than normal recursion at the moment. 1234567891011public class FactorialSample {  // benchmark 59. 565 ns/op  static long factorialStream(long num) {    return LongStream. rangeClosed(1, num)        . reduce(1, (n1, n2) -&gt; n1 * n2);  }  public static void main(String[] args) {    System. out. println(factorialStream(20)); // 2432902008176640000  }}Consider using stream API or recursion when writing Java code for readability and immutability, but if performance is critical or if the number of iterations will be huge use standard loops. Lazy evaluation: Lazy evaluation or non-strict evaluation is the process of delaying evaluation of an expression until it is needed. In general, Java does strict evaluation but for operands like &amp;&amp;, || and ?: it does a lazy evaluation. We can utilize this to do lazy evaluations when writing java code. Take this example where Java eagerly evaluates everything. 1234567891011121314151617181920public class EagerSample {  public static void main(String[] args) {    System. out. println(addOrMultiply(true, add(4), multiply(4))); // 8    System. out. println(addOrMultiply(false, add(4), multiply(4))); // 16  }  static int add(int x) {    System. out. println( executing add ); // this is printed since the functions are evaluated first    return x + x;  }  static int multiply(int x) {    System. out. println( executing multiply ); // this is printed since the functions are evaluated first    return x * x;  }  static int addOrMultiply(boolean add, int onAdd, int onMultiply) {    return (add) ? onAdd : onMultiply;  }}This will produce the below output and we can see that both functions are executed always 123456executing addexecuting multiply8executing addexecuting multiply16We can use lambda expressions and higher-order-functions to rewrite this into a lazily evaluated version 1234567891011121314151617181920212223242526public class LazySample {  public static void main(String[] args) {    // This is a lambda expression behaving as a closure    UnaryOperator&lt;Integer&gt; add = t -&gt; {      System. out. println( executing add );      return t + t;    };    // This is a lambda expression behaving as a closure    UnaryOperator&lt;Integer&gt; multiply = t -&gt; {      System. out. println( executing multiply );      return t * t;    };    // Lambda closures are passed instead of plain functions    System. out. println(addOrMultiply(true, add, multiply, 4));    System. out. println(addOrMultiply(false, add, multiply, 4));  }  // This is a higher-order-function  static &lt;T, R&gt; R addOrMultiply(      boolean add, Function&lt;T, R&gt; onAdd,      Function&lt;T, R&gt; onMultiply, T t  ) {    // Java evaluates expressions on ?: lazily hence only the required method is executed    return (add ? onAdd. apply(t) : onMultiply. apply(t));  }}This outputs the below and we can see that only required functions were executed 1234executing add8executing multiply16Type system: Java has a strong type system and with the introduction of the var keyword it now also has pretty decent type inference. The only thing missing compared to other functional programming languages are case classes. There are proposals for value classes and case classes for future Java versions. Let’s hope they make it. Referential transparency: From Wikipedia:  Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent. Unfortunately, there are not many ways to limit data mutation in Java, however by using pure functions and by explicitly avoiding data mutations and reassignment using other concepts we saw earlier this can be achieved. For variables, we can use the final keyword which is a non-access modifier to avoid mutations by reassignments. For example, the below will produce an error at compilation 123final var list = Arrays. asList( Apple ,  Orange ,  Banana ,  Grape );list = Arrays. asList( Earth ,  Saturn );But this will not help when variables are holding references to other objects, for example, the below mutation will work irrespective of the final keyword. 1234final var list = new ArrayList&lt;&gt;();list. add( Test );list. add( Test 2 );final keyword allows the internal state of referenced variables to be mutated and hence from a functional programming perspective final keyword is useful only for constants and to catch reassignments. Data structures: When using functional programming techniques it is encouraged to use functional data types such as Stacks, Maps and Queues. Hence maps are better than arrays or hash sets in functional programming as data stores. Conclusion: This is just an introduction for those who are trying to apply some functional programming techniques in Java. There are lot more that can be done in Java and Java 8 added a lot of API to make it easy to do functional programming in Java, like the stream API, Optional interface, functional interfaces and so on. As I said earlier functional programming is not a silver bullet but it offers a lot of useful techniques for more understandable, maintainable and testable code. It can co-exist perfectly well with imperative and object-oriented programming styles. In fact, we all should be using the best of everything. Here is a video from a meetup, I presented, covering this content.                          I hope you find this useful. If you have any question or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:60,url:"https://deepu.tech/jhipster-microservices-with-istio-service-mesh-on-kubernetes/",title:"How to set up Java microservices with Istio service mesh on Kubernetes",body:"2019/07/23 - Originally published at Medium on 17-Nov-2018. This post has been updated on Jan-2020 to work with the latest version of JHipster(7. 0. 0-beta. 1) and Istio(1. 8. 2). Istio is the coolest kid on the DevOps and Cloud block now. For those of you who aren’t following close enough — Istio is a service mesh for distributed application architectures, especially the ones that you run on the cloud with Kubernetes. Istio plays extremely nice with Kubernetes, so nice that you might think that it’s part of the Kubernetes platform. Platforms like Google Cloud even provide it out of the box when you use the Google Kubernetes Engine. If you are still wondering, what the heck is a service mesh or Istio? then let’s have an overview of Istio. Istio: Istio provides the following functionality in a distributed application architecture:    Service discovery — Traditionally provided by platforms like Netflix Eureka or Consul.     Automatic load balancing — You might have used Netflix Zuul for this.     Routing, circuit breaking, retries, fail-overs, fault injection — Think of Netflix Ribbon, Hytrix and so on.     Policy enforcement for access control, rate limiting, A/B testing, traffic splits, and quotas — Again you might have used Zuul to do some of these.     Metrics, logs, and traces — Think of ELK or Stack driver     Secure service-to-service communication  Below is the architecture of Istio. Istio architecture It can be classified into 2 distinct planes. Data plane: Is made of Envoy proxies deployed as sidecars to the application containers. They control all the incoming and outgoing traffic to the container. Control plane: It uses Pilot to manages and configure the proxies to route traffic. It also configures Mixer to enforce policies and to collect telemetry. It also has other components like Citadel, to manage security, and Galley, to manage configurations. You can also configure Grafana, Prometheus, Jaeger/Zipkin, and Kiali for Monitoring and Observability as they work well with Istio. You can use this or use your existing monitoring stack as well if you wish to do so. If you use the demo profile that comes with Istio, it preconfigures dashboards and configurations for Grafana, Kiali, and so on. I hope this provides an overview of Istio, now let’s focus on the goal of this article. Preparing the Kubernetes cluster: First, let us prepare a Kubernetes cluster to deploy Istio and our application containers. Follow the instructions for any one of the platforms you prefer. Prerequisites: We will be using istioctl to install Istio on the Kubernetes cluster and kubectl for deploying the applications. kubectl: The command-line tool to interact with Kubernetes. Install and configure it. Create a cluster on Azure Kubernetes Service(AKS): If you are going to use Azure, then install Azure CLI to interact with Azure. Install and login with your Azure account (you can create a free account if you don’t have one already). If not skip this section. First, let us create a resource group. You can use any region you like here instead of East-US. 1$ az group create --name eCommerceCluster --location eastusCreate the Kubernetes cluster: 1234567$ az aks create  --resource-group eCommerceCluster  --name eCommerceCluster  --node-count 4  --kubernetes-version 1. 15  --enable-addons monitoring  --generate-ssh-keysThe node-count flag is important as the setup requires at least four nodes with the default CPU to run everything. You can try to use a higher kubernetes-version if it is supported, else stick to 1. 15 The cluster creation could take a while so sit back and relax. 🍹 Once the cluster is created, fetch its credentials to be used from kubectl by running the below command. It automatically injects the credentials to your kubectl configuration under ~/. kube/config 123$ az aks get-credentials  --resource-group eCommerceCluster  --name eCommerceClusterYou can view the created cluster in the Azure portal: Kubernetes cluster in AKS Run kubectl get nodes to see it in the command line and to verify that kubectl can connect to your cluster. Cluster Nodes Proceed to the Install and setup Istio section. Create a cluster on Google Kubernetes Engine(GKE): If you are going to use Google Cloud Platform(GCP) then install Gcloud CLI to interact with GCP. Install and login with your GCP account (you can create a free account if you don’t have one already). You can set a region and zone using the below commands or you can pass the zone option while executing each command. 12$ gcloud config set compute/region europe-west1$ gcloud config set compute/zone europe-west1-bFirst, we need a GCP project, you can either use an existing project that you have or create a new one using GCloud CLI with the below command: 1$ gcloud projects create jhipster-demo-deepuSet the project you want to use as the default project and enable the container API. You also need to enable billing for the project via the Google Cloud Console 12$ gcloud config set project jhipster-demo-deepu$ gcloud services enable container. googleapis. comNow let us create a cluster for our application with the below command: 1234$ gcloud container clusters create hello-hipster   --cluster-version latest   --num-nodes 4   --machine-type n1-standard-2The num-nodes and machine-type flags are important as the setup requires at least four nodes with a bigger CPU to run everything. You can try to use the latest cluster-version if it is supported, else stick to 1. 17. The cluster creation could take a while so sit back and relax. 🍹 Once the cluster is created, fetch its credentials to be used from kubectl by running the below command. It automatically injects the credentials to your kubectl configuration under ~/. kube/config 1$ gcloud container clusters get-credentials hello-hipsterYou can view the created cluster in the GCP GUI. Kubernetes cluster on GKE Run kubectl get nodes to see it in the command line and to verify that kubectl can connect to your cluster. Cluster Nodes Install and setup Istio: Install Istio on your local machine by following these steps: 123456789$ cd ~/$ export ISTIO_VERSION=1. 8. 2$ curl -L https://istio. io/downloadIstio | sh -$ ln -sf istio-$ISTIO_VERSION istio$ export PATH=~/istio/bin:$PATHNow let us install Istio on our Kubernetes cluster using the provided charts from Istio. 123456789# Install the Istio CRDs and components from the Demo profile$ istioctl install --set profile=demo -y# Install Addons like Grfana, Prometheus, Kiali and Zipkin$ cd ~/istio$ kubectl apply -f samples/addons/grafana. yaml$ kubectl apply -f samples/addons/prometheus. yaml$ kubectl apply -f samples/addons/kiali. yaml$ kubectl apply -f samples/addons/extras/zipkin. yamlWait for the pods to run, these will be deployed to the istio-system namespace. 1$ watch kubectl get pods -n istio-systemOnce the pods are in running status, exit the watch loop and run the below to get the Ingress gateway service details. This is the only service that is exposed to an external IP. 1234$ kubectl get svc istio-ingressgateway -n istio-systemNAME          TYPE      CLUSTER-IP   EXTERNAL-IPistio-ingressgateway  LoadBalancer  10. 27. 249. 83  35. 195. 81. 130If the istio-ingressgateway shows external IP as , wait a few minutes until an IP address has been assigned. The external IP is very important here, let us save this to an environment variable so that we can use it in further commands. 1234$ export  INGRESS_IP=$(kubectl -n istio-system get svc  istio-ingressgateway  -o jsonpath='{. status. loadBalancer. ingress[0]. ip}')There is an additional step that you need to do here for Istio. The automatically created firewall rule does not open port 15017. This Pilot discovery validation webhook needs this so we need to add that. First, fetch the firewall rules added by Istio for the project by running the below command. If there is more than one, just pick anyone. If you can’t find any run the command without filter and look for a rule with Istio in the description. 1$ gcloud compute firewall-rules list --filter= name~k8s-fw-[0-9a-z]*  --format=jsonNow update this rule to add the needed port using the command below, replace the name with the actual name of the rule you fetched above, and add all the original ports to the list 1gcloud compute firewall-rules update &lt;firewall rule name&gt; --allow tcp:15017,tcp:80,tcp:443,&lt;append all other protocol:port comma separated&gt;Now our Kubernetes cluster is ready for Istio. 🎉 For advanced Istio setup options refer to https://istio. io/docs/setup/kubernetes/ Creating the microservice application stack: In one of my previous posts, I showcased how to create a full-stack microservice architecture using JHipster and JDL. You can read the post here if you want to learn more details about it. For this exercise, we will use the same application but we will not use the Eureka service discovery option we used earlier. Also, note that the store application is further split into Gateway and Product applications. Architecture: Here is the architecture of the microservice that we are going to create and deploy today. Microservice architecture with Istio It has a gateway application and three microservice applications. Each of them has its own database. You can see that each application has an Envoy proxy attached to the pod as a sidecar. Istio control plane components are also deployed to the same cluster along with Prometheus, Grafana, and Jaeger. The Ingress gateway from Istio is the only entry point for traffic and it routes traffic to all microservices accordingly. Telemetry is collected from all the containers running in the cluster, including the applications, databases, and Istio components. Compared to the architecture of the original application here, you can clearly see that we replaced the JHipster registry and Netflix OSS components with Istio. The ELK monitoring stack is replaced with Prometheus, Grafana and Jaeger configured by Istio. Here is the original architecture diagram without Istio for a quick visual comparison. Microservice architecture with Netflix OSS Application JDL: Let’s take a look at the modified JDL declaration. You can see that we have declared serviceDiscoveryType no here since we will be using Istio for that. 400: Invalid requestDeployment JDL: JHipster version 5. 7. 0 introduced support for deployment declaration straight in the JDL Towards the future of #JHipster, #ScaffoldingAsCodeCheck this out. Soon you will be able to define apps, entities and deployment options with a single JDL file and generate everything with a single command. Hopefully, we can demo this for @Devoxx https://t. co/u28cwaymfc &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) October 28, 2018We have the below in our JDL which declares our Kubernetes deployment: 12345678910deployment { deploymentType kubernetes appsFolders [store, invoice, notification, product] dockerRepositoryName  deepu105  serviceDiscoveryType no istio true kubernetesServiceType Ingress kubernetesNamespace jhipster ingressDomain  35. 195. 81. 130. nip. io }The serviceDiscoveryType is disabled and we have enabled Istio support — the Envoy sidecars are injected automatically for the selected applications. Istio routes are also generated for the applications automatically. The kubernetesServiceType is set as Ingress, which is very important as Istio can only work with an Ingress controller service type. For Ingress, we need to set the domain DNS and this is where the Istio ingress gateway IP is needed. Now we need a DNS for our IP. For real use-cases, you should map a DNS for the IP, but for testing and demo purposes we can use a wildcard DNS service like nip. io to resolve our IP. Just append nip. io to our IP and use that as the ingressDomain. Note: I was switching between multiple clusters while writing this article as I didn’t want to keep them running and hence my istio-ingressgateway IP might be different between samples and screenshots. Use the IP based on your own setup if you are running these samples. Generate the applications and deployment manifests: Now that our JDL is ready, let us scaffold our applications and Kubernetes manifests. Create a new directory and save the above JDL in the directory. Let us name it app-istio. jdl and then run the jdl command. 12$ mkdir istio-demo &amp;&amp; cd istio-demo$ jhipster jdl app-istio. jdl --forkThis will generate all the applications and install the required NPM dependencies in each of them. Once the applications are generated the deployment manifests will be generated and some useful instruction will be printed to the console. Generation output Open the generated code in your favorite IDE/Editor and explore the code. Interim issues with generated code: There was a bug in the latest JHipster version which creates some incorrect URLs for Istio, it has been fixed as of JHipster version 6. 3. 0 here is the PR for the issue. Deploy to Kubernetes cluster using Kubectl: Now let us build and deploy our applications. Run the . /gradlew bootJar -Pprod jibDockerBuild command in the store, product, invoice, and notification folders to build the docker images. Once the images are built, push them to your docker repo with these commands. Note to change the Docker hub id from deepu105 to your id. 1234567891011$ docker image tag store deepu105/store$ docker push deepu105/store$ docker image tag invoice deepu105/invoice$ docker push deepu105/invoice$ docker image tag notification deepu105/notification$ docker push deepu105/notification$ docker image tag product deepu105/product$ docker push deepu105/productOnce the images are pushed, navigate into the generated Kubernetes directory and run the provided startup script. (If you are on windows you can run the steps in kubectl-apply. sh manually one by one. ) 12$ cd kubernetes$ . /kubectl-apply. sh -fRun watch kubectl get pods -n jhipster to monitor the status. Deployed applications: Once all the pods are in running status we can explore the deployed applications Application gateway: The store gateway application is the entry point for our microservices. Get the URL for the store app by running echo store. jhipster. $INGRESS_IP. nip. io, we already stored the INGRESS_IP to environment variables while creating the Istio setup. The URLs are also printed on the console by the kubectl-apply. sh script. Visit the URL in your favorite browser and explore the application. Try creating some entities for the microservices: Store gateway application Monitoring: Istio setup includes Grafana and Prometheus configured to collect and show metrics from our containers. Let’s take a look. Let us look at Grafana by visiting the provided URL. Get it by running istioctl dashboard grafana: Grafana dashboard for the Store application Grafana uses the metrics scraped by Prometheus. You can view the Prometheus instance by running istioctl dashboard prometheus Prometheus dashboard Observability: Istio configures Zipkin for distributed tracing and Kiali for service observability. Let us take a look at them. Get the Zipkin URL by running istioctl dashboard zipkin: Zipkin tracing dashboard You can make some requests in the application and find them in the tracing dashboard by querying for the service. Click on any request to see tracing details. Let us now look at Kiali. Get the URL by running istioctl dashboard kiali, use the credentials user: admin, password: admin to log in: Kiali service graph Conclusion: Istio provides building blocks to build distributed microservices in a more Kubernetes-native way and takes the complexity and responsibility of maintaining those blocks away from you. This means you do not have to worry about maintaining the code or deployments for service discovery, tracing, and so on. Istio documentation says  Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, etc. , in a consistent way across the services, for the application as a whole. Werner Vogels (CTO of AWS) quoted at AWS Re:Invent  “In the future, all the code you ever write will be business logic. ” Istio Service mesh helps to make that reality closer. This lets you worry only about the applications that you are developing and with JHipster that future is truly here and you just need to worry about writing your business logic. While this is great, it is not a silver bullet. Keep in mind that Istio is fairly new compared to other stable and battle-tested solutions like JHipster Registry (Eureka) or Consul and overall such architectures are suitable only for complex distributed applications. Also, another thing to keep in mind is the resource requirements. The same microservices with JHipster Registry or Consul can be deployed to a 2 node cluster with 1 vCPU and 3. 75 GB of memory per node in GCP while you need a 4 node cluster with 2 vCPUs and 7. 5 GB of memory per node for Istio enabled deployments. The demo profile from Istio, we used, doesn’t apply any request limits for resources, and by adding and tuning those, the minimum requirement could be reduced. But still, I don’t think you can get it as low as that is needed for the JHipster registry option. In a real-world use case, the advantages of not having to maintain the complex parts of your infra vs having to pay for more resources might be a decision that has to be taken based on your priorities and goals. A huge shout out to Ray Tsang for helping me figure out an optimal cluster size for this application originally. Also a huge thank you from myself and the community to both Ray and Srinivasa Vasu for adding the Istio support to JHipster. JHipster provides a great Kubernetes setup to start with which you can further tweak as per your needs and platform. The Istio support will improve further over time, but it’s still a great starting point especially to learn. To learn more about JHipster and Full stack development, check out my book “Full Stack Development with JHipster” on Amazon and Packt. There is a great Istio tutorial from Ray Tsang here. If you liked this article you might like my book as well. You can get it from Packt and Amazon.  Devoxx 2018: I did a talk at Devoxx 2018 along with Julien Dubois doing the same demo and promised that I’d write a detailed blog about it. This blog was originally based on that. Had a wonderful time at #devoxx2018, sad that I had to leave early due to work commitments. Had 3 talks and 1 of them with my friend and the real hipster @juliendubois, all the talks were well received and I&#39;m grateful for all the positive feedback. Will write blogs about it soon &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) November 15, 2018You can watch this video to see JHipster + Istio in action.                          Here are the slides on Speaker Deck.                                                 If you like JHipster don’t forget to give it a star on Github. If you like this article, please leave likes/comments. I hope to write more about Istio soon. You can follow me on Twitter and LinkedIn. "},{id:61,url:"https://deepu.tech/make-the-most-out-of-vscode/",title:"My VS Code setup - Making the most out of VS Code",body:"2019/07/17 - Visual Studio Code(I like the sound of VS Code better), I just love it. It is my primary code editor. I always loved lightweight editors over IDEs. Many years ago I was using Eclipse for development and Notepad++ with some plugins for all other lightweight stuff. Then I discovered sublime text and was using it for a while. I still was finding Eclipse too heavyweight when I was doing web development. Then came Brackets from Adobe. It was a fairly nice editor especially for web development and I started using it heavily for web development. But Brackets was bit slow back then on a large codebase. Then came Atom which revolutionized the NodeJS desktop application landscape by introducing the Atom shell which ultimately became Electron. So I switched to Atom and loved its slick interface and nice pluggable features. It became my primary editor for all web development. So Electron paved the way for VS Code and though at first, I was skeptical due to the association with Visual Studio, I tried it out and was amazed by its speed and user experience. There was no turning back now. I slowly started using VS Code for most of my day to day development, except for JVM development which I was using IntelliJ Ultimate by now and it is hard to beat IntelliJ for JVM development. Fast forward now below are the editor/IDE I use for development.  VS Code: JavaScript, TypeScript, EJS, HTML, CSS, Golang, Rust, Python, Ruby, Shell, Docker, Kubernetes, Terraform and everything in between including writing this blog post.  IntelliJ Idea: Java, Scala, Kotlin, Groovy VIM: For quick edits from the command line. PluginsOf course VS Code makes all this possible by allowing the use of plugins and there is a lot to choose from. Here are the plugins that I personally use to work on the above-said languages. You can use the code --install-extension command to install them from the terminal. Language support: Based on the Languages you work with you can add syntax, utility and language support plugins for those. I use the below JavaScript/TypeScript/Web:  EJS language support - Adds EJS template support.      code --install-extension DigitalBrainstem. javascript-ejs-support     Close HTML/XML tag - Auto close HTML/XML tags.      code --install-extension Compulim. compulim-vscode-closetag     ESLint - Adds support for ESLint rules.      code --install-extension dbaeumer. vscode-eslint     TSLint - Adds support for TSLint rules.      code --install-extension ms-vscode. vscode-typescript-tslint-plugin     Prettier - Adds support for Prettier formatter.      code --install-extension esbenp. prettier-vscode     es-beautifier - Formats JS according to Eslint rules.      code --install-extension dai-shi. vscode-es-beautifier    Go:  Go - Adds rich language support for Golang.      code --install-extension ms-vscode. Go    JVM:  Language Support for Java - Adds Java language support.      code --install-extension redhat. java     Debugger for Java - Adds lightweight Java debugging support.      code --install-extension vscjava. vscode-java-debug     JHipster JDL - Adds syntax support for JHipster JDL files.      code --install-extension jhipster-ide. jdl    The Java support indeed is getting better and better, so I hope one day I can completely switch to VS Code. Announcing the Visual Studio Code Installer for #Java https://t. co/u6lyKW0xFS &mdash; Markus Eisele (@myfear) June 16, 2019Python:  Language Support for Python - Adds Python language support, linting and debugging support.      code --install-extension ms-python. python    Cloud, Container &amp; others:    Docker - Adds Docker support(view and manage containers) and support for Docker, docker-compose files.      code --install-extension ms-azuretools. vscode-docker       Jenkinsfile Support - Adds syntax highlighting support for Jenkinsfile’s.      code --install-extension secanis. jenkinsfile-support       Terraform - Adds support for Terraform files.      code --install-extension mauve. terraform       Markdown all in one - Full markdown support with live preview, keyboard shortcuts, etc.      code --install-extension yzhang. markdown-all-in-one       PlantUML - Rich PlantUML support with live preview.      code --install-extension jebbs. plantuml       Visual Studio IntelliCode - Adds AI assisted intellisense support for multiple languages.      code --install-extension VisualStudioExptTeam. vscodeintellicode       YAML - Adds YAML support.      code --install-extension redhat. vscode-yaml    Theme: Dark++ Italic: My default theme. Similar to VS Code default dark theme but has support for FiraCode and Operator Mono fonts. I personally use FiraCode.  code --install-extension idbartosz. darkpp-italic Material icon theme: A nice icon theme based on material icons.  code --install-extension PKief. material-icon-theme Peacock: Subtly changes the workspace color of your workspace. Helpful to identify when you have many windows open.  code --install-extension johnpapa. vscode-peacock Tools: Auto rename tag: Automatically rename paired HTML/XML tags  code --install-extension formulahendry. auto-rename-tag Bracket pair colorizer 2: Marks matching bracket pairs with unique colors. This really makes reading code nicer  code --install-extension CoenraadS. bracket-pair-colorizer-2 Change case: Convert between different cases. Trust me this is so handy  code --install-extension wmaurer. change-case Code spell checker: Fairly useful for spell checking within code. Takes cameCase etc into account  code --install-extension streetsidesoftware. code-spell-checker Easy snippet maker: Useful to store re usable snippets.  code --install-extension tariky. easy-snippet-maker EditorConfig for VS Code: Add support for EditorConfig.  code --install-extension EditorConfig. EditorConfig Git History: Enable viewing Git history within VS Code.  code --install-extension donjayamanne. githistory Gitignore: Makes it easy to work with . gitignore files.  code --install-extension codezombiech. gitignore Hide gitignored: Hides patterns defined in . gitignore from the editors explorer.  code --install-extension npxms. hide-gitignored Mark as excluded: Exclude stuff right from the explorer tree.  code --install-extension jcmordan. mark-as-excluded Toggle Excluded Files: Easily toggle between showing and hiding excluded files/folders.  code --install-extension eamodio. toggle-excluded-files IntelliJ IDEA Keybindings: I have bad muscle memory so wanted to use the same keyboard shortcuts as IntelliJ. There are mappings available for Sublime, Atom and so on.  code --install-extension k--kato. intellij-idea-keybindings Sort JSON: Sorts JSON object keys.  code --install-extension richie5um2. vscode-sort-json Test Explorer UI: Adds an explorer panel for running tests. Supports multiple languages and testing frameworks.  code --install-extension hbenl. vscode-test-explorer Todo Tree: Aggregate TODO, FIXME, etc in a tree view in explorer.  code --install-extension Gruntfuggly. todo-tree Terminal setupIf you are using Zsh shell with Oh-my-zsh like me as explained here, you might want to do the below to get the same terminal experience in the integrated VSCode terminal as well.  Follow these steps  Download and install a patched font.  On Linux, run fc-cache -f -v to refresh font cache.  On VSCode, open Preferences → Settings and click on the {} icon to open JSON mode and set the below123456   terminal. integrated. shell. linux :  /usr/bin/zsh ,   terminal. integrated. fontFamily :  'SauceCodePro Nerd Font Mono','Source Code Pro' ,   terminal. integrated. rightClickCopyPaste : true,   terminal. integrated. fontSize : 14,   terminal. integrated. cursorStyle :  underline ,   terminal. integrated. cursorBlinking : trueReplace linux with osx if you are on a Mac. ConclusionThis might seem like too many plugins but on my configuration VS Code is lightning fast and loads up immediately and is faster then IntelliJ to load and work with. The beauty of VS Code is that you don’t need all the plugin all the time, you can disable the ones not required per workspace to make it even faster. Many people ask me why I use VS Code when I have IntelliJ and my answer have been always the same. IntelliJ is great but its also quite heavy. While all those advanced features are needed for Java, Scala or Kotlin development, VS Code is perfectly capable of giving a nice developer experience for lightweight languages like JS, TS, Go, Python, Rust, Ruby, etc. As a regular user of both IntelliJ and VS Code, I prefer VS Code as much as possible. The user experience is much nicer for my taste. In fact, I like the developer experience in VS Code better for JavaScript, TypeScript, Web, Python, and Golang. Also switching between them for JVM projects and others don’t feel weird for me as I have the same keyboard mappings for both. The only time I fire up IntelliJ these days is when I want to do full-fledged Java development. For everything else, I use VS Code. I hope you find this useful. If you have any questions or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:62,url:"https://deepu.tech/reflection-on-golang/",title:"My reflections on Golang",body:"2019/07/12 - Do I like Go? Yes. Would I use it for every use case I have? Definitely not. Having worked on most of these said languages I won&#39;t choose Go for general purpose atleast not current version of Go, may be 2. 0 has more potential. &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) March 7, 2019Don’t get me wrong, I like Go for what it is but like every other programming language, it is always a love-hate relationship. No programming language is perfect and all of them have their own merits and use cases. I hate it when I see people overusing something and I see that pattern with Go these days. To be fair, I have done my fair share of overusing in my career as well (mostly with JavaScript) and I can see why people do it. This is not gonna be a blog bashing Go or praising Go, it is just what I think of it after using it for over 9 months. Before I start a rant on the good and bad of Go, here is some background. After being in the tech industry for over 10 years, I would like to think of myself as a pragmatic programmer or at least as someone getting closer to that - that should be a programmer’s Nirvana. I didn’t even plan to be a programmer, if you ask the 18-year-old self of me, he would say that he wanted to be an astrophysicist or a robotics engineer(Yes building space robots were my dream). Like most teenage dreams, it didn’t happen and I ended up in tech instead. Though landing an IT Job was an accident, programming wasn’t alien to me. I did learn some C/C++ when I was in high school to help my girlfriend with her project and did dabble in some PHP, JavaScript, HTML and Flash(ActionScript) during my early college years for personal projects and blogs. So when I got a real IT job without having an IT background, I did what many in that situation did, I started learning the language that I stumbled upon first based on the task I was given, which happened to be Java. Being a quick learner and having some idea of programming concepts from C/C++ Java wasn’t that hard to learn and I was a pretty decent Java programmer in a few months. Then I was tasked with building some Web UI and I dived deep into the world of HTML, CSS, and JavaScript and honestly fell in love with JavaScript due to its flexibility and ease. I mastered JQuery and soon become the go-to guy for front end stuff in the office. I was anything but pragmatic back then, I was preaching JavaScript to everyone and would vehemently debate anyone who thought JS was a bad language. Fast forward to now and if I look back I have done projects in C/C++, PHP, JavaScript, TypeScript, HTML, CSS, Java, Groovy, Scala, Python and recently Go. I think this exposure probably helped me become more pragmatic as I have started to look at programming languages as tools and each of them has their own strengths and weaknesses. Well, there is more to this story but that’s for another time, the point is to set a baseline for the below reflections so that I don’t sound like someone just trying Go and going on a rant. Go is the latest language I learned and worked with, I have worked on a CLI project built with Go for over 9 months now, building a powerful scaffolding engine with my team(Yes, pretty much like JHipster) that uses Go templates where you could create what we call blueprints at XebiaLabs. So yes I have done much more than a hello world app with Go. Without wasting more time on unrelated things here is what I like about Go and what I don’t like. What I like about Go: Simplicity: I like the fact that Go is a simple language(Going through the entire language features on the tour page literally takes 15 minutes unless you do the exercises) and unlike Scala, Rust or even JavaScript Go doesn’t have many ways of doing the same thing which is extremely valuable for people working in teams and companies wanting to write maintainable code where even a newly joined employee can read and understand the code without needing much help. I think this is one of the biggest reasons that is driving Go adoption. If you have worked on large scale projects you know how difficult it is when the code is unreadable and every new team member has to spend so much time trying to understand what a piece of code does. So I was really happy when I saw that Go doesn’t have features that rely heavily on implicit and such. The language features and concepts are easy to grasp and you can start being productive in Go quite soon. The only concepts that might seem bit complex are the concurrency part and even that is simpler compared to other languages. Language provided code style and vetting: This is such a time saver. IMO every language should just do this so that you don’t waste time debating code style and setting up lint rules. Go provides opinionated formatting, linting &amp; vet tool as part of the package and the Go compiler even enforces things like unused variables and stuff. Most of the IDE/Editor plugins also use these tools for formatting and linting and hence helps to keep consistent code style across Go projects which again adds to readability and maintenance. Goroutines &amp; Channels: This is one of the biggest strengths of Go. The native support for concurrency and parallelism. This makes Go an ideal candidate for applications that require heavy concurrent and/or parallel processing, networking and so on. Goroutines makes it so easy to start lightweight threads and channels provide a way to communicate between these threads acting like a message bus. 1234567891011func main() {\tmessages := make(chan string)\tcollected := make([]string, 2)\tgo func() { messages &lt;-  ping  }()\tgo func() { messages &lt;-  pong  }()\tcollected = append(collected, &lt;-messages)\tcollected = append(collected, &lt;-messages)\tfmt. Println(collected) // [ pong ping ]}Closures &amp; callbacks: If you have used JavaScript you would know how useful closures and callbacks are. Go like JavaScript treats functions as objects and hence can be assigned to variables, stored in maps, passed as function parameters and returned from functions. It also supports creating nested closures and anonymous functions which helps to encapsulate context. The behavior is pretty much similar to JavaScript. So you can apply some functional programming concepts in Go as well. 123456789101112131415161718192021222324func main() {\t// an unnecessarily complicated example\ttype fnType = func(a int, b int) int\tfnMap := map[string]fnType{\t\t ADD : func(a int, b int) int {\t\t\treturn a + b\t\t},\t\t SUB : func(a int, b int) int {\t\t\treturn a - b\t\t},\t}\t// this is a closure\tlocalFn := func(method string) fnType {\t\treturn fnMap[method] // returns a function\t}\tprinter := func(fn func(method string) fnType, method string) {\t\tfmt. Println(fn(method)(10, 5)) // callback\t}\t// function passed as parameter\tprinter(localFn,  ADD )\tprinter(localFn,  SUB )}Type assertion and switches: Go provides a nice way of asserting types and can be used with a switch statement which makes it easier to do reflection and such. Multiple returns: This is quite a handy feature like in Python, we are used to deconstructing objects/arrays to achieve this in JavaScript and using Tuples and such in some languages. The returns can also be named which is nice for readability. Tooling: The in-code test coverage highlight in VsCode for @golang is slick. This is the best way to ensure you have good coverage@code pic. twitter. com/nk8iMwenCz &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) February 14, 2019As mentioned earlier Go provides standard tooling for formatting, linting and so on and the language design makes it easy to build tooling for Go and hence editors/IDE has nice features like test generation, code coverage and so on. For example, the VSCode integration for Go provides the below options which helps with consistency and less boilerplate to write by hand.  Doesn’t need a runtime: Go doesn’t need a runtime like JVM or NodeJS, Go applications can be compiled into an executable cross-platform binary using the standard Go tooling. This makes Go applications portable and platform-independent. What I don’t like about Go: Simplicity: This is where the love-hate relationship starts, Go is a simple language which is nice but at times it feels too simple &amp; verbose and coming from Java/JavaScript ecosystem you are spoiled with some nice features &amp; syntax sugars which IMO makes the code more expressive and helps to keep it DRY. The things that I miss the most are  Generics: This is currently being considered in the next major iteration of Go, but until then this just makes you repeat code unnecessarily. I have lost count of the number of times I had to repeat the same block of code for different types where Generics would have kept it nice and simple. This is also one reason you don’t see libraries like Lodash for Go.  Standard error handling: This also seems to be coming in the next major iteration of Go but until it lands I can complain. Anyone writing Go will remember doing if err != nil uncountable times in your code. Removing those might cut the codebase in size by at least 20% Default values: I would love to see this in Go, this is quite useful. Maybe I’m just spoiled by JS. Too much boilerplate(not suitable for DRY): Go being too simple means you would have to write a lot of code as the language doesn’t offer constructs like map, reduce, and so on, and add the lack of generic on top means you would end up writing a lot of utility code and a lot of that will be repeated to accommodate different types. Imagine writing a map function in Go, you would have to write one for every combination of maps that can be used. These factors don’t make it easy to do DRY programming in Go. Dependency management: The dependency management in the Go ecosystem feels immature and too basic compared to other mainstream languages. Importing packages from Git is nice but it also makes it more fragile. What can go wrong when you are depending on a Git branch on your production application right! There is no way to use relative dependencies(Can’t beat NPM link!). These problems are similar to the issues with the dependency range in Node package managers. Glide seems to be a popular choice but still is not as mature as solutions in other languages. In the project, I work on we used Gradle along with Gogradle and though it works fine the developer experience is not as good as using Gradle/Maven for Java project or using NPM on a NodeJS project. Source code in GOPATH: Go recommends you to create your Go projects under the GOPATH. Maybe it is just me, but I hate this as I would normally like to organize my code. For example, I have a ~/workspace/ folder where I organize my projects by the organization. If I follow the Go recommendation I have to put the project under /home/deepu/go/src along with all the library source code that is downloaded. If you don’t follow this then most of the Go tooling just doesn’t work. Currently, I have a specific Gradle task that copies over all the vendor libs to my local Gopath inside ~/workspace/XL/&lt;project&gt; to workaround this. Confusing pointer behaviors: Go has pretty good pointer support and the default behavior is to pass an object by value. If you want to pass something by reference you have to mark it specifically. But this behavior is not very consistent as Maps and Slices by default are passed by reference and hence this could be a bit surprising to beginners. Struct hell: This is more of a nitpick. Structs are what you would use to create data structures in Go. It might look like an object but they are not exactly objects. While structs are fine functionally, in many cases you will end up with structs that look like the ugly brother of JSON. In real-world projects, you always will end up creating complex structs, especially if the application is doing some generic json or yaml parsing and soon your code will start to look like this. This is not that big of a concern but it just hurts my eyes every time I debug something or write tests. 123456789101112131415161718192021222324252627282930313233343536373839func main() {\ttype MyYamlDoc struct {\t\tfoo []map[interface{}][]map[interface{}]interface{}\t\tbar interface{}\t}\tohno := MyYamlDoc{\t\t[]map[interface{}][]map[interface{}]interface{}{\t\t\t{\t\t\t\t Foo : {\t\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{\t\t\t\t\t\t Foo : {\t\t\t\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{\t\t\t\t\t\t\t\t Foo : {\t\t\t\t\t\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{\t\t\t\t\t\t\t\t\t\t Foo : {\t\t\t\t\t\t\t\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{}},\t\t\t\t\t\t\t\t\t\t},\t\t\t\t\t\t\t\t\t}},\t\t\t\t\t\t\t\t},\t\t\t\t\t\t\t}},\t\t\t\t\t\t},\t\t\t\t\t}},\t\t\t\t},\t\t\t},\t\t\tmap[interface{}][]map[interface{}]interface{}{\t\t\t\t Foo : {\t\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{}},\t\t\t\t},\t\t\t},\t\t},\t\tmap[interface{}][]map[interface{}]interface{}{\t\t\t Foo : {\t\t\t\t{ Bar : map[interface{}][]map[interface{}]interface{}{}},\t\t\t},\t\t},\t}\tfmt. Println(ohno)}Weird interface construct: The interface concept in Go is weird. These are the only implicit construct in Go. If you come from other languages that have interfaces then this will feel weird. The fact that they are implicit means its really easy to mess things up. Refactoring is messy unless you have a smart IDE, and you can accidentally implement someone’s interface by just naming your method a certain way. While implicit interfaces certainly help with polymorphism and decoupling code I personally would still prefer interfaces that are explicit. Another interface Gotcha is null value checks, in Go, an interface is made up of two parts a type and a value, so an interface is nil only when both type and value are null, this means you can’t just simply do null checks on interfaces. This is so confusing the Go has a specific FAQ for this. Below article explains this in more detail         https://dev. to/pauljlucas/go-tcha-when-nil--nil-hic      Single GC algorithm: Go implements a concurrent tri-color mark-sweep collector as its garbage collector. This specific GC implementation is optimized for better pause times while ignoring program throughput, pause frequency and many other parameters that are considered during GC. Some people in the Go community claims this as the best ever GC. Having some Java background I would have to disagree as most JVM implementations provide multiple GC algorithms you can choose from which includes a concurrent mark-sweep collector as well and most of these are balanced to take care of many more parameters than just pause times. This articles analyses this in detail. So some use cases that produce a high amount of garbage might actually be slower in Go compared to another language due to frequent GC. Developer experience: This is purely based on personal experience and hence will vary from others. Being a polyglot developer who has worked with many languages, the developer experience from Go is not the best I have experienced. The DX of the JavaScript ecosystem is the best I have experienced so far. It feels like there are things missing in the Go ecosystem. Dependency management and toolchains need improvement. A bit more sensible language features and some syntax sugar wouldn’t hurt as well. Conclusion: Having worked with many major languages I can’t just use Go for every use case but I can see why people would use Go for every use-case out there if they haven’t worked with other languages. So where would I use Go?:  I would definitely use Go when the use case requires a lot of parallel processing and/or concurrency(both are not the same thing but are closer to each other) as you can make use of Goroutines for this and is much simpler and efficient than managing threads like in a Java application or working around it in JavaScript using callback hell since JS is actually single-threaded. Here is a nice article explaining the advantage of Goroutines.  Simple microservices where boilerplate is not a concern Networking applications or web servers, especially with async workloads, can greatly benefit from Go. But to be fair you can do these in Java, Python, JS, etc as well but Go in the end will provide better efficiency and would be easier to implement.  System programming. While Rust or C is a much better choice for this but if those are not in your arsenal then Go is the next best thing. With decent support for pointers and its standard library its easier for system programs than other mainstream languages. Many popular system tools like Docker, Kubernetes, etc are indeed written in Go. Where I wouldn’t use Go?:  Complex web application: I would choose Java with a framework like Spring or Micronaut as its much more maintainable and battle-tested and you would focus more on business logic than writing boilerplate infrastructure code. One common argument against this stack is its memory footprint but it is possible to get lower memory footprint with Spring and frameworks like Micronaut and Quarkus actually promises that OOB.  After writing a high-level CLI tool in Go, I hate the experience, I kept thinking that doing it in JavaScript would have been 10 times more productive and a nicer experience. SO I would choose JavaScript or TypeScript running on NodeJS for CLI tool any day. Mainly due to the ecosystem and the sheer joy and speed of getting things done without spending all your time writing boilerplate code. But this wouldn’t be applicable if the CLI in question a system tool or a networking tool, in those cases Go could be a good option. I do hope Go evolves into a general-purpose language over time and many of these concerns are solved. In the meantime, I’ll try to follow this mantra. Remember this manthra &quot;right tools for the right job, right pattern for the use-case&quot; #engineering #development #architecture #microservices https://t. co/SA42jQ5TLH &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) July 2, 2019But then you can always choose to fasten a screw using a hammer. Using the wrong tool for the job. #programming pic. twitter. com/5RdVqGuZoj &mdash; Rory Preddy☕ (@rorypreddy) June 24, 2019If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image credit: Image from egonelbre/gophers created by @egonelbre. "},{id:63,url:"https://deepu.tech/configure-a-beautiful-terminal-on-unix/",title:"Configure a beautiful terminal on Unix with Zsh",body:"2019/07/01 - I was a long-time Windows user, a fairly happy one, but as a developer, there were a lot of things that were missing for me and one of the main was the terminal experience. I’m not a fan of the closed ecosystem of Apple so Linux was an easy choice for me and I switched to Linux almost 3 years ago. I did start out with Ubuntu and later switched to Fedora which is my primary OS now. You can read about my setup here As a senior developer and open source community lead, I spent a lot of time on the terminal and a terminal with a nice developer experience instantly makes you happier and more productive. The default bash terminal is good for beginners but if you really want a powerful terminal you need something more than bash. Let’s see how to configure a powerful and productive terminal experience. The setup is based on what I have configured on my Fedora machine. The same setup can be recreated on any other Linux distribution, BSD or Mac as well. You just need to use the installation instruction from the tools for the given platform.  Below are the tools we would need for this. Zsh: Zsh is one of the most feature-rich shells for Unix. It works on Linux, Mac, WSL, and BSD. There are alternatives like Fish which also offers similar features but I personally like Zsh.    Check if Zsh is already installed by running zsh --version on your terminal. If not found, install it using your package manager.      Fedora: sudo dnf install zsh   Mac: brew install zsh zsh-completions   RHEL/CentOS: sudo yum update &amp;&amp; sudo yum -y install zsh   Ubuntu/Debian: sudo apt install zsh   For other platform refer this    Now make Zsh your default shell by running chsh -s $(which zsh).  Log out and log in back again to use your new default shell.  Test that it worked with echo $SHELL. Expected result: /bin/zsh or similar.  Test with $SHELL --version. Expected result: zsh 5. 6. 2 or similarNote: If you have installed Zsh for the first time and launch the shell it would prompt you to configure some settings. You can choose to ignore that by hitting q as we will configure it later on. Oh-My-Zsh: Oh-My-Zsh gives the Zsh shell superpowers. Its a framework to manage Zsh configuration. It has plugins and themes for Zsh(A lot of them). From their Github page:  Once installed, your terminal shell will become the talk of the town or your money back! With each keystroke in your command prompt, you’ll take advantage of the hundreds of powerful plugins and beautiful themes. Strangers will come up to you in cafés and ask you, “that is amazing! are you some sort of genius?” Just install it. You need it :) 1sh -c  $(curl -fsSL https://raw. githubusercontent. com/robbyrussell/oh-my-zsh/master/tools/install. sh) Terminal emulator/multiplexer: Optionally you can use a Terminal emulator that can manage windows and panes for you. For Linux I would recommend using Tilix, I have been using it for 3 years and its just amazing. For Mac, you can use iTerm2 which is very popular. Alternatively, you can also use tmux if you want something lighter on your existing Terminal app on Linux, BSD or Mac. Configuring Zsh: This is the fun part. Let us make the terminal awesome. Install plugins: First, let us install some additional plugins that are not bundled with Oh-My-Zsh. zsh-autosuggestions: Provides auto completion for shell commands. Run git clone https://github. com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/plugins/zsh-autosuggestions to install zsh-syntax-highlighting: Provides syntax highlighting on the shell. Run git clone https://github. com/zsh-users/zsh-syntax-highlighting. git ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/plugins/zsh-syntax-highlighting to install autojump: Provides a smarter directory navigation system. Install autojump for your OS following instructions here. Now let us configure the ~/. zshrc file with some settings. Here is my full . zshrc file. Your mileage may vary. Add exports: We will start with some exports. 1234567891011121314export TERM= xterm-256color  # This sets up colors properly# set shellexport SHELL=/usr/bin/zsh# If you come from bash you might have to change your $PATH. export NODE_PATH=$NODE_PATH:$HOME/. npm-global/lib/node_modulesexport JAVA_HOME=/usr/java/latestexport PATH=$JAVA_HOME/bin:~/. npm-global/bin:$HOME/bin:/usr/local/bin:$PATH# Add exports from your profilesource ~/. profile# Path to your oh-my-zsh installation. export ZSH=$HOME/. oh-my-zshZsh settings: Now we can configure some Zsh specific settings 1234DISABLE_MAGIC_FUNCTIONS=trueZSH_AUTOSUGGEST_MANUAL_REBIND=1COMPLETION_WAITING_DOTS=trueDISABLE_UNTRACKED_FILES_DIRTY=trueZsh theme: Now, Let’s set up a nice theme. I’m using powerlevel10k as my current theme and it’s fast and looks great. You can use the default or you can choose any theme you like from the list here. If you like my theme then follow these instructions. Thanks to Roman Perepelitsa for some cool tips Run git clone https://github. com/romkatv/powerlevel10k. git ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/themes/powerlevel10k to install the theme. Install a Powerline font. I use Adobe Source Code Pro Add the below configuration to the ~/. zshrc file. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# Set name of the theme to load. Optionally, if you set this to  random # it'll load a random theme each time that oh-my-zsh is loaded. # See https://github. com/robbyrussell/oh-my-zsh/wiki/ThemesZSH_THEME= powerlevel10k/powerlevel10k ############ POWERLEVEL THEME SETTINGS ##############POWERLEVEL9K_MODE='awesome-fontconfig'POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir vcs nvm)POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(disk_usage time)POWERLEVEL9K_PROMPT_ADD_NEWLINE=truePOWERLEVEL9K_PROMPT_ON_NEWLINE=truePOWERLEVEL9K_SHOW_RULER=truePOWERLEVEL9K_RULER_CHAR='─'POWERLEVEL9K_RULER_BACKGROUND=nonePOWERLEVEL9K_RULER_FOREGROUND=237POWERLEVEL9K_LEFT_SEGMENT_END_SEPARATOR=POWERLEVEL9K_LEFT_SEGMENT_SEPARATOR=POWERLEVEL9K_LEFT_SUBSEGMENT_SEPARATOR=' 'POWERLEVEL9K_RIGHT_SEGMENT_END_SEPARATOR=POWERLEVEL9K_RIGHT_SEGMENT_SEPARATOR=POWERLEVEL9K_RIGHT_SUBSEGMENT_SEPARATOR=POWERLEVEL9K_WHITESPACE_BETWEEN_LEFT_SEGMENTS=POWERLEVEL9K_SHORTEN_DIR_LENGTH=2POWERLEVEL9K_SHORTEN_STRATEGY= truncate_middle POWERLEVEL9K_DIR_SHOW_WRITABLE=truePOWERLEVEL9K_DISK_USAGE_NORMAL_BACKGROUND=nonePOWERLEVEL9K_DISK_USAGE_WARNING_BACKGROUND=magentaPOWERLEVEL9K_DISK_USAGE_CRITICAL_BACKGROUND=redPOWERLEVEL9K_TIME_BACKGROUND=nonePOWERLEVEL9K_TIME_FOREGROUND=whitePOWERLEVEL9K_DIR_HOME_BACKGROUND=nonePOWERLEVEL9K_DIR_HOME_SUBFOLDER_BACKGROUND=nonePOWERLEVEL9K_DIR_ETC_BACKGROUND=nonePOWERLEVEL9K_DIR_DEFAULT_BACKGROUND=nonePOWERLEVEL9K_DIR_NOT_WRITABLE_BACKGROUND=nonePOWERLEVEL9K_DIR_HOME_FOREGROUND=bluePOWERLEVEL9K_DIR_HOME_SUBFOLDER_FOREGROUND=bluePOWERLEVEL9K_DIR_ETC_FOREGROUND=bluePOWERLEVEL9K_DIR_DEFAULT_FOREGROUND=bluePOWERLEVEL9K_DIR_NOT_WRITABLE_FOREGROUND=redPOWERLEVEL9K_OS_ICON_BACKGROUND= white POWERLEVEL9K_OS_ICON_FOREGROUND= blue POWERLEVEL9K_VCS_GIT_ICON='%fon %F{040} 'POWERLEVEL9K_VCS_GIT_GITHUB_ICON='%fon %F{040} 'POWERLEVEL9K_VCS_GIT_BITBUCKET_ICON='%fon %F{040} 'POWERLEVEL9K_VCS_GIT_GIT_GITLAB_ICON='%fon %F{040} 'POWERLEVEL9K_VCS_CLEAN_BACKGROUND=nonePOWERLEVEL9K_VCS_UNTRACKED_BACKGROUND=nonePOWERLEVEL9K_VCS_MODIFIED_BACKGROUND=nonePOWERLEVEL9K_VCS_LOADING_BACKGROUND=nonePOWERLEVEL9K_VCS_CLEAN_FOREGROUND= 040 POWERLEVEL9K_VCS_UNTRACKED_FOREGROUND= red POWERLEVEL9K_VCS_MODIFIED_FOREGROUND= yellow POWERLEVEL9K_VCS_LOADING_FOREGROUND= grey POWERLEVEL9K_VCS_UNTRACKED_ICON=$'%{\b?%}'POWERLEVEL9K_VCS_UNSTAGED_ICON=$'%{\b!%}'POWERLEVEL9K_VCS_STAGED_ICON=$'%{\b+%}'POWERLEVEL9K_DIR_NOT_WRITABLE_VISUAL_IDENTIFIER_COLOR=redPOWERLEVEL9K_LOCK_ICON=$''POWERLEVEL9K_MULTILINE_FIRST_PROMPT_PREFIX=''local p='%F{ %(?. green. red)}${${${KEYMAP:-0}:#vicmd}:+❯}${${$((!${#${KEYMAP:-0}:#vicmd})):#0}:+❮}%f 'POWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX= $p POWERLEVEL9K_NVM_BACKGROUND=nonePOWERLEVEL9K_NVM_FOREGROUND=greenPOWERLEVEL9K_NODE_ICON='%fvia %F{green}⬢'############ END- POWERLEVEL THEME SETTINGS ##############Enable plugins: We can finish off by enabling the plugins and some tweaks 1234plugins=(zsh-autosuggestions git docker docker-compose autojump zsh-syntax-highlighting dnf npm)source $ZSH/oh-my-zsh. shAnd that’s it we are ready. Start a new terminal session and enjoy. Issues &amp; workarounds: If you use Tilix as your terminal emulator, then this might be required for proper pane splitting. Add this to your ~/. zshrc 123if [[ $TILIX_ID ]]; then    source /etc/profile. d/vte. shfiIf you are getting errors from the zsh-completion plugin, you might want to add this to the beginning of your ~/. zshrc 1234# workaround as per https://superuser. com/questions/1222867/zsh-completion-functions-brokenFPATH=$HOME/. oh-my-zsh/plugins/git:$HOME/. oh-my-zsh/functions:$HOME/. oh-my-zsh/completions:/usr/share/zsh/site-functions:/usr/share/zsh/$ZSH_VERSION/functionsexport FPATHIf you encounter an error from Oh-My-Zsh saying [oh-my-zsh] Insecure completion-dependent directories detected, set ZSH_DISABLE_COMPFIX=true right before the line source $ZSH/oh-my-zsh. sh in your ~/. zshrc file and restart your session or run exec zsh Dockerized playground. : If you have Docker installed then you can use the below snippet to try this setup in a sandbox without installing anything or affecting your existing setup. 123456789docker run -e LANG=C. UTF-8 -e LC_ALL=C. UTF-8 -e TERM=$TERM -it --rm ubuntu bash -uexc ' apt update &amp;&amp; apt install -y git curl zsh autojump &amp;&amp; cd /root sh -c  $(curl -fsSL https://raw. githubusercontent. com/robbyrussell/oh-my-zsh/master/tools/install. sh)  --skip-chsh --unattended git clone https://github. com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/plugins/zsh-autosuggestions git clone https://github. com/zsh-users/zsh-syntax-highlighting. git ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github. com/romkatv/powerlevel10k. git ${ZSH_CUSTOM:-~/. oh-my-zsh/custom}/themes/powerlevel10k curl -fsSLO http://bit. ly/Spaceship10kTheme echo  source ~/Spaceship10kTheme  &gt;~/. zshrc exec zsh'VSCode Tip: If you are using VSCode like me, you might want to do the below to get the same terminal experience in the integrated VSCode terminal as well.  Follow these steps  Download and install a patched font.  On Linux, run fc-cache -f -v to refresh font cache.  On VSCode, open Preferences → Settings and click on the {} icon to open JSON mode and set the below123456   terminal. integrated. shell. linux :  /usr/bin/zsh ,   terminal. integrated. fontFamily :  'SauceCodePro Nerd Font Mono','Source Code Pro' ,   terminal. integrated. rightClickCopyPaste : true,   terminal. integrated. fontSize : 14,   terminal. integrated. cursorStyle :  underline ,   terminal. integrated. cursorBlinking : trueReplace linux with osx if you are on a Mac. I hope you like it. If you have any questions or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:64,url:"https://deepu.tech/must-have-gnome-plugins/",title:"Must have GNOME extensions",body:"2019/06/24 - I’m a sucker for nice polished UI and great UX. While there are a lot of Linux Desktop environments out there providing great UX and UI, I found GNOME to be the perfect one for my liking. Yes, I have seen/tried a few others. I also found some which are more polished and providing a better default UX out of the box than GNOME like Deepin and Elementary. But below plugins bridge that gap and hence I choose to stick with GNOME which is the default in Fedora, hence quite stable, unless I had a compelling reason to switch. So if you like me are a GNOME fan then below are some of the GNOME plugins you must try if you haven’t already. I have listed the plugins I use in my earlier post in the series. here I detail the ones that are a must-have. GNOME Tweaks: This nifty tool lets you tweak/configure a lot of GNOME configuration and should have been included by default in every distro shipping with GNOME. You can customize the appearance, install extensions, configure mouse &amp; keyboard and so on. It can be found in the software center of your distro. Search for “Tweaks”.  Gnome extensions: You can install below extensions by visiting the link in the title of the extension below and by clicking on the on switch on the top right corner. On Chrome, you would need the GNOME Shell integration plugin to enable the switch. On Firefox, it will prompt you to install the plugin if it doesn’t exist. Dash to Dock: GNOME without this plugin almost feels annoying. IMO this plugin also should be the default GNOME setting. This one moves your GNOME dash into a highly configurable dock which can be placed on the sides or top/bottom of the screen. I find it perfect on the left side of the screen in GNOME. It can be a floating dock or fixed to look like those on Mint or KDE.  Always Zoom Workspaces: By default, the GNOME launcher does not show the workspaces, you have to hover over the right edge to see it. I find it unnecessary given you have enough real estate on the full-screen launcher and the workspace view takes only a little bit. This plugin keeps it zoomed by default.  Steal My Focus/NoAnnoyance: This is another default in GNOME that is annoying. When something needs focus these plugins brings the window up instead of the default notification. You can use any one of the plugins as both do the same thing. AlternateTab: This replaces the default Alt+Tab with a more classical window-based switcher which IMO is more user-friendly as the default requires more keyboard navigation using the arrow keys.  Window List: This is a classic plugin that adds the window list to the bottom of the screen and is a must if you use multiple monitors as the windows are grouped and placed in the right monitor screen. Caffeine: This one adds the ability to temporarily disable screensaver/auto-suspend and automatically activates when you go full-screen. A must-have if you are using your computer for watching videos, presentations, screencast and so on. Clipboard Indicator: This is one my favorite. It adds a nifty clipboard manager to the top bar and provides shortcuts to cycle through clipboard entries. A real time saver.  Gistnotes: As a developer using GitHub gist a lot, this one is a very useful plugin. It lets you manage your Gists right from the desktop and you can use it like a notes app.  System-monitor: A nice system monitor plugin that sits on the top bar with a detailed view as a popup.  TopIcons Plus: This moves legacy icons form applications to the top bar for consistent UX. I hope you find this useful. If you have any question or if you think I missed something please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. Cover image photo by Brigitta Schneiter on Unsplash "},{id:65,url:"https://deepu.tech/my-beautiful-linux-development-environment/",title:"My beautiful Linux development environment",body:"2019/06/16 -  There is a new version of this post where I wrote about my current setup. Check it out here. One of the questions that I often get after a conference talk is weirdly not about what I presented but about my Linux desktop environment. People are more curious about that beautiful distro rather than the awesome presentation I just did 😂 Not that I’m complaining, I love my desktop setup. I love it so much that I was afraid of getting a new PC when I was due for one. I was afraid that I would mess things up(I have done that many times in the past, I think Linux users can relate to me) So I decided to capture the most important aspects of my distro for anyone interested in using Linux as their primary OS for development.  This is not just my work laptop; it’s my primary machine which I use for all of the below.  Java, JS, TS, Go, Python &amp; web development JHipster development Running multiple web applications locally Running Docker containers VirtualBox for Windows testing &amp; other VM stuff Kubernetes, Terraform, CloudFormation development and deployments Azure, AWS &amp; GCP deployments using required CLI tools Heavy browser usage Email, chat &amp; video conferencing Plex media server Blogging Youtube &amp; Social mediaMachine configuration: The configuration of the machine is also quite important for any development setup. So my laptop is a Dell Precision 5530 Mobile Workstation. I had the exact same setup with my old Dell 5510 as well, which is quite a similar configuration to 5530. I still have it as a backup Laptop, its two years old now, but it can still give most of the top-end laptops today a run for its money. I used the custom configuration option from Dell to get the best possible setup at that time. It’s not cheap but my company, XebiaLabs, provided a handsome budget and I think it is worth every penny. This, in my opinion, is one of the best Laptops for developers. So here is what I have. Processor: Intel® Core™ i9-8950HK CPU @ 2. 90GHz × 12 Memory: 32GB, DDR4-2666MHz SDRAM, 2 DIMMS, Non-ECC HDD: M. 2 1TB NVMe PCIe SED class 40 SSD Graphics: NVIDIA Quadro P2000 with 4 GB GDDR5 memory &amp; Intel® UHD Graphics 630 (Coffeelake 3x8 GT2) Wireless: Intel Wifi Link 9260 2x2 802. 11AC + BT 4. 2 vPro wireless card Keyboard: English QWERTY US, backlit Display: 15. 6” FHD 1920x1080 Anti-Glare LED-backlit Non-touch IPS UltraSharp™ Battery: 6-cell (97Wh) Lithium-Ion battery with ExpressCharge™ Operating system and desktop environment: The most important, of course, is the operating system. I’m running Fedora 30 at the moment with GNOME 3. 32. 2 as the Desktop, and I’m very happy with it. I find Fedora more suitable for development machines than other distros as it has a short release cycle and is fairly stable, so you get the latest &amp; stable software all the time.  What good is a desktop without a nice theme, right? GNOME is great when it comes to themes, and I went with Arc-Flatabulous theme and never looked back. For icons, I use Paper as I like the material icon theme.  Of course, it won’t be complete without some nice GNOME plugins. Below are the plugins that I use.  Dash to Dock Always Zoom Workspaces Auto Move Windows Native Window Placement Launch new instance Steal My Focus AlternateTab Window List Applications Menu Caffeine Clipboard Indicator Gistnotes OpenWeather Places Status Indicator System-monitor Todo. txt TopIcons Plus User ThemesDevelopment tools: Now, these are mostly objective choices and really don’t matter as long as you are comfortable with the tools you choose. Below are my choices for some of the important categories for development. I’m not including obvious things like Vim, Git, NodeJS, Docker, Kubernetes, etc. Shell: This is one of the most important for a developer. I use ZSH along with the awesome Oh My ZSH as my shell. Now, this won’t be complete without some nice plugins and themes. I use powerlevel9k theme with some customizations. I also use zsh-autosuggestions, git, docker, docker-compose, autojump, zsh-syntax-highlighting, dnf, and npm plugins for Oh My ZSH. Here is my . zshrc with all the customizations. Update: A comment on this post suggested powerlevel10k as an alternative theme, and I tried it, and it turns out it is really way faster than powerlevel9k. So I think I’m gonna use powerlevel10k as my shell theme. Terminal: What good is a nice shell without a good terminal. Fortunately, we have Tilix, one of the best terminal applications out there. It has workspaces, tabs, split windows, Quake mode, and so on.  Integrated development environment(IDE): IntelliJ IDEA Ultimate - I use this for Java &amp; other JVM language Development Code Editors: Visual Studio Code - My go-to editor. I love it. I use VSCode for web development, Go, Python, JS development, DevOps, and everything other than JVM languages. A VSCode setup is never complete without some good plugins. Here are the plugins that I’m using. You can run the script to install those. Other notable development tools I use are GitKraken for Git repo management, Beyond Compare for code comparisons, VirtualBox, NVM for NodeJS version management and SDKMan for JDK version management. Productivity tools: Productivity tools are also quite important, and below are my choices. Browser: Google Chrome is my primary browser. I also use Firefox &amp; Opera sometimes. I do love Opera in terms of its UX, I would love to use it as my primary browser, but I miss everything I have synchronized with my Google account in Chrome. Email: I use Mailspring as my e-mail client. It’s a fairly decent mail client with nice themes and a simple UI. Office suite: I mostly use Google Docs &amp; Microsoft office online, but when I have to work on something on my Desktop I use LibreOffice which is a good office suite and even handles Microsoft Office &amp; Keynote formats. Communication: Of course I use Slack and for video conference I use BlueJeans. Screen capture: I use this nifty tool called Peek for screen recording and Shutter for screenshots. Conclusion: There are many other small and nifty utilities that I use; most are command-line utilities. There are some notable mentions like Timeshift, which is nice for backing up your machine. Of course, not everything is perfect in the Linux world, but it is the same with every OS. I was a long-time Windows user before switching to Linux. So like every Linux user, I have from time to time messed things up(With great power comes great responsibility, Peter). There are many quirks in the Linux world, but there is nothing that bothers me much. Some of the most annoying issues I had in the past are below, and for now, I don’t have any noticeable issues.  Scroll position jumping when switching apps - Fixed after upgrading to Fedora 30 Hibernation was broken - Fixed after upgrading to Fedora 30 Audio output selection was broken when plugging in headphones- Fixed after Fedora 28 for meThis has been a good day, upgraded to #Fedora 30 and hibernate started to work again. Sweet. I was putting off tinkering that for a long time. #Linux &mdash; Deepu K Sasidharan (@deepu105@mastodon. online) (@deepu105) June 14, 2019I hope you find this useful. If you have any questions or if you think I missed something, please add a comment. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. "},{id:66,url:"https://deepu.tech/why-im-moving-away-from-medium/",title:"Why I’m moving away from Medium",body:"2019/06/13 - After much deliberation, I have decided to move my blogs away from Medium. I was considering setting up my own blog with Hugo but then decided to go with Dev. to. Below are the reasons why I decided to leave Medium and why I chose Dev. to. All considerations were purely from a technical writing perspective as I was using Medium for publishing technical content. The love-hate relationship with Medium: I loved Medium when I started writing here, reasons being;    A simple minimal &amp; clean UI — It still is one of the best     Ease of authoring and publishing     Community and visibility     Publications     Ease of customization  But there were also things I didn’t like much which slowly become quite annoying  The weird commenting mechanism(Every comment is a post, and they literally mess up your stories listing page)    Medium had a weird WYSIWYG editor interface which is great for normal content creation but not so great for technical content creation. It had some markdown like shortcuts, but it could never match the ease of using proper markdown editors.     Export only in HTML (Duh!!)  But these annoyances were not the main reason I decided to switch platforms. Below are the main reasons why I decided Medium isn’t a good fit for me. Medium has been aggressively pushing for content to be put behind a paywall and they have made it clear that content not opting in will not get any push inside the platform. This means the community and visibility part is applicable only if you opt-in for the paywall. I understand why Medium does and I think its a great monetary source for established authors but it doesn’t work for mere mortals like me.  As a result of the above, the traffic you get from Medium itself is very low compared to external sources. See one of my stories below for an example. For newer stories, it is even lower.  So writing in Medium seems to have no benefit over other platforms as I could get similar views from external sources and might get better writing experience elsewhere. UpdateSo after a week of moving to the Dev community below are my stats and its incredible, I have ~50k views, ~1k reactions and ~300 followers and one of my post was featured in top 7 of the week and all this in just 1 week. I didn’t get anything remotely close to this from Medium in a year.  Enter Dev. to: When I was trying to find a different platform, some of the most important aspects I considered were below    Community: A community without paywall and a community were your blogs get visibility and get traffic.     Ease of authoring: Authoring experience was important, hence at minimum Markdown support was a must. This way I can author posts in my favorite editor(VsCode in this case) and doesn’t have to be restricted with the platform’s capability. Also, this ensures that I can easily move my posts to another platform in the future if needed.  Dev. to satisfied these needs and provided a nice and clean UI and descent publishing experience on top. Conclusion: I think Medium is still perfect for normal blogging and for content creators who have subscribers willing to pay even if they put articles behind a paywall. But for technical content creators who do not want their content behind a paywall, there are better platforms. I might still crosspost between Dev. to and Medium from time to time but Dev. to will be my primary blogging platform. Originally published in Medium on June 13, 2019 Cover image credit: Photo by MILKOVÍ on Unsplash "},{id:67,url:"https://deepu.tech/deploy-a-web-app-to-azure-app-service-using-terraform/",title:"Deploy a web app to Azure App Service using Terraform",body:"2019/06/12 - Deploying Java web applications to Azure is easy and has been tried, tested and explained many times by many people. My friend Julien Dubois has a nice series on it here. Azure makes it really easy to use its App Service as it provides many different ways of deploying a web app. If you are a modern full-stack Java developer there is a high chance that you are deploying your application as a Docker image. Hence today let’s see how we can deploy a Java web application to Azure App Service using Docker and Terraform in the true spirit of infrastructure as code. The approach is pretty much the same for any web application that is built as a docker image and not necessarily tied down to just Java. To try this out you would need to have Java, NodeJS, Terraform, Docker and Azure CLI installed. Follow the links to install them if needed. As one of the lead developer of JHipster (A handy development platform to generate, develop and deploy Spring Boot + Angular/React/Vue Web applications and Spring microservices), I would use a JHipster web application as the example here. So let’s get started. Let’s build a very simple web application using JHipster. We will use the JDL feature to scaffold our application. We will use the below JDL for our application. Save it to a file named app. jdl in a directory where you want to create the application. application {  config {    baseName helloJHipster,    applicationType monolith,    packageName tech. jhipster. demo,    authenticationType jwt,    buildTool gradle,    clientFramework react,    databaseType sql,    prodDatabaseType mysql,    languages [en, nl]  }}Now let us scaffold this using JHipster. Open your favorite console/terminal and run the below command in the directory where you saved the above JDL file, make sure it’s an empty directory. 1$ npx generator-jhipster import-jdl app. jdlIf you already have JHipster installed you can just run 1$ jhipster import-jdl app. jdlThis will scaffold the application and install the required client-side dependencies. It might take a few minutes(NPM!) so maybe its time for that coffee. You can see the application in action by running . /gradlew on the same terminal once the scaffolding is done. You can refer to the generated Readme. md for more instructions regarding the application. Now let’s move on to the focus of this post, deploying this to Azure App Service with Terraform. Let us first build and publish the docker image for our application. JHipster conveniently provides everything that is required to build docker images. Let us use the provided docker integration using JIB to build the images. Run the below Gradle command. 1$ . /gradlew bootJar -Pprod jibDockerBuildNow let us tag and push this to our docker registry, make sure you have logged into docker and run these commands. Use your own docker hub account name. 123$ docker tag hellojhipster:latest deepu105/hellojhipster:latest$ docker push deepu105/hellojhipster:latestYou can also push to Azure Container registry instead of Docker Hub if you like. Now that our application and Docker images are ready, let’s prepare the Terraform infrastructure for App Service and MySQL database. For other ways of deploying a JHipster web app to Azure check this out. First, create a folder for our terraform files. Let’s name the folder terraform. Now create three files called main. tf, outputs. tf, and variables. tf in this folder. Let us define the variables we will use. Save the below in variables. tf. 12345678910111213141516171819202122232425variable  prefix  { description =  The prefix used for all resources in this example  default   =  xl }variable  location  { description =  The Azure location where all resources in this example should be created }variable  subscription_id  { description =  Azure Subscription ID to be used for billing }variable  my_sql_master_password  { description =  MySql master password }variable  docker_image  { description =  Docker image name }variable  docker_image_tag  { description =  Docker image tag }Now let us define our main. tf First, let us add a configuration for Azure resource manager and create an Azure resource group to hold our resources. 123456789provider  azurerm  { version     =  =1. 24. 0  subscription_id =  ${var. subscription_id} }resource  azurerm_resource_group   main  { name   =  ${var. prefix}-resources  location =  ${var. location} }Now let us add the configuration to create a MySQL database server along with the required firewall rules to let App Service access the DB. If you want to add local access from your machine add a firewall rule block for your IP as well. 12345678910111213141516171819202122232425262728293031323334353637383940414243# This creates a MySQL serverresource  azurerm_mysql_server   main  { name        =  ${var. prefix}-mysql-server  location      =  ${azurerm_resource_group. main. location}  resource_group_name =  ${azurerm_resource_group. main. name}  sku {  name   =  B_Gen5_2   capacity = 2  tier   =  Basic   family  =  Gen5  } storage_profile {  storage_mb      = 5120  backup_retention_days = 7  geo_redundant_backup =  Disabled  } administrator_login     =  mysqladminun  administrator_login_password =  ${var. my_sql_master_password}  version           =  5. 7  ssl_enforcement       =  Disabled }# This is the database that our application will useresource  azurerm_mysql_database   main  { name        =  ${var. prefix}_mysql_db  resource_group_name =  ${azurerm_resource_group. main. name}  server_name     =  ${azurerm_mysql_server. main. name}  charset       =  utf8  collation      =  utf8_unicode_ci }# This rule is to enable the 'Allow access to Azure services' checkboxresource  azurerm_mysql_firewall_rule   main  { name        =  ${var. prefix}-mysql-firewall  resource_group_name =  ${azurerm_resource_group. main. name}  server_name     =  ${azurerm_mysql_server. main. name}  start_ip_address  =  0. 0. 0. 0  end_ip_address   =  0. 0. 0. 0 }This will create a MySQL server, a database for our app on the server and enable access from App Service. Now let us configure the App Service itself along with a service plan. 123456789101112131415161718192021222324252627282930313233343536373839# This creates the plan that the service useresource  azurerm_app_service_plan   main  { name        =  ${var. prefix}-asp  location      =  ${azurerm_resource_group. main. location}  resource_group_name =  ${azurerm_resource_group. main. name}  kind        =  Linux  reserved      = true sku {  tier =  Standard   size =  S1  }}# This creates the service definitionresource  azurerm_app_service   main  { name        =  ${var. prefix}-appservice  location      =  ${azurerm_resource_group. main. location}  resource_group_name =  ${azurerm_resource_group. main. name}  app_service_plan_id =  ${azurerm_app_service_plan. main. id}  site_config {  app_command_line =     linux_fx_version =  DOCKER|${var. docker_image}:${var. docker_image_tag}   always_on    = true } app_settings = {   WEBSITES_ENABLE_APP_SERVICE_STORAGE  =  false    DOCKER_REGISTRY_SERVER_URL      =  https://index. docker. io   # These are app specific environment variables   SPRING_PROFILES_ACTIVE    =  prod,swagger    SPRING_DATASOURCE_URL    =  jdbc:mysql://${azurerm_mysql_server. main. fqdn}:3306/${azurerm_mysql_database. main. name}?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC    SPRING_DATASOURCE_USERNAME  =  ${azurerm_mysql_server. main. administrator_login}@${azurerm_mysql_server. main. name}    SPRING_DATASOURCE_PASSWORD  =  ${var. my_sql_master_password}  }}In this configuration, under site_config we use linux_fx_version to declare our docker image and set always_on to true so that the application is not shut down when there is inactivity for some time. In the app_settings section we need to disable storage using the flag WEBSITES_ENABLE_APP_SERVICE_STORAGE and also specify DOCKER_REGISTRY_SERVER_URL. Everything else is specific to our app. The flags passed to the MySQL connection URL is important. Now that our main. tf is ready let us define some output properties that are handy. In the outputs. tf file add the below 1234567output  app_service_name  { value =  ${azurerm_app_service. main. name} }output  app_service_default_hostname  { value =  https://${azurerm_app_service. main. default_site_hostname} }Now we are ready to rock and roll! let us deploy the app. Make sure you have set up your Azure CLI and have logged in using az login. Now in a terminal/console navigate to the terraform folder we created and execute these commands. Please change the values for prefix, location &amp; docker_image accordingly. 1234567$ terraform init$ terraform apply -var prefix=myAwesomeApp -var location=northeurope -var docker_image=deepu105/hellojhipster -var docker_image_tag=latestThis will prompt you to enter a master password for MySQL server and your Azure subscription ID(You can find this from Azure portal or by running az account list- the id field is the subscription ID). Once you provide the values and confirm, Terraform will get to work and will start creating the resources. this could take a while since we are provisioning a Database server. Wait for it or go have that second coffee ;) Once the deployment is complete, Terraform will print out the outputs which include the app_service_default_hostname. Copy the URL and open it in your favorite browser. The first time could take a while since the app will be started(cold start) only during the first request.  I hope you found this useful. This is my first post in dev. to, I hope to migrate my blogs from Medium to dev. to soon. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. My other related posts:  Create full Microservice stack using JHipster Domain Language under 30 minutes Deploying JHipster Microservices on Azure Kubernetes Service (AKS) How to set up JHipster microservices with Istio service mesh on Kubernetes Continuous delivery of Microservices with XebiaLabs — a. k. a DevOps as Code"},{id:68,url:"https://deepu.tech/react-done-right-with-typescript/",title:"React components done right with TypeScript mapped and conditional types",body:"2018/11/19 - You’ve probably heard about TypeScript, If not you should check it out. You may have heard someone claiming how great type safety is. TypeScript is great. As someone who hates to transpile his code, I would definitely do it with TypeScript if I had to. So much has been said about TypeScript, and there isn’t really anything new that I can add. But I do believe that type safety is not all about making your code ugly with type definitions everywhere. So how can we write type-safe code without having to litter type declarations everywhere? Type inference and advanced features like derived and dynamic types are the answer. Editors and IDEs we use are smart enough to handle code with inferred type gracefully without us having to see the types all the time visually. (Of course, they all usually show you the type when you hover over an inferred type. ) TypeScript has very good type inference. As a rule of thumb, you can always start without declaring the type for any variable and see if the compiler infers it. With modern editors like VSCode, you can see this immediately. So set your tsconfig to the strict mode. Then start declaring types when the compiler complains. Additionally, TypeScript 2. 1 and 2. 8 introduced a bunch of cool lookup types. Now you can dynamically infer types using different techniques like Intersection types, Union types, Index types, mapped types and conditional types. Index types: Index types enable us to check properties and types of an interface or type dynamically using the keyof T (index type query operator) and T[K] (indexed access operator). Let’s take the below interface for example. 123456interface Person {  name: string;  age: number;  address: string;  sayHi: (msg: string) =&gt; string;}The keyof T operator gets a union type of all the key names of the type T and hence keyof Person will give us 'name' | 'age' | 'address' | sayHi' as result. The T[K] operator gets the type for the provided key. Person['name'] will result in string and Person[*keyof* Person] will result in string | number | ((msg: string) =&gt; string). Mapped types: Let us see what mapped types are. Let us say we have the below interface for a Person. 123456interface Person {  name: string;  age: number;  address: string;  sayHi: (msg: string) =&gt; string;}Now in every project, it is almost always a common requirement to have variations of a certain interface. For example, let’s say we need a read-only version of the person as below. 123456interface ReadonlyPerson {  readonly name: string;  readonly age: number;  readonly address: string;  readonly sayHi: (msg: string) =&gt; string;}In this case, we would have to replicate the Person interface and we have to keep them in sync manually. This is where mapped types will come in handy, so let us use the builtin mapped type, Readonly, for this. 1type ReadonlyPerson = Readonly&lt;Person&gt;;If you hover over the ReadonlyPerson type you can see the inferred type as below. Inferred type view in VsCode That is cool, right? Now we can create types from existing types and don’t have to worry about keeping them in sync. How does it work, what does Readonly&lt;Person&gt; do? Let’s take a look at the mapped type. 123type Readonly&lt;T&gt; = {  readonly [K in keyof T]: T[K];};The in operator from TypeScript does the trick here. It maps all the declarations of the existing type into the new type. The keyof operator provides the keys from our type for the mapping. Let us build our own mapped type. Let us say we need a read-only Person interface where all the fields are nullable as well. We can build a mapped type as below for that. 123type ReadonlyNullablePerson = {  readonly [P in keyof Person]: Person[P] | null;};And it is inferred as below Let’s make it generic so that it can be used with any interface. 12345type ReadonlyNullable&lt;T&gt; = {  readonly [K in keyof T]: T[K] | null;};type ReadonlyNullablePerson = ReadonlyNullable&lt;Person&gt;;TypeScript includes Readonly&lt;T&gt;, Partial&lt;T&gt;, Pick&lt;T, K extends keyof T&gt; and Record&lt;K extends string, T&gt; as built-in mapped types. Pick and Record can be used as below, check them in your editor to see what types they produce. 123type PersonMinimal = Pick&lt;Person,  name  |  age &gt;;type RecordedPerson = Record&lt; name  |  address , string&gt;;For every other use case, you can build your own mapped types. Conditional types:  A conditional type selects one of two possible types based on a condition expressed as a type relationship test. Let us look at an example. 1234567type Foo&lt;T, U&gt; = T extends U ? string : boolean;interface Me {}interface You extends Person {}type FooBool = Foo&lt;Me, Person&gt;; // will result in booleantype FooString = Foo&lt;You, Person&gt;; // will result in stringThe type dynamically inferred from Foo&lt;T, U&gt; will be either string or boolean depending on what the first generic is extended from. Let us see how we can mix conditional types with mapped types to infer a new type from Person which only includes the non-function properties. 123456789101112131415type NonFunctionPropNames&lt;T&gt; = {  [K in keyof T]: T[K] extends Function ? never : K;}[keyof T];type NonFunctionProps&lt;T&gt; = Pick&lt;T, NonFunctionPropNames&lt;T&gt;&gt;;type PersonProps = NonFunctionProps&lt;Person&gt;;/* Produces the below type  type PersonProps = {    name: string;    age: number;    address: string;  }  */We first get all the non-function property names from the interface. Then use the Pick mapped type to pick those from the interface to form the new interface. TypeScript provides the following inbuilt conditional types:    Exclude&lt;T, U&gt; – Exclude from T those types that are assignable to U.     Extract&lt;T, U&gt; – Extract from T those types that are assignable to U.     NonNullable&lt;T&gt; – Exclude null and undefined from T.     ReturnType&lt;T&gt; – Obtain the return type of a function type.     InstanceType&lt;T&gt; – Obtain the instance type of a constructor function type.  Let us put it into use: These advanced types become even more powerful when you combine them together. Let’s see some practical uses of this in React. React component and Redux reducer in ES6: Let see a simple React component with a reducer written in ES6. Take a look at index. jsx in the below code sandbox:                         As you can see, we use the prop-types library to define the component props. It is not the most efficient way, as it includes considerable overhead during development. It doesn’t provide full type safety anyway. React component and Redux reducer in TypeScript: Now let us convert this simple example to TypeScript so that it is type safe. Take a look at index. tsx in the below code sandbox:                         As you can see, the code is more type-safe now. It is also much more verbose even without PropTypes library and all the type inference. React component and Redux reducer in TypeScript with advanced types: Now let us apply the advanced types that we learned to make this example less verbose and even more type safe. Take a look at index. tsx in the below code sandbox:                         As you can see, we used Readonly and ReturnType mapping along with some other type inference techniques to write a more type-safe but less verbose version of the component. Conclusion: If you are using React with TypeScript, then these are some of the techniques you must apply. If you are considering a type system for React, then look no further than TypeScript. It has great features, great tooling, excellent IDE/Editor support and an awesome community. I gave a talk on TypeScript for Devoxx 2018, and you can see the video and slides if you like here.                                                                         Check out my book “Full Stack Development with JHipster” on Amazon and Packt if you like to learn about Full stack development with an awesome stack that includes TypeScript and React. If you like JHipster don’t forget to give it a star on Github. If you like this article, please like or comment. You can follow me on Twitter and LinkedIn. Originally published in Medium on November 19, 2018 "},{id:69,url:"https://deepu.tech/deploying-jhipster-microservices-on-azure-kubernetes-service-aks/",title:"Deploying JHipster Microservices on Azure Kubernetes Service (AKS)",body:"2018/10/01 - If you are developing and deploying applications to production, especially cloud, you would have heard about Kubernetes. Kubernetes(k8s) is a container orchestration platform originally developed by Google and makes deploying containerized/dockerized applications to production more manageable and scalable. Kubernetes has been crowned as the undeniable champion of container orchestration for a while now and every other K*S offering that we see sprouting up are testimonials for that. The K obviously stands for Kubernetes and S/E stands for Service/Engine and the first letter stands for the product offering it. So far we have AKS(Azure), GKE(Google), and EKS(Amazon ECS) and PKS(Pivotal) and also some flavors from Oracle and RedHat(read Openshift) One of my colleagues have written a nice article about it, I highly recommend you read it as well. In this article, we will see how we can deploy a microservice architecture created by JHipster to Azure Kubernetes Service. Azure Kubernetes Service(AKS) is the managed Kubernetes platform offering from Microsoft to host your containerized applications. Creating the microservice application: In one of my previous posts, I showcased how to create a full stack microservice architecture using JHipster and JDL, read the post here if you want to learn more details about it. For this exercise, we will use the same application. Let us recap the steps required. Create a JDL file, let’s say app. jdl, and copy the below content into it. 400: Invalid requestNow create a directory called ecommerce and navigate into it. Run the JHipster import-jdl command. It could take a few minutes, especially the NPM install step. 12$ mkdir ecommerce &amp;&amp; cd ecommerce$ jhipster import-jdl app. jdlOnce the JHipster process is complete, you will see that we have our store gateway, invoice service and notification service created and ready for us. The process until this is explained in more detail in my previous post here and you can deploy the application locally using Docker as explained in that post. If you haven’t done that before I strongly suggest that step so that you get an idea of the application and you also can make sure it works locally on your machine. Generating the Kubernetes configuration: Now that our application is ready, let us create the required configurations for Kubernetes using JHipster. This can also be done using JDL by adding below snippet to the JDL file we used earlier. 123456789deployment { deploymentType kubernetes appsFolders [store, invoice, notification] serviceDiscoveryType eureka dockerRepositoryName  deepu105  // use your own docker repo username here kubernetesNamespace jhipster kubernetesServiceType LoadBalancer monitoring no}For now, let us use the JHipster CLI to do this. In the ecommerce folder, we created earlier, create a new directory, let’s call in k8s so that we get the below structure. 12345├── app. jdl├── invoice├── kubernetes├── notification└── storeCreate the kubernetes directory and navigate to it. Now run the JHipster Kubernetes command there. 12$ mkdir kubernetes &amp;&amp; cd kubernetes$ jhipster kubernetesThe generator will ask you a few questions and choose the answers as highlighted below, as you can see the questions are very similar to the ones asked by jhipster docker-compose command. For the “base Docker repository name” provide your own docker hub account id(For example, my Docker Hub id is deepu105). For real-world use cases, you could also use a private image repository like the Azure Container Registry and in that case, you would have to provide the ACR login server name here. For now, let us keep it simple. 12345678910111213141516171819202122232425262728⎈ Welcome to the JHipster Kubernetes Generator ⎈Files will be generated in folder: /home/deepu/workspace/temp/ecommerce/kubernetes✔ Docker is installed? Which *type* of application would you like to deploy? Microservice application? Enter the root directory where your gateway(s) and microservices are located . . /3 applications found at /home/deepu/workspace/temp/ecommerce/? Which applications do you want to include in your configuration? invoice, notification, store? Do you want to setup monitoring for your applications ? No? Which applications do you want to use with clustered databases (only available with MongoDB and Couchbase)? JHipster registry detected as the service discovery and configuration provider used by your apps? Enter the admin password used to secure the JHipster Registry admin? What should we use for the Kubernetes namespace? jhipster? What should we use for the base Docker repository name? &lt;your Docker hub account id&gt;? What command should we use for push Docker image to repository? docker push? Do you want to enable Istio? No? Choose the kubernetes service type for your edge services LoadBalancer - Let a kubernetes cloud provider automatically assign an IPThe generator will go to work with this and will create the following files and output. 123456789101112131415161718192021222324252627282930313233343536373839404142  create invoice/invoice-deployment. yml  create invoice/invoice-service. yml  create invoice/invoice-mysql. yml  create notification/notification-deployment. yml  create notification/notification-service. yml  create notification/notification-mongodb. yml  create store/store-deployment. yml  create store/store-service. yml  create store/store-mysql. yml  create README. md  create registry/jhipster-registry. yml  create registry/application-configmap. yml  create kubectl-apply. shWARNING! Kubernetes configuration generated with missing images!To generate the missing Docker image(s), please run: . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/invoice . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/notification . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/storeWARNING! You will need to push your image to a registry. If you have not done so, use the following commands to tag and push the images: docker image tag invoice deepu105/invoice docker push deepu105/invoice docker image tag notification deepu105/notification docker push deepu105/notification docker image tag store deepu105/store docker push deepu105/storeYou can deploy all your apps by running the following script: . /kubectl-apply. shUse these commands to find your application's IP addresses: kubectl get svc storeCongratulations, JHipster execution is complete!As you can see the generator creates all the required Kubernetes configuration files and prints out useful information to proceed further (Note that the docker hub id you provided will be in the instructions in place of deepu105 here). Go through the generated k8s files and familiarize yourself. Now we are ready. Let us build and push the docker images for our application. Follow the instructions above and build docker images in each of the application folders and then tag and push the images to your Docker hub account. Preparing AKS Cluster: Now that our applications are built and pushed its time for us to deploy them to AKS. But before that let’s make sure we have all the prerequisites ready. You will need the below tools.    kubectl: The command line tool to interact with Kubernetes. Install and configure it.     Azure CLI: The command line tool to interact with Azure. Install and log in with your Azure account(You can create a free account if you don’t have one already).  Once the tools are ready let us prepare our Kubernetes cluster. First, let us create a resource group. Run the below command. This will create a resource group named eCommerceCluster in US east location(You can use other Azure regions as well). 1$ az group create --name eCommerceCluster --location eastusNow let us create an AKS cluster on the resource group we just created. Run the below command to create a cluster named eCommerceCluster with two nodes(We would need some room to run all those containers). It also enables the Azure monitor on the cluster through the add-on specified. 123$ az aks create --resource-group eCommerceCluster --name eCommerceCluster --node-count 2 --enable-addons monitoring --generate-ssh-keysThis would take several minutes to complete hence be patient and have a coffee :) Did I emphasize on several minutes? Once it’s done you should see the cluster information printed out as JSON. Now, let us configure kubectl to connect to the AKS cluster we just created. This can be done automatically using the Azure CLI by running the below handy command. Note: Some Azure CLI commands might take a while to execute, especially if you are on a slow network, sometimes if the below commands seem stalled or if it is timed out, kill it and retry again. 1$ az aks get-credentials --resource-group eCommerceCluster --name eCommerceClusterVerify that we are able to connect to the cluster by running kubectl get nodes 1234$ kubectl get nodesNAME            STATUS  ROLES   AGE    VERSIONaks-nodepool1-34429729-0  Ready   agent   22m    v1. 9. 9aks-nodepool1-34429729-1  Ready   agent   22m    v1. 9. 9Deploying the application to AKS: Now that our cluster is ready, let us deploy our microservice stack to this cluster. We can deploy our application using the kubectl apply command for this we have to navigate to the k8s folder we created earlier and run the below commands in the same order 1234567$ kubectl apply -f registry$ kubectl apply -f invoice$ kubectl apply -f notification$ kubectl apply -f storeOr you could also just run the handy kubectl-apply. sh script generated which runs the above. So we are deploying the JHipster Registry first as it is required for other services, followed by the microservices and finally our gateway(store). If you get a timeout during any of these, as I did, just try the command again. Though the services get created fast, the actual applications might not be up and running yet, give the entire thing a minute or two to start. Now run kubectl get pods to see the status of our containers. 1234567891011$ kubectl get pods -wNAME                  READY STATUS invoice-5ffb46d944-h8x42        1/1  Runninginvoice-mysql-66bc4b7874-p7ghk     1/1  Runningjhipster-registry-0           1/1  Runningjhipster-registry-1           1/1  Runningnotification-76847b7667-d7xjb      1/1  Runningnotification-mongodb-6db986b556-8bw8z  1/1  Runningstore-8dc5cd6f7-s2dpx          1/1  Runningstore-mysql-779d66685d-tmkqd      1/1  RunningNote: I have removed some info for brevity in the above output. Wait until all the containers are in Running status. Once the containers are running we can run the kubectl get service command to get the external IP for the application. 1234$ kubectl get service storeNAME  TYPE     CLUSTER-IP  EXTERNAL-IP  PORT(S)     AGEstore LoadBalancer 10. 0. 189. 145 40. 117. 140. 228 8080:30143/TCP 18mIn this case, the external IP for our gateway application is 40. 117. 140. 228 running on port 8080. Let us open it up in a web browser. The Gateway application login page The JHipster registry is deployed as a headless service by default. If we need to access the registry we need to create a secondary service with a Load Balancer. Run the below command to expose the second service. 1$ kubectl expose service jhipster-registry --type=LoadBalancer --name=exposed-registryNow run the kubectl get service command to get the IP. 1234$ kubectl get service exposed-registryNAME       TYPE     CLUSTER-IP  EXTERNAL-IP  PORT(S)exposed-registry LoadBalancer 10. 0. 107. 121 104. 211. 15. 142 8761:32628/TCPVisit the URL in a browser to see the registry in action JHipster Registry home page We can now scale any of our services by simply running kubectl scale command. For example, let us scale our Invoice service. 1$ kubectl scale --replicas=2 deployment/invoiceNow we can visit the Eureka -&gt; Instances on our Registry and see that the Invoice service has two instances. JHipster Registry instances page Running kubectl get pods will also show you the new instance. 123456789101112$ kubectl get pods NAME                 READY STATUS  AGEinvoice-5ffb46d944-g8j6j       1/1  Running 4minvoice-5ffb46d944-h8x42       1/1  Running 2hinvoice-mysql-66bc4b7874-p7ghk    1/1  Running 2hjhipster-registry-0          1/1  Running 2hjhipster-registry-1          1/1  Running 2hnotification-76847b7667-d7xjb     1/1  Running 2hnotification-mongodb-6db986b556-8bw8z 1/1  Running 2hstore-8dc5cd6f7-s2dpx         1/1  Running 2hstore-mysql-779d66685d-tmkqd     1/1  Running 2hThat is it, we have successfully got our application deployed to AKS and scaled our service on demand. Cleanup: Once you are done its always a good idea to clean up especially since we don’t want to keep unnecessary resources that might eat up our free credits on Azure. Let us delete the cluster from AKS and related resources created by deleting the entire resource group. 1$ az group delete --name eCommerceCluster --yes --no-waitCluster related activities like creation/update/deletion could take several minutes on AKS so we have to be patient again here. Conclusion: Kubernetes is definitely the best way to deploy microservice applications to production but creating and managing Kubernetes clusters itself is not an easy task, but Kubernetes services like GKE and AKS makes it a cakewalk. In my personal experience, the Kubernetes service from Google(GKE) and Azure(AKS) are by far the best in terms of ease of use and available tooling. These services provide very handy command line tools which work nicely together with kubectl to provide a very nice experience. They also have nice UI portals if you are not a fan of the CLI. JHipster provides a great Kubernetes setup to start with which you can further tweak as per your needs and platform. In upcoming posts, we will look at more services like GKE(Google), EKS(Amazon) and Openshift(RedHat) To learn more about JHipster, check out my book “Full Stack Development with JHipster” on Amazon and Packt. If you like JHipster don’t forget to give it a star on Github. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. My other related posts:    Create full Microservice stack using JHipster Domain Language under 30 minutes     JHipster microservices with Istio service mesh on Kubernetes  Originally published in Medium on Oct 01, 2018 "},{id:70,url:"https://deepu.tech/create-full-microservice-stack-using-j-hipster-domain-language-under-30-minutes/",title:"Create full Microservice stack using JHipster Domain Language under 30 minutes",body:"2018/09/22 - It’s been quite a while since I wrote a blog, I did a few some years ago but never really continued writing. So when I decided to start writing again, I didn’t have to think a lot about a topic as it was very obvious — JHipster. JHipster is a development platform for Java web applications and microservices development. If you are a JVM developer you might have already heard about JHipster. If not, well, you are missing out on a lot and I highly recommend you check it out. You can also check out my book “Full Stack Development with JHipster” on Amazon and Packt to learn about JHipster. I have been working on JHipster from April 2015 and the coolest feature that I got to implement so far is definitely multiple applications generation using JDL. This feature is available in the latest version of JHipster. If you are not familiar with JDL, I recommend you to check out the docs at https://www. jhipster. tech/jdl/ The E-Commerce application: So let us see how we can create a microservice stack using JHipster. We will build an e-commerce store today. The stack includes-    Service discovery using JHipster Registry, a Spring boot application that packs Eureka server and Spring cloud config server.     API management and Gateway using Spring Boot, Netflix Zuul, ReactJS, and Swagger.     Microservices using Spring Boot.     Monitoring using JHipster Console which is made of the Elastic stack(ELK) and Zipkin.  Microservice application architecture The Gateway routes incoming requests to two microservices, Invoice application, and Notification application. Requirements: In order to follow this tutorial, you would need a recent version of Docker, Docker-compose, NodeJS and Java installed on your computer. The below are the versions I have installed(Update: With JHipster 6+ you can use Java 11 &amp; 12). 12345678910111213$ docker -v                                                            Docker version 18. 06. 1-ce, build e68fc7a$ docker-compose -v                docker-compose version 1. 20. 1, build 5d8c71b$ node -v        v8. 11. 4$ java -version     openjdk version  1. 8. 0_212 OpenJDK Runtime Environment (Zulu 8. 38. 0. 13-CA-linux64) (build 1. 8. 0_212-b04)OpenJDK 64-Bit Server VM (Zulu 8. 38. 0. 13-CA-linux64) (build 25. 212-b04, mixed mode)First, install the latest version of JHipster 1$ npm install generator-jhipster -gVerify that you have version 5. 3. 4 or above by running 1$ jhipster --versionCreating the JDL: Now let us create our JDL. Head over to the JDL Studio or your favorite IDE/Editor(You can use JHipster IDE plugin if you like). First, let us define our applications. We will start with the Gateway 123456789101112131415application { config {  baseName store,  applicationType gateway,  packageName com. jhipster. demo. store,  serviceDiscoveryType eureka,  authenticationType jwt,  prodDatabaseType mysql,  cacheProvider hazelcast,  buildTool gradle,  clientFramework react,  testFrameworks [protractor] } entities *}Most of the options are self-explanatory, we are building an application named Store of type Gateway with JWT authentication and Eureka-based service discovery. The application uses a MySQL database and Hazelcast for the cache. It’s built using Gradle. For the client-side, it uses React and Sass. It also has Protractor for end-to-end testing. At the end of the definition you can see entities *, we will come to this later. Now let us define our Invoice microservice 12345678910111213application { config {  baseName invoice,  applicationType microservice,  packageName com. jhipster. demo. invoice,  serviceDiscoveryType eureka,  authenticationType jwt,  prodDatabaseType mysql,  buildTool gradle,  serverPort 8081 } entities Invoice, Shipment}It follows similar options like our Gateway and since it is microservice it doesn’t define any client-side options and also skips user management as it will be handled by the Gateway. Additionally, we have also mentioned a custom port 8081 since we do not want this application to conflict with the default port 8080 used by the Gateway. Now let us define the second microservice, the Notification application 123456789101112131415application { config {  baseName notification,  applicationType microservice,  packageName com. jhipster. demo. notification,  serviceDiscoveryType eureka,  authenticationType jwt,  databaseType mongodb,  cacheProvider no,  enableHibernateCache false,  buildTool gradle,  serverPort 8082 } entities Notification}This application follows many options similar to the Gateway and Invoice application but instead of using MySQL it uses MongoDB as its database and also disables cache. Now that our application definitions are done, we will proceed to define our entity model. For our Gateway store application, let us define the below entities and enums 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** Product sold by the Online store */entity Product {  name String required  description String  price BigDecimal required min(0)  size Size required  image ImageBlob}enum Size {  S, M, L, XL, XXL}entity ProductCategory {  name String required  description String}entity Customer {  firstName String required  lastName String required  gender Gender required  email String required pattern(/^[^@s]+@[^@s]+. [^@s]+$/)  phone String required  addressLine1 String required  addressLine2 String  city String required  country String required}enum Gender {  MALE, FEMALE, OTHER}entity ProductOrder {  placedDate Instant required  status OrderStatus required  code String required  invoiceId Long}enum OrderStatus {  COMPLETED, PENDING, CANCELLED}entity OrderItem {  quantity Integer required min(0)  totalPrice BigDecimal required min(0)  status OrderItemStatus required}enum OrderItemStatus {  AVAILABLE, OUT_OF_STOCK, BACK_ORDER}relationship OneToOne {  Customer{user(login) required} to User}relationship ManyToOne {  OrderItem{product(name) required} to Product}relationship OneToMany {  Customer{order} to ProductOrder{customer(email) required},  ProductOrder{orderItem} to OrderItem{order(code) required},  ProductCategory{product} to Product{productCategory(name)}}service Product, ProductCategory, Customer, ProductOrder, OrderItem with serviceClasspaginate Product, Customer, ProductOrder, OrderItem with paginationThe JDL defines the entities, enums, the relationship between entities and options like pagination and service layer. The entity field definition follows the syntax 123entity &lt;entity name&gt; { &lt;field name&gt; &lt;type&gt; [&lt;validation&gt;*]}The relationship definition follows the syntax 12345relationship (OneToMany | ManyToOne | OneToOne | ManyToMany) {  &lt;from entity&gt;[{&lt;relationship name&gt;[(&lt;display field&gt;)]}]   to   &lt;to entity&gt;[{&lt;relationship name&gt;[(&lt;display field&gt;)]}]}Refer the JDL docs for full DSL reference. The Invoice microservice application has the following entities 12345678910111213141516171819202122232425262728293031entity Invoice {  code String required  date Instant required  details String  status InvoiceStatus required  paymentMethod PaymentMethod required  paymentDate Instant required  paymentAmount BigDecimal required}enum InvoiceStatus {  PAID, ISSUED, CANCELLED}entity Shipment {  trackingCode String  date Instant required  details String}enum PaymentMethod {  CREDIT_CARD, CASH_ON_DELIVERY, PAYPAL}relationship OneToMany {  Invoice{shipment} to Shipment{invoice(code) required}}service Invoice, Shipment with serviceClasspaginate Invoice, Shipment with paginationmicroservice Invoice, Shipment with invoicePay attention to the last microservice option declared here, it specifies that these entities belong to the microservice named invoice so that our Gateway knows where to route requests for these entities. Now let us see the entities for the Notification microservice application 1234567891011121314entity Notification {  date Instant required  details String  sentDate Instant required  format NotificationType required  userId Long required  productId Long required}enum NotificationType {  EMAIL, SMS, PARCEL}microservice Notification with notificationNow let us go back to the entities keyword we used in our application definitions. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061application { config {  . . .  } entities *}application { config {  . . .  } entities Invoice, Shipment}application { config {  . . .  } entities Notification}/* Entities for Store Gateway */entity Product {  . . . }entity ProductCategory {  . . . }entity Customer {  . . . }entity ProductOrder {  . . . }entity OrderItem {  . . . }microservice Invoice, Shipment with invoice/* Entities for Invoice microservice */entity Invoice {  . . . }entity Shipment {  . . . }/* Entities for notification microservice */entity Notification {  . . . }microservice Notification with notificationHere we instruct the store gateway application that it should contain all the entities defined in the JDL and the gateway will know to skip server-side code for the entities that belong to another microservice and hence will only generate the client-side code for those, here namely Invoice, Shipment, and Notification. We also instruct the Invoice application and Notification application to include its entities. Generating the applications: Create a folder where we want to create our microservice stack. 1$ mkdir ecommerce &amp;&amp; cd ecommerceNow, let us put everything together into a JDL file. Let us call it app. jdl and save it into this folder. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169application { config {  baseName store,  applicationType gateway,  packageName com. jhipster. demo. store,  serviceDiscoveryType eureka,  authenticationType jwt,  prodDatabaseType mysql,  cacheProvider hazelcast,  buildTool gradle,  clientFramework react,  testFrameworks [protractor] } entities *}application { config {  baseName invoice,  applicationType microservice,  packageName com. jhipster. demo. invoice,  serviceDiscoveryType eureka,  authenticationType jwt,  prodDatabaseType mysql,  buildTool gradle,  serverPort 8081 } entities Invoice, Shipment}application { config {  baseName notification,  applicationType microservice,  packageName com. jhipster. demo. notification,  serviceDiscoveryType eureka,  authenticationType jwt,  databaseType mongodb,  cacheProvider no,  enableHibernateCache false,  buildTool gradle,  serverPort 8082 } entities Notification}/* Entities for Store Gateway *//** Product sold by the Online store */entity Product {  name String required  description String  price BigDecimal required min(0)  size Size required  image ImageBlob}enum Size {  S, M, L, XL, XXL}entity ProductCategory {  name String required  description String}entity Customer {  firstName String required  lastName String required  gender Gender required  email String required pattern(/^[^@s]+@[^@s]+. [^@s]+$/)  phone String required  addressLine1 String required  addressLine2 String  city String required  country String required}enum Gender {  MALE, FEMALE, OTHER}entity ProductOrder {  placedDate Instant required  status OrderStatus required  code String required  invoiceId Long}enum OrderStatus {  COMPLETED, PENDING, CANCELLED}entity OrderItem {  quantity Integer required min(0)  totalPrice BigDecimal required min(0)  status OrderItemStatus required}enum OrderItemStatus {  AVAILABLE, OUT_OF_STOCK, BACK_ORDER}relationship OneToOne {  Customer{user(login) required} to User}relationship ManyToOne { OrderItem{product(name) required} to Product}relationship OneToMany {  Customer{order} to ProductOrder{customer(email) required},  ProductOrder{orderItem} to OrderItem{order(code) required} ,  ProductCategory{product} to Product{productCategory(name)}}service Product, ProductCategory, Customer, ProductOrder, OrderItem with serviceClasspaginate Product, Customer, ProductOrder, OrderItem with pagination/* Entities for Invoice microservice */entity Invoice {  code String required  date Instant required  details String  status InvoiceStatus required  paymentMethod PaymentMethod required  paymentDate Instant required  paymentAmount BigDecimal required}enum InvoiceStatus {  PAID, ISSUED, CANCELLED}entity Shipment {  trackingCode String  date Instant required  details String}enum PaymentMethod {  CREDIT_CARD, CASH_ON_DELIVERY, PAYPAL}relationship OneToMany {  Invoice{shipment} to Shipment{invoice(code) required}}service Invoice, Shipment with serviceClasspaginate Invoice, Shipment with paginationmicroservice Invoice, Shipment with invoice/* Entities for notification microservice */entity Notification {  date Instant required  details String  sentDate Instant required  format NotificationType required  userId Long required  productId Long required}enum NotificationType {  EMAIL, SMS, PARCEL}microservice Notification with notificationNow let us invoke JHipster CLI to import this file 1$ jhipster import-jdl app. jdlThis will create the store, invoice and notification folders and will do the below in each of the folders    Generate the appropriate application and entities configuration.     Generate the application and entities source code based on the configurations.     Install the NPM dependencies for the application.  Once the process is complete you should see the below on your console 123456789Entity Product generated successfully. Entity ProductCategory generated successfully. Entity Customer generated successfully. Entity ProductOrder generated successfully. Entity OrderItem generated successfully. Entity Invoice generated successfully. Entity Shipment generated successfully. Entity Notification generated successfully. Congratulations, JHipster execution is complete!Walk around the generated code to familiarize yourself. Running the applications with Docker: Now that our applications are created its time to test them locally using Docker. To do this first let us generate some docker compose configurations using JHipster. Create a new folder inside the ecommerce folder and run the JHipster docker-compose command 12$ mkdir docker-compose &amp;&amp; cd docker-compose$ jhipster docker-composeIt will prompt you with a few questions, choose the answers as highlighted below 12345678910111213141516171819202122🐳 Welcome to the JHipster Docker Compose Sub-Generator 🐳Files will be generated in folder: /home/deepu/workspace/temp/ecommerce/docker-compose✔ Docker is installed? Which *type* of application would you like to deploy? Microservice application? Which *type* of gateway would you like to use? JHipster gateway based on Netflix Zuul? Enter the root directory where your gateway(s) and microservices are located . . /3 applications found at /home/deepu/workspace/temp/ecommerce/? Which applications do you want to include in your configuration? invoice, notification, store? Which applications do you want to use with clustered databases (only available with MongoDB and Couchbase)? ? Do you want to setup monitoring for your applications ? Yes, for logs and metrics with the JHipster Console (based on ELK and Zipkin)? You have selected the JHipster Console which is based on the ELK stack and additional technologies, which one do you want to use ? Zipkin, for distributed tracing (only compatible with JHipster &gt;= v4. 2. 0)JHipster registry detected as the service discovery and configuration provider used by your apps? Enter the admin password used to secure the JHipster Registry? adminThis will generate all the required docker-compose configurations for the stack and will also print out further instructions to build the docker images. Note: In the latest JHipster versions we migrated to using Jib for creating Docker images. This is a huge improvement over the Docker Maven plugin that we were using, as a result the command to create an image has changed to . /gradlew -Pprod bootWar jibDockerBuild. 12345Docker Compose configuration generated with missing images!To generate the missing Docker image(s), please run: . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/invoice . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/notification . /gradlew -Pprod bootWar jibDockerBuild in /home/deepu/workspace/temp/ecommerce/storeFollow the instructions and build the docker images. Once all 3 images are built run the below command from the docker-compose folder to fire everything up. 1$ docker-compose up -dOnce the containers start you can stream the logs using below command 1$ docker-compose logs -fNow point your favorite browser to http://localhost:8080/ and see the E-Commerce microservice application in action. Gateway application(Store) You can see the JHipster registry in action at http://localhost:8761/ JHipster Registry And finally the JHipster console at http://localhost:5601/ JHipster Console- Kibana dashboard Once you are done playing around, you can shut everything down by running the below command on the docker-compose folder 1docker-compose downHope you had fun creating microservices using JHipster. To learn how to convert a JHipster monolith to microservices check out my book “Full Stack Development with JHipster” on Amazon and Packt. In the coming weeks, I’ll write some posts about deploying this microservice stack to various cloud providers like GCP, Azure, AWS, Heroku and so on. If you like JHipster don’t forget to give it a star on Github. If you like this article, please leave a like or a comment. You can follow me on Twitter and LinkedIn. My other related posts:    Deploying JHipster Microservices on Azure Kubernetes Service (AKS)     JHipster microservices with Istio service mesh on Kubernetes  Originally published in Medium on Sep 22, 2018 "}],idx=lunr(function(){this.ref("id"),this.field("title"),this.field("body"),documents.forEach(function(e){this.add(e)},this)});function lunr_search(e){if(document.getElementById("lunrsearchresults").innerHTML="<ul></ul>",e){document.getElementById("lunrsearchresults").innerHTML="<p>Search results for '"+e+"'</p>"+document.getElementById("lunrsearchresults").innerHTML;var t=idx.search(e);if(0<t.length)for(var a=0;a<t.length;a++){var o=t[a].ref,n=documents[o].url,i=documents[o].title,o=documents[o].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+n+"'><span class='title'>"+i+"</span><br /><span class='body'>"+o+"</span><br /><span class='url'>"+n+"</span></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>No results found...</li>"}return!1}function lunr_search(e){if($("#lunrsearchresults").show(400),$("body").addClass("modal-open"),document.getElementById("lunrsearchresults").innerHTML='<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>',e){document.getElementById("modtit").innerHTML="<h5 class='modal-title'>Search results for '"+e+"'</h5>"+document.getElementById("modtit").innerHTML;var t=idx.search(e);if(0<t.length)for(var a=0;a<t.length;a++){var o=t[a].ref,n=documents[o].url,i=documents[o].title,o=documents[o].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+n+"'><span class='title'>"+i+"</span><br /><small><span class='body'>"+o+"</span><br /><span class='url'>"+n+"</span></small></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>"}return!1}$(function(){$("#lunrsearchresults").on("click","#btnx",function(){$("#lunrsearchresults").hide(5),$("body").removeClass("modal-open")})});